{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c4b139-0996-4471-990d-08f1a7bbe79a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ff1fe6-6bf8-4150-ac65-9a9f45fe8df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ and not 'installed_libs' in globals():\n",
    "  #CUDA = 'cu121' \n",
    "  installed_libs = True\n",
    "  \n",
    "  \n",
    "  !pip install torch==2.1.0  torchvision==0.16.0 torchtext==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "  import torch\n",
    "  #os.environ['TORCH'] = torch.__version__\n",
    "  #print(torch.__version__)\n",
    "  #torch_version = '2.0.0+cu118'\n",
    "  \n",
    "  #!pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html # torch_spline_conv\n",
    "  !pip install torch_geometric\n",
    "  !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "  #!pip install torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  #!pip install torch_scatter -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  #!pip install pyg_lib -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  !pip install sentence-transformers\n",
    "  !pip install torcheval\n",
    "  !pip install matplotlib\n",
    "  !pip install pandas\n",
    "  !pip install tensorboard\n",
    "  \n",
    "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "  ROOT_FOLDER = '/dbfs/FileStore/GraphNeuralNetworks/'\n",
    "else:\n",
    "  ROOT_FOLDER = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efe06c00-84ff-49e4-89a9-cad18f63b110",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amos/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dataset on disk:  2.279761238 gb\n",
      "loading saved heterodata object\n",
      "for skill job edges keep top k edges per job, k is  50\n",
      "keep tensor(1208056) of total 16289586\n"
     ]
    }
   ],
   "source": [
    "from learnings_sampler_v1 import get_datasets, uniform_hgt_sampler, get_minibatch_count, add_reverse_edge_original_attributes_and_label_inplace, get_hgt_linkloader, get_single_minibatch_count\n",
    "\n",
    "train_data, val_data, test_data = get_datasets(get_edge_attr=False, filename=ROOT_FOLDER+'HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt', filter_top_k=True, top_k=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  courses_and_programs={ x=[55796, 815] },\n",
       "  qualifications={ x=[1242, 786] },\n",
       "  skills={ x=[264052, 775] },\n",
       "  people={ x=[293444, 25] },\n",
       "  jobs={ x=[55638, 775] },\n",
       "  organizations={ x=[13613, 3] },\n",
       "  (skills, qualification_skill, qualifications)={\n",
       "    edge_index=[2, 1041],\n",
       "    edge_label=[445],\n",
       "    edge_label_index=[2, 445],\n",
       "  },\n",
       "  (skills, course_and_program_skill, courses_and_programs)={\n",
       "    edge_index=[2, 168024],\n",
       "    edge_label=[72010],\n",
       "    edge_label_index=[2, 72010],\n",
       "  },\n",
       "  (courses_and_programs, course_qualification, qualifications)={\n",
       "    edge_index=[2, 1368],\n",
       "    edge_label=[586],\n",
       "    edge_label_index=[2, 586],\n",
       "  },\n",
       "  (courses_and_programs, course_and_programs_student, people)={\n",
       "    edge_index=[2, 360300],\n",
       "    edge_label=[154413],\n",
       "    edge_label_index=[2, 154413],\n",
       "  },\n",
       "  (jobs, job_student, people)={\n",
       "    edge_index=[2, 191033],\n",
       "    edge_label=[81871],\n",
       "    edge_label_index=[2, 81871],\n",
       "  },\n",
       "  (people, supervisor_supervisee, people)={\n",
       "    edge_index=[2, 141868],\n",
       "    edge_label=[60800],\n",
       "    edge_label_index=[2, 60800],\n",
       "  },\n",
       "  (people, organization_student, organizations)={\n",
       "    edge_index=[2, 190132],\n",
       "    edge_label=[81484],\n",
       "    edge_label_index=[2, 81484],\n",
       "  },\n",
       "  (jobs, job_job, jobs)={\n",
       "    edge_index=[2, 11969],\n",
       "    edge_label=[5129],\n",
       "    edge_label_index=[2, 5129],\n",
       "  },\n",
       "  (skills, job_skill, jobs)={\n",
       "    edge_index=[2, 786446],\n",
       "    edge_label=[337047],\n",
       "    edge_label_index=[2, 337047],\n",
       "  },\n",
       "  (jobs, broader_job_job, jobs)={\n",
       "    edge_index=[2, 35537],\n",
       "    edge_label=[15229],\n",
       "    edge_label_index=[2, 15229],\n",
       "  },\n",
       "  (skills, skill_skill, skills)={\n",
       "    edge_index=[2, 1470887],\n",
       "    edge_label=[630379],\n",
       "    edge_label_index=[2, 630379],\n",
       "  },\n",
       "  (qualifications, rev_qualification_skill, skills)={ edge_index=[2, 1041] },\n",
       "  (courses_and_programs, rev_course_and_program_skill, skills)={ edge_index=[2, 168024] },\n",
       "  (qualifications, rev_course_qualification, courses_and_programs)={ edge_index=[2, 1368] },\n",
       "  (people, rev_course_and_programs_student, courses_and_programs)={ edge_index=[2, 360300] },\n",
       "  (people, rev_job_student, jobs)={ edge_index=[2, 191033] },\n",
       "  (people, rev_supervisor_supervisee, people)={ edge_index=[2, 141868] },\n",
       "  (organizations, rev_organization_student, people)={ edge_index=[2, 190132] },\n",
       "  (jobs, rev_job_job, jobs)={ edge_index=[2, 11969] },\n",
       "  (jobs, rev_job_skill, skills)={ edge_index=[2, 786446] },\n",
       "  (jobs, rev_broader_job_job, jobs)={ edge_index=[2, 35537] },\n",
       "  (skills, rev_skill_skill, skills)={ edge_index=[2, 1470887] }\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  courses_and_programs={ x=[55796, 815] },\n",
       "  qualifications={ x=[1242, 786] },\n",
       "  skills={ x=[264052, 775] },\n",
       "  people={ x=[293444, 25] },\n",
       "  jobs={ x=[55638, 775] },\n",
       "  organizations={ x=[13613, 3] },\n",
       "  (skills, qualification_skill, qualifications)={\n",
       "    edge_index=[2, 1486],\n",
       "    edge_label=[62],\n",
       "    edge_label_index=[2, 62],\n",
       "  },\n",
       "  (skills, course_and_program_skill, courses_and_programs)={\n",
       "    edge_index=[2, 240034],\n",
       "    edge_label=[10322],\n",
       "    edge_label_index=[2, 10322],\n",
       "  },\n",
       "  (courses_and_programs, course_qualification, qualifications)={\n",
       "    edge_index=[2, 1954],\n",
       "    edge_label=[82],\n",
       "    edge_label_index=[2, 82],\n",
       "  },\n",
       "  (courses_and_programs, course_and_programs_student, people)={\n",
       "    edge_index=[2, 514713],\n",
       "    edge_label=[22138],\n",
       "    edge_label_index=[2, 22138],\n",
       "  },\n",
       "  (jobs, job_student, people)={\n",
       "    edge_index=[2, 272904],\n",
       "    edge_label=[11736],\n",
       "    edge_label_index=[2, 11736],\n",
       "  },\n",
       "  (people, supervisor_supervisee, people)={\n",
       "    edge_index=[2, 202668],\n",
       "    edge_label=[8716],\n",
       "    edge_label_index=[2, 8716],\n",
       "  },\n",
       "  (people, organization_student, organizations)={\n",
       "    edge_index=[2, 271616],\n",
       "    edge_label=[11682],\n",
       "    edge_label_index=[2, 11682],\n",
       "  },\n",
       "  (jobs, job_job, jobs)={\n",
       "    edge_index=[2, 17098],\n",
       "    edge_label=[734],\n",
       "    edge_label_index=[2, 734],\n",
       "  },\n",
       "  (skills, job_skill, jobs)={\n",
       "    edge_index=[2, 1123493],\n",
       "    edge_label=[48322],\n",
       "    edge_label_index=[2, 48322],\n",
       "  },\n",
       "  (jobs, broader_job_job, jobs)={\n",
       "    edge_index=[2, 50766],\n",
       "    edge_label=[2182],\n",
       "    edge_label_index=[2, 2182],\n",
       "  },\n",
       "  (skills, skill_skill, skills)={\n",
       "    edge_index=[2, 2101266],\n",
       "    edge_label=[90376],\n",
       "    edge_label_index=[2, 90376],\n",
       "  },\n",
       "  (qualifications, rev_qualification_skill, skills)={ edge_index=[2, 1486] },\n",
       "  (courses_and_programs, rev_course_and_program_skill, skills)={ edge_index=[2, 240034] },\n",
       "  (qualifications, rev_course_qualification, courses_and_programs)={ edge_index=[2, 1954] },\n",
       "  (people, rev_course_and_programs_student, courses_and_programs)={ edge_index=[2, 514713] },\n",
       "  (people, rev_job_student, jobs)={ edge_index=[2, 272904] },\n",
       "  (people, rev_supervisor_supervisee, people)={ edge_index=[2, 202668] },\n",
       "  (organizations, rev_organization_student, people)={ edge_index=[2, 271616] },\n",
       "  (jobs, rev_job_job, jobs)={ edge_index=[2, 17098] },\n",
       "  (jobs, rev_job_skill, skills)={ edge_index=[2, 1123493] },\n",
       "  (jobs, rev_broader_job_job, jobs)={ edge_index=[2, 50766] },\n",
       "  (skills, rev_skill_skill, skills)={ edge_index=[2, 2101266] }\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1eaa98-6d39-486c-a20c-67b7bec2fdda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (node_type_embedding): Embedding(6, 256)\n",
       "  (head): TransE(6, num_relations=22, hidden_channels=256)\n",
       "  (gnn): HGT(\n",
       "    (lin_dict): ModuleDict(\n",
       "      (courses_and_programs): Linear(-1, 256, bias=True)\n",
       "      (qualifications): Linear(-1, 256, bias=True)\n",
       "      (skills): Linear(-1, 256, bias=True)\n",
       "      (people): Linear(-1, 256, bias=True)\n",
       "      (jobs): Linear(-1, 256, bias=True)\n",
       "      (organizations): Linear(-1, 256, bias=True)\n",
       "    )\n",
       "    (convs): ModuleList(\n",
       "      (0-1): 2 x HGTConv(-1, 256, heads=8)\n",
       "    )\n",
       "    (lin): Linear(256, 256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from models.TransE import TransE\n",
    "from models.DistMult import DistMult\n",
    "from models.HGT import HGT\n",
    "import torch_geometric\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, gnn : torch.nn.Module, head :  torch.nn.Module, node_types, edge_types, ggn_output_dim, pnorm=1, margin=1):\n",
    "        super().__init__()\n",
    "        # edge_type onehot lookup table with keys\n",
    "        # node_type onehot lookup table with keys\n",
    "        self.node_type_embedding = torch.nn.Embedding(len(node_types), ggn_output_dim) # hidden channels should be the output dim of gnn\n",
    "        \n",
    "        self.edge_types = edge_types\n",
    "        for edge_type in edge_types:\n",
    "            if edge_type[1].startswith('rev_'):\n",
    "                self.edge_types.remove(edge_type)\n",
    "        \n",
    "        # create edge to int mapping\n",
    "        self.edgeindex_lookup = {edge_type:torch.tensor(i)  for i, edge_type in enumerate(edge_types)}\n",
    "            \n",
    "        # hidden channels should be the output dim of gnn\n",
    "        if head=='TransE': \n",
    "            self.head = TransE(len(node_types), len(edge_types) , ggn_output_dim, p_norm= pnorm, margin=margin)  # KGE head with loss function\n",
    "        elif head=='DistMult':\n",
    "            self.head = DistMult(len(node_types), len(edge_types) , ggn_output_dim, p_norm= pnorm, margin=margin)  # KGE head with loss function\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        self.gnn = gnn\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, hetero_data1, target_edge_type, edge_label_index, edge_label, hetero_data2=None):\n",
    "        \n",
    "        if hetero_data2 is not None:\n",
    "            assert target_edge_type[0] != target_edge_type[2], 'when passing two data objects, the edge type has to contain two different node types'\n",
    "            head_embeddings = self.gnn(hetero_data1.x_dict, hetero_data1.edge_index_dict)[target_edge_type[0]][edge_label_index[0,:]]\n",
    "            tail_embeddings = self.gnn(hetero_data2.x_dict, hetero_data2.edge_index_dict)[target_edge_type[2]][edge_label_index[1,:]]\n",
    "        else:\n",
    "            assert target_edge_type[0] == target_edge_type[2], 'when passing one data object, the edge type has to contain the same node types'\n",
    "\n",
    "\n",
    "            embeddings = self.gnn(hetero_data1.x_dict, hetero_data1.edge_index_dict)\n",
    "            head_embeddings = embeddings[target_edge_type[0]][edge_label_index[0,:]]\n",
    "            tail_embeddings = embeddings[target_edge_type[2]][edge_label_index[1,:]]\n",
    "\n",
    "        \n",
    "        edgeindex = self.edgeindex_lookup[target_edge_type]\n",
    "        loss = self.head.loss(head_embeddings, edgeindex.to(device), tail_embeddings, edge_label)\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "metadata = train_data.metadata()\n",
    "# add selfloops\n",
    "for node_type in train_data.node_types:\n",
    "    metadata[1].append((node_type, 'self_loop', node_type))    \n",
    "    \n",
    "out_channels = 256\n",
    "hidden_channels = 256\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "pnorm = 2\n",
    "head = 'TransE'\n",
    "margin=0.1\n",
    "gnn = HGT(hidden_channels=out_channels, out_channels=out_channels, num_heads=num_heads, num_layers=num_layers, node_types=train_data.node_types, data_metadata=metadata)\n",
    "\n",
    "model = Model(gnn, head=head, node_types=metadata[0], edge_types=metadata[1], ggn_output_dim=out_channels, pnorm=pnorm,margin=margin)\n",
    "#torch_geometric.compile(model, dynamic=True)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4965836"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# get trainable parameter count\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebad4a8-bb51-45f9-8f93-8036fd9e89ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_node_types 6\n",
      "num_neighbors [133, 1333]\n",
      "avg_num_neighbors [4.15625, 41.65625, 0]\n",
      "writer runs/learningpeople_hgt_20231103_194142_margin_0.1_pnrom2_llr0.0002_bs32_neighbors_133_1333_head_TransE_hiddenchannels_256_outchannels_256_numheads_8_numlayers_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4826 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.1037\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4826 [00:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amos/programming/create_graphds/learnings_training_v1_learningpeople_hgt_euclidean_2layers.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_training_v1_learningpeople_hgt_euclidean_2layers.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_training_v1_learningpeople_hgt_euclidean_2layers.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_training_v1_learningpeople_hgt_euclidean_2layers.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m         minibatch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(val_sampler)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_training_v1_learningpeople_hgt_euclidean_2layers.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_training_v1_learningpeople_hgt_euclidean_2layers.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=72'>73</a>\u001b[0m         val_sampler \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(val_sampler)\n",
      "File \u001b[0;32m~/programming/create_graphds/learnings_sampler_v1.py:338\u001b[0m, in \u001b[0;36mget_hgt_linkloader.<locals>.get_hgt_2types_with_selfloops\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39m# batch the start and end supervision nodes separately\u001b[39;00m\n\u001b[1;32m    337\u001b[0m hgt_batch1 \u001b[39m=\u001b[39m get_hgt(data, target_edge[\u001b[39m0\u001b[39m], original_edge_label_nodes_class1)\n\u001b[0;32m--> 338\u001b[0m hgt_batch2 \u001b[39m=\u001b[39m get_hgt(data, target_edge[\u001b[39m2\u001b[39;49m], original_edge_label_nodes_class2)\n\u001b[1;32m    341\u001b[0m \u001b[39m# ** We dont need to remove any edges ** since the supervision edges wont be sampled by hgt\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39mif\u001b[39;00m sampling_mode\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtriplet\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/programming/create_graphds/learnings_sampler_v1.py:250\u001b[0m, in \u001b[0;36mget_hgt_linkloader.<locals>.get_hgt\u001b[0;34m(data, input_nodetype, input_mask)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_hgt\u001b[39m(data, input_nodetype, input_mask):\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(HGTLoader(\n\u001b[1;32m    251\u001b[0m             data,\n\u001b[1;32m    252\u001b[0m             \u001b[39m# Sample 512 nodes per type and per iteration for 4 iterations\u001b[39;49;00m\n\u001b[1;32m    253\u001b[0m             num_samples\u001b[39m=\u001b[39;49mnum_neighbors_hgtloader,\n\u001b[1;32m    254\u001b[0m             batch_size\u001b[39m=\u001b[39;49minput_mask\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m    255\u001b[0m             input_nodes\u001b[39m=\u001b[39;49m(input_nodetype, input_mask),\n\u001b[1;32m    256\u001b[0m             num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    257\u001b[0m             pin_memory\u001b[39m=\u001b[39;49mpin_memory,\n\u001b[1;32m    258\u001b[0m             prefetch_factor\u001b[39m=\u001b[39;49mprefetch_factor,\n\u001b[1;32m    259\u001b[0m         )))\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch_geometric/loader/node_loader.py:134\u001b[0m, in \u001b[0;36mNodeLoader.collate_fn\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Samples a subgraph from a batch of input nodes.\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m input_data: NodeSamplerInput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_data[index]\n\u001b[0;32m--> 134\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_sampler\u001b[39m.\u001b[39;49msample_from_nodes(input_data)\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_per_worker:  \u001b[39m# Execute `filter_fn` in the worker process\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_fn(out)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch_geometric/sampler/hgt_sampler.py:62\u001b[0m, in \u001b[0;36mHGTSampler.sample_from_nodes\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_from_nodes\u001b[39m(\n\u001b[1;32m     57\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     58\u001b[0m     inputs: NodeSamplerInput,\n\u001b[1;32m     59\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m     60\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m HeteroSamplerOutput:\n\u001b[0;32m---> 62\u001b[0m     node, row, col, edge \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mtorch_sparse\u001b[39m.\u001b[39;49mhgt_sample(\n\u001b[1;32m     63\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolptr_dict,\n\u001b[1;32m     64\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrow_dict,\n\u001b[1;32m     65\u001b[0m         {inputs\u001b[39m.\u001b[39;49minput_type: inputs\u001b[39m.\u001b[39;49mnode},\n\u001b[1;32m     66\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_samples,\n\u001b[1;32m     67\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_hops,\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m HeteroSamplerOutput(\n\u001b[1;32m     71\u001b[0m         node\u001b[39m=\u001b[39mnode,\n\u001b[1;32m     72\u001b[0m         row\u001b[39m=\u001b[39mremap_keys(row, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_edge_type),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     76\u001b[0m         metadata\u001b[39m=\u001b[39m(inputs\u001b[39m.\u001b[39minput_id, inputs\u001b[39m.\u001b[39mtime),\n\u001b[1;32m     77\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/_ops.py:692\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    688\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    691\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "batch_size = 32\n",
    "num_node_types = len(train_data.node_types)\n",
    "print('num_node_types', num_node_types)\n",
    "one_hop_neighbors = (25 * batch_size)//num_node_types # per relationship type\n",
    "two_hop_neighbors = (25 * 10 * batch_size)//num_node_types # per relationship type\n",
    "three_hop_neighbors = (25 * 10 * 2 * batch_size)//num_node_types # per relationship type\n",
    "num_neighbors = [one_hop_neighbors, two_hop_neighbors] # three_hop_neighbors\n",
    "# num_neighbors [36, 363, 1454]\n",
    "\n",
    "print('num_neighbors', num_neighbors)\n",
    "print('avg_num_neighbors', [num_neighbors[0]/batch_size,num_neighbors[1]/batch_size,  num_neighbors[2]/batch_size if len(num_neighbors)==3 else 0 ])\n",
    "\n",
    "input_edgetype = ('people', 'rev_course_and_programs_student', 'courses_and_programs')\n",
    "add_reverse_edge_original_attributes_and_label_inplace(train_data['courses_and_programs', 'course_and_programs_student', 'people'], reverse_edge=train_data[input_edgetype] )\n",
    "add_reverse_edge_original_attributes_and_label_inplace(val_data['courses_and_programs', 'course_and_programs_student', 'people'], reverse_edge=val_data[input_edgetype] )\n",
    "\n",
    "train_sampler = get_hgt_linkloader(train_data, input_edgetype, batch_size, True, 'triplet', 1, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
    "val_sampler = get_hgt_linkloader(val_data, input_edgetype, batch_size, False, 'triplet', 1, num_neighbors, num_workers=0, prefetch_factor=None, pin_memory=True)\n",
    "\n",
    "learning_rate = 2e-4\n",
    "# torch get optimizer by string name\n",
    "optimizer = 'Adam'\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) #2e-15\n",
    "\n",
    "\n",
    "# create a tensorboard writer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "neighbors = '_'.join([str(n) for n in num_neighbors])\n",
    "\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "writer = SummaryWriter(ROOT_FOLDER+f'runs/learningpeople_hgt_{timestamp}_margin_{margin}_pnrom{pnorm}_lr{learning_rate}_bs{batch_size}_neighbors_{neighbors}_head_{head}_hiddenchannels_{hidden_channels}_outchannels_{out_channels}_numheads_{num_heads}_numlayers_{num_layers}')\n",
    "print('writer',ROOT_FOLDER+f'runs/learningpeople_hgt_{timestamp}_margin_{margin}_pnrom{pnorm}_llr{learning_rate}_bs{batch_size}_neighbors_{neighbors}_head_{head}_hiddenchannels_{hidden_channels}_outchannels_{out_channels}_numheads_{num_heads}_numlayers_{num_layers}')\n",
    "\n",
    "model.train()\n",
    "total_minibatches = get_single_minibatch_count(train_data, batch_size, input_edgetype)\n",
    "start_epoch = 1\n",
    "for epoch in range(start_epoch, start_epoch+1000):\n",
    "    for i,minibatch in tqdm(enumerate(train_sampler), total=total_minibatches):\n",
    "        \n",
    "        optimizer.zero_grad() \n",
    "        # batching is different depending on if node types in edge are same or different\n",
    "        \n",
    "        minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id, _, _ = minibatch\n",
    "        #print(minibatchpart1['jobs'].device, minibatchpart2['jobs'].device, edge_label_index.device, edge_label.device)\n",
    "        loss = model(minibatchpart1.to(device), input_edgetype, edge_label_index.to(device), edge_label.to(device), minibatchpart2.to(device))\n",
    "        #loss, pos, neg = model(minibatchpart1, target_edge_type, edge_label_index, edge_label, minibatchpart2)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_samples_seen = i * batch_size\n",
    "        writer.add_scalar('Loss/train', loss.item(), total_samples_seen)\n",
    "        \n",
    "        if i == total_minibatches-1:\n",
    "            print(f'{i} loss: {loss.item():.4f}')\n",
    "            writer.add_scalar('Epoch Loss/train', loss.item(), total_samples_seen)\n",
    "        \n",
    "        # print loss and minibatch in the same line\n",
    "        print(f'{i} loss: {loss.item():.4f}', end='\\r')\n",
    "        \n",
    "        if i % 300 == 0 or i == total_minibatches-1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for _ in range(3):\n",
    "                    try:\n",
    "                        minibatch = next(val_sampler)\n",
    "                    except StopIteration:\n",
    "                        val_sampler = iter(val_sampler)\n",
    "                        minibatch = next(val_sampler)\n",
    "\n",
    "                    minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id, _, _ = minibatch\n",
    "                    val_loss += model(minibatchpart1.to(device), input_edgetype, edge_label_index.to(device), edge_label.to(device), minibatchpart2.to(device)).item()\n",
    "            \n",
    "            val_loss /= 3\n",
    "            if i == 0:\n",
    "                writer.add_scalar('Epoch Loss/val', val_loss, total_samples_seen)\n",
    "                writer.add_scalar('Loss/val', val_loss, total_samples_seen)\n",
    "            elif i == total_minibatches-1:\n",
    "                writer.add_scalar('Epoch Loss/val', val_loss, total_samples_seen)\n",
    "            else:\n",
    "                writer.add_scalar('Loss/val', val_loss, total_samples_seen)\n",
    "            \n",
    "\n",
    "            print(f'val_loss: {val_loss:.4f}', end='\\r')\n",
    "            model.train()\n",
    "\n",
    "        writer.flush()\n",
    "        \n",
    "        if i % 1000 == 0 or i == total_minibatches-1:\n",
    "            folder = 'models'\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            \n",
    "            run_folder = ROOT_FOLDER+f'{folder}/learningpeople_hgt_{timestamp}_margin_{margin}_pnrom{pnorm}_llr{learning_rate}_bs{batch_size}_neighbors_{neighbors}_head_{head}_hiddenchannels_{hidden_channels}_outchannels_{out_channels}_numheads_{num_heads}_numlayers_{num_layers}'\n",
    "            if not os.path.exists(run_folder):\n",
    "                os.makedirs(run_folder)\n",
    "                \n",
    "            print('saving model to', run_folder)\n",
    "            # save model and optimizer\n",
    "            is_epoch = f'Ep{epoch}_' if i == total_minibatches-1 else ''\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, run_folder+f'/{is_epoch}model_samplesseen{total_samples_seen}.pt')\n",
    "            \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c4d9e1a-f524-40ce-a32b-9666b6fc3e4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(minibatch)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "learnings_training_v1_learningpeople_hgt_euclidean",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "pyg_torch21",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
