{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7kMzXI99v30",
        "outputId": "7a9b9b16-0ecb-4ae6-a458-c72f12fab3ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/amos/mambaforge/envs/pyg/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import drive\n",
        "  colab_path = '/content/'\n",
        "  drive.mount('/content/drive',force_remount=True)\n",
        "  DRIVE_FOLDER = Path('/content/drive/MyDrive/DataExplorationProject/Skill_Ontology_GNN')\n",
        "  colab = True\n",
        "else:\n",
        "  colab_path = ''\n",
        "  colab = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMPbBOCDDKIG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "import gc\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, criterion, optimizer, device, metrics=[]):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.metrics_history = self.create_metrics_history(metrics)\n",
        "        self.epoch = 0\n",
        "\n",
        "    def create_metrics_history(self, metrics):\n",
        "        metrics = set(metrics)\n",
        "        metrics.add('epoch')\n",
        "        metrics.add('minibatch')\n",
        "        metrics.add('accuracy')\n",
        "        metrics.add('loss')\n",
        "\n",
        "        metrics = list(metrics)\n",
        "        metrics_history={}\n",
        "        for split in ['train','val']:\n",
        "            metrics_history[split]={}\n",
        "            for metric in metrics:\n",
        "                metrics_history[split][metric]=[]\n",
        "        return metrics_history\n",
        "\n",
        "    def free_memory(self):\n",
        "        \"\"\"Clears the GPU cache and triggers garbage collection, to reduce OOMs.\"\"\"\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    def train(self, dataloader, n_epochs, save_interval, save_path):\n",
        "        self.free_memory()\n",
        "        self.model.train()\n",
        "        for epoch in range(self.epoch, self.epoch+n_epochs):\n",
        "            print(f'=============== Epoch {epoch} ===============')\n",
        "            for batch_idx, (data, target) in enumerate(dataloader):\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                self.train_losses.append(loss.item())\n",
        "                if batch_idx % save_interval == 0:\n",
        "                    self.save_checkpoint(batch_idx, save_path)\n",
        "\n",
        "                print(f'Mini-Batch {batch_idx}, Loss: {loss}')\n",
        "\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data, target in dataloader:\n",
        "                data, target = data.to(self.device), target.to(self.device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "                self.val_losses.append(loss.item())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def save_checkpoint(self, batch_idx, save_path):\n",
        "        print('save')\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'metrics_history': self.metrics_history,\n",
        "        }, f'{save_path}/checkpoint_{batch_idx}.pt')\n",
        "\n",
        "    def load_checkpoint(self, load_path):\n",
        "        checkpoint = torch.load(load_path)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.train_losses = checkpoint['train_losses']\n",
        "        self.val_losses = checkpoint['val_losses']\n",
        "\n",
        "    def plot_losses(self):\n",
        "        plt.figure(figsize=(10,5))\n",
        "        plt.title(\"Training and Validation Loss\")\n",
        "        plt.plot(self.train_losses,label=\"train\")\n",
        "        plt.plot(self.val_losses,label=\"val\")\n",
        "        plt.xlabel(\"iterations\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCh0sDNr-I8D",
        "outputId": "c78900b5-79c1-412b-8f99-b991c20e376a"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "    # Install required packages.\n",
        "    import os\n",
        "    import torch\n",
        "    os.environ['TORCH'] = torch.__version__\n",
        "    print(torch.__version__)\n",
        "\n",
        "    !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "    !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "    !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "    !pip install sentence-transformers\n",
        "    !pip install torcheval\n",
        "    # unpack datasets\n",
        "    if not 'unzipped' in globals():\n",
        "        !unzip /content/drive/MyDrive/DataExplorationProject/Skill_Ontology_GNN/neo4jgraph.zip\n",
        "        unzipped =True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFUH8XJGsiaq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "p1oM_TnG9udQ",
        "outputId": "094d85e4-269c-4906-ee4b-396d85fec6d1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch_geometric.data import HeteroData\n",
        "import torch_geometric.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb6bIOsz9udU"
      },
      "outputs": [],
      "source": [
        "# only use skill nodes which have normalized_name != NaN, this is some indication of quality skill (?)\n",
        "skill_nodes = pd.read_csv(colab_path+'neo4jgraph/skills.csv').dropna(subset=['normalized_name']).reset_index()\n",
        "job_nodes = pd.read_csv(colab_path+'neo4jgraph/onet_skills_unique.csv')\n",
        "\n",
        "# drop some skills \"or\"\n",
        "skill_nodes = skill_nodes.loc[~skill_nodes.skill.isin(['or','technology'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jj8pvb49udV"
      },
      "outputs": [],
      "source": [
        "# There are duplicate normalized names\n",
        "skill_nodes.shape[0]-skill_nodes.normalized_name.unique().shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTuAjvSa9udV"
      },
      "outputs": [],
      "source": [
        "# There are not as many skill names which are duplicate\n",
        "skill_nodes.shape[0]-skill_nodes.skill.unique().shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwaRmpWL9udW"
      },
      "outputs": [],
      "source": [
        "# we can not use normalized name instead of skill, because it is ambiguous, e.g. communication points to different normalized names\n",
        "skill_nodes.loc[skill_nodes.skill=='communication']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIlL1tPh9udW"
      },
      "outputs": [],
      "source": [
        "skill_nodes.drop_duplicates(subset='skill', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPzgTNR79udW"
      },
      "outputs": [],
      "source": [
        "\n",
        "skill_job_edges = pd.read_csv(colab_path+'neo4jgraph/tfidf_skill_job_edge.csv')\n",
        "#skill_job_edges = skill_job_edges.loc[skill_job_edges.scaled_tfidf>8]\n",
        "# only use edges where we have the skill and job for from the other files\n",
        "skill_job_edges = skill_job_edges.loc[skill_job_edges['skill'].isin(skill_nodes['skill'])]\n",
        "skill_job_edges = skill_job_edges.loc[skill_job_edges['alt_title'].isin(job_nodes.index)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVB9kvu49udX"
      },
      "outputs": [],
      "source": [
        "skill_job_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gppNxu5m9udX"
      },
      "outputs": [],
      "source": [
        "#for each alt title select the first 20 skill_job edges, ordered by tfidf\n",
        "skill_job_edges = skill_job_edges.groupby('alt_title').apply(lambda group: group.nlargest(20,'scaled_tfidf')).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0twUk4Pf9udX"
      },
      "outputs": [],
      "source": [
        "skill_job_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCzFw-fn9udY"
      },
      "outputs": [],
      "source": [
        "skillmapping ={}\n",
        "for i,skill in enumerate(skill_nodes.skill.unique()):\n",
        "    skillmapping[skill] =i\n",
        "\n",
        "jobmapping ={}\n",
        "for i,index in enumerate(job_nodes['index'].unique()):\n",
        "    jobmapping[index] =i\n",
        "\n",
        "inverted_skillmapping = {v:k for k,v in skillmapping.items()}\n",
        "inverted_jobmapping = {v:k for k,v in jobmapping.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHirmQzn9udY"
      },
      "outputs": [],
      "source": [
        "skill_job_edges['skill_dst'] = skill_job_edges['skill'].apply(lambda x:skillmapping[x])\n",
        "skill_job_edges['job_src'] = skill_job_edges['alt_title'].apply(lambda x:jobmapping[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yfi1fRd9udY"
      },
      "outputs": [],
      "source": [
        "if colab:\n",
        "    onet_alttitles = pd.read_csv(colab_path+'/content/neo4jgraph/onet_alt_titles_unique.csv')\n",
        "else:\n",
        "    onet_alttitles = pd.read_csv('neo4jgraph/onet_alt_titles_unique.csv')\n",
        "del onet_alttitles['Unnamed: 0']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FgE8nyo9udY"
      },
      "outputs": [],
      "source": [
        "onet_alttitle_str_mapping = {}\n",
        "for i,row in onet_alttitles.iterrows():\n",
        "    onet_alttitle_str_mapping[row['index']] = row['Alternate Title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1_J7gV59udY"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yTzdBoU9udZ"
      },
      "outputs": [],
      "source": [
        "# create alttitle sbert embeddings to get pca dim\n",
        "\n",
        "alttitle_sbert_embeddings = embedder.encode(list(onet_alttitle_str_mapping.values()), convert_to_tensor=False, device='cuda')\n",
        "#alttitle_sbert_indices = [k for k,v in temp]\n",
        "#corpus_embeddings = util.normalize_embeddings(corpus_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de19o3FI9udZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "v = alttitle_sbert_embeddings[0]\n",
        "np.matmul(v.T,v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQsE8smF9udZ"
      },
      "outputs": [],
      "source": [
        "skill_sbert_embeddings = embedder.encode(list(skillmapping.keys()), convert_to_tensor=False, device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAT18fhw9udZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.decomposition import PCA\n",
        "X = np.concatenate([alttitle_sbert_embeddings,skill_sbert_embeddings])\n",
        "\n",
        "# print('Original:',X.shape[1])\n",
        "# for variance_retained in [0.99,0.95,0.9,0.8,0.75,0.7]:\n",
        "#     pca = PCA(n_components=variance_retained)\n",
        "#     pca.fit(X)\n",
        "#     n_components_retained = pca.n_components_\n",
        "#     print(n_components_retained,' components retained', variance_retained, ' variance retained')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK1p6Rf-9udZ"
      },
      "outputs": [],
      "source": [
        "# choose 128\n",
        "pca = PCA(n_components=128)\n",
        "pca.fit(X)\n",
        "\n",
        "skill_sbert_embeddings = pca.transform(embedder.encode(skill_nodes['skill'].tolist(), convert_to_numpy=True, device='cuda'))\n",
        "job_sbert_embeddings = pca.transform(embedder.encode(job_nodes['Alternate Title'].tolist(), convert_to_numpy=True, device='cuda'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vD1fRkf9udZ"
      },
      "outputs": [],
      "source": [
        "# add job-job edges, dataset see https://www.onetcenter.org/dictionary/26.3/excel/related_occupations.html\n",
        "job_job_edges = pd.read_csv(colab_path+'neo4jgraph/onet_related_occupations.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMsTH1Sx9udZ"
      },
      "outputs": [],
      "source": [
        "job_job_edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1lR651a9uda"
      },
      "outputs": [],
      "source": [
        "job_job_edges['job_src'] = job_job_edges['index_x'].apply(lambda x: jobmapping[x])\n",
        "job_job_edges['job_dst'] = job_job_edges['index_y'].apply(lambda x: jobmapping[x])\n",
        "relatedness_weight = {\n",
        "    'Supplemental':1,\n",
        "    'Primary-Long':2,\n",
        "    'Primary-Short':4\n",
        "}\n",
        "job_job_edges['relatedness_weight'] = job_job_edges['Relatedness Tier'].apply(lambda x: relatedness_weight[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1Wc7CIL9uda"
      },
      "outputs": [],
      "source": [
        "skill_skill_edges = pd.read_csv(colab_path+'neo4jgraph/skill_skill_edges.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qOHTGfL9uda"
      },
      "outputs": [],
      "source": [
        "#filter out potentially bad skills (which are not in our original skillmapping)\n",
        "skill_skill_edges = skill_skill_edges.loc[(skill_skill_edges.skill.isin(list(skillmapping.keys()))) & (skill_skill_edges.related_skill.isin(list(skillmapping.keys())))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQhTLGTV9uda"
      },
      "outputs": [],
      "source": [
        "skill_skill_edges['skill_src'] = skill_skill_edges['skill'].apply(lambda x: skillmapping[x])\n",
        "skill_skill_edges['skill_dst'] = skill_skill_edges['related_skill'].apply(lambda x: skillmapping[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "La85Q2M59uda"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'HeteroData' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/amos/programming/create_graphds/colab.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/colab.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m HeteroData()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/colab.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mSkill\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(skill_sbert_embeddings)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/colab.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mJob\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(job_sbert_embeddings)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'HeteroData' is not defined"
          ]
        }
      ],
      "source": [
        "data = HeteroData()\n",
        "data['Skill'].x = torch.tensor(skill_sbert_embeddings)\n",
        "data['Job'].x = torch.tensor(job_sbert_embeddings)\n",
        "\n",
        "data['Job','REQUIRES','Skill'].edge_index = torch.tensor(skill_job_edges[['job_src','skill_dst']].to_numpy().T)\n",
        "data['Skill','IS_SIMILAR_SKILL','Skill'].edge_index = torch.tensor(skill_skill_edges[['skill_src','skill_dst']].to_numpy().T)\n",
        "data['Job','IS_SIMILAR_JOB','Job'].edge_index = torch.tensor(job_job_edges[['job_src','job_dst']].to_numpy().T)\n",
        "\n",
        "\n",
        "data['Job','REQUIRES','Skill'].edge_weight = torch.tensor(skill_job_edges['scaled_tfidf'].to_numpy()).to(torch.float)\n",
        "data['Skill','IS_SIMILAR_SKILL','Skill'].edge_weight = torch.tensor(skill_skill_edges['cosine_sim_score'].to_numpy()).to(torch.float)\n",
        "data['Job','IS_SIMILAR_JOB','Job'].edge_weight = torch.tensor(job_job_edges['relatedness_weight'].to_numpy()).to(torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data['Job'].x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at node degree statistics\n",
        "\n",
        "from torch_geometric.utils import to_dense_adj, degree\n",
        "\n",
        "\n",
        "\n",
        "job_n = data['Job'].x.shape[0]\n",
        "skill_n = data['Skill'].x.shape[0]\n",
        "\n",
        "JRS_J = degree(data['Job','REQUIRES','Skill'].edge_index[0], num_nodes=job_n)\n",
        "JRS_S = degree(data['Job','REQUIRES','Skill'].edge_index[1], num_nodes=skill_n)\n",
        "SSIMS_1 = degree(data['Skill','IS_SIMILAR_SKILL','Skill'].edge_index[0], num_nodes=skill_n)\n",
        "SSIMS_2 = degree(data['Skill','IS_SIMILAR_SKILL','Skill'].edge_index[1], num_nodes=skill_n)\n",
        "S_SIM_S = SSIMS_1 + SSIMS_2\n",
        "JSIM_1 = degree(data['Job','IS_SIMILAR_JOB','Job'].edge_index[0], num_nodes=job_n)\n",
        "JSIM_2 = degree(data['Job','IS_SIMILAR_JOB','Job'].edge_index[1], num_nodes=job_n)\n",
        "J_SIM_J = JSIM_1 + JSIM_2\n",
        "\n",
        "actual_skill_n = torch.nonzero(JRS_S+S_SIM_S).shape[0] # only skills which have any edge at all\n",
        "actual_job_n = torch.nonzero(JRS_J+J_SIM_J).shape[0] # only job which have any edge at all\n",
        "print(f'Jobs: {job_n}, actual Jobs used (in at least one edge): {actual_job_n}')\n",
        "print(f'Skills: {skill_n}, actual Skills used (in at least one edge): {actual_skill_n}')\n",
        "\n",
        "print('\\nFollowing metrics only include Skills and Jobs with at least one edge:\\n')\n",
        "\n",
        "print(f'Average JRS Job degree: {torch.sum(JRS_J)/actual_job_n}, Skill: {torch.sum(JRS_S)/actual_skill_n}')\n",
        "print(f'Median JRS Job degree: {torch.median(JRS_J[JRS_J!=0])}, Skill: {torch.median(JRS_S[JRS_S!=0])}\\n')\n",
        "print(f'Average S_SIM_S degree: {torch.sum(S_SIM_S)/actual_skill_n}')\n",
        "print(f'Median S_SIM_S degree: {torch.median(S_SIM_S[S_SIM_S!=0])}')\n",
        "print(f'Average J_SIM_J degree: {torch.sum(J_SIM_J)/actual_job_n}')\n",
        "print(f'Average J_SIM_J degree: {torch.median(J_SIM_J[J_SIM_J!=0])}\\n')\n",
        "\n",
        "\n",
        "print(f'Average total degree: Job: {(torch.sum(JRS_J)+torch.sum(J_SIM_J))/actual_job_n}')\n",
        "print(f'Average total degree: Skill: {(torch.sum(JRS_S)+torch.sum(S_SIM_S))/actual_skill_n}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.title('JRS Job degree distribution')\n",
        "plt.hist(JRS_J[JRS_J!=0], bins=10);\n",
        "plt.show()\n",
        "plt.title('JRS Skill degree distribution')\n",
        "plt.hist(JRS_S[JRS_S!=0], bins=100);\n",
        "plt.show()\n",
        "plt.title('S_SIM_S Skill degree')\n",
        "plt.hist(S_SIM_S[S_SIM_S!=0], bins=100);\n",
        "plt.show()\n",
        "plt.title('J_SIM_J Job degree')\n",
        "plt.hist(J_SIM_J[J_SIM_J!=0], bins=100);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add features:\n",
        "# - node degree\n",
        "# - normalize edge weights by node degree\n",
        "# - (triangle count)\n",
        "\n",
        "\n",
        "# add node degree statistics:\n",
        "\n",
        "job_degrees = torch.cat((JRS_J.reshape(-1,1) / 20, J_SIM_J.reshape(-1,1)/ 126), dim=1) # divide by max degrees\n",
        "skill_degrees = torch.cat((JRS_S.reshape(-1,1) / 6681, S_SIM_S.reshape(-1,1)/ 1428), dim=1) # divide by max degrees\n",
        "\n",
        "data['Job'].x = torch.cat((data['Job'].x, job_degrees), dim=1)\n",
        "data['Skill'].x = torch.cat((data['Skill'].x, skill_degrees), dim=1)\n",
        "\n",
        "# normalize edge weights by node degree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# adj_matmul\n",
        "# row1 to rowS * full_matrix\n",
        "# rowS+1 to rowT * full_matrix\n",
        "# ....\n",
        "#cat(row_blocks, dim=0)\n",
        "from tqdm.auto import tqdm\n",
        "from torch_sparse.diag import get_diag\n",
        "from torch_sparse.cat import cat\n",
        "def blockwise_sparse_square_mmul(adj_matrix, blocks=None):\n",
        "    row_blocks = []\n",
        "    \n",
        "    if blocks is None:\n",
        "        row_block_size = 10000\n",
        "        rows = adj_matrix.size(0)\n",
        "        for block in tqdm(range(0,rows, row_block_size), desc='blockwise sparse matrix-multiplication'):\n",
        "            start = block\n",
        "            end = min(block+row_block_size, rows)\n",
        "            row_blocks.append(adj_matrix[start:end].spspmm(adj_matrix))\n",
        "    else:\n",
        "        for block in tqdm(blocks, desc='blockwise sparse matrix-multiplication'):\n",
        "            row_blocks.append(block.spspmm(adj_matrix))\n",
        "\n",
        "    return row_blocks\n",
        "\n",
        "def blockwise_sparse_get_diag(blocks):\n",
        "    diags = []\n",
        "    for block in tqdm(blocks, desc='get blockwise sparse matrix diagonal'):\n",
        "        diags.append(get_diag(block))\n",
        "    \n",
        "    return torch.cat(diags, dim=0)\n",
        "\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_sparse import SparseTensor\n",
        "from torch_sparse.diag import get_diag\n",
        "\n",
        "def undirected_triangle_counts(edge_index, max_num_nodes): \n",
        "    \"\"\"Get triangles **per node**, to get count for whole graph, divide by 3\"\"\"\n",
        "    ud = to_undirected(edge_index)\n",
        "    adj_matrix = SparseTensor(row=ud[0], col=ud[1], sparse_sizes=(max_num_nodes, max_num_nodes))\n",
        "    x = blockwise_sparse_square_mmul(adj_matrix)\n",
        "    x = blockwise_sparse_square_mmul(adj_matrix, x)\n",
        "    triangles = blockwise_sparse_get_diag(x) / 2\n",
        "    return triangles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "homogeneous_data = data.to_homogeneous()\n",
        "#triangles = undirected_triangle_count(homogeneous_data.edge_index, homogeneous_data.x.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wdwwdwdw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ud = to_undirected(homogeneous_data.edge_index)\n",
        "adj_matrix = SparseTensor(row=ud[0], col=ud[1], sparse_sizes=(homogeneous_data.x.shape[0], homogeneous_data.x.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "blockwise_sparse_mmul(adj_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wdwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "skill_job_edges[['job_src','skill_dst']].job_src.unique().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "edge_index = torch.tensor([[0, 0, 1, 2, 3],\n",
        "                           [0, 1, 0, 3, 3]])\n",
        "batch = torch.tensor([0, 0, 1, 1])\n",
        "to_dense_adj(edge_index, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7tf8glw9uda"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZAF_riX9uda"
      },
      "outputs": [],
      "source": [
        "data.has_isolated_nodes(), data.has_self_loops()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5MjvjO09uda"
      },
      "outputs": [],
      "source": [
        "#data = data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMAfCM0u9uda"
      },
      "outputs": [],
      "source": [
        "import torch_geometric.transforms as T\n",
        "\n",
        "transform = T.Compose([\n",
        "       T.RemoveIsolatedNodes(),\n",
        "       T.RemoveDuplicatedEdges(),\n",
        "       T.ToUndirected(merge=False) # don't merge reversed edges into the original edge type\n",
        "])\n",
        "\n",
        "data = transform(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk4D68v69udb"
      },
      "outputs": [],
      "source": [
        "transform = T.RandomLinkSplit(\n",
        "    is_undirected=True,\n",
        "    edge_types=[\n",
        "        ('Job', 'REQUIRES', 'Skill'),\n",
        "        ('Skill', 'IS_SIMILAR_SKILL', 'Skill'),\n",
        "        ('Job', 'IS_SIMILAR_JOB', 'Job')\n",
        "        ],\n",
        "    # rev_edge_types=[\n",
        "    #     ('Skill', 'rev_REQUIRES', 'Job'),\n",
        "    #     ('Skill', 'rev_IS_SIMILAR_SKILL', 'Skill'),\n",
        "    #     ('Job', 'rev_IS_SIMILAR_JOB', 'Job')\n",
        "    # ],\n",
        "    num_val=0.001,\n",
        "    num_test=0.001,\n",
        "    add_negative_train_samples=False, # only adds neg samples for val and test, neg train are added by LinkNeighborLoader. This means for each train batch, negs. are different, for val and train they stay the same\n",
        "    neg_sampling_ratio=1.0,\n",
        "    disjoint_train_ratio=0 #  training edges are shared for message passing and supervision\n",
        "\n",
        "    )\n",
        "train_data, val_data, test_data = transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG7w63019udb"
      },
      "outputs": [],
      "source": [
        "# from torch_geometric.loader import NeighborLoader\n",
        "\n",
        "# train_loader = NeighborLoader(\n",
        "#     train_data,\n",
        "#     # Sample 15 neighbors for each node and each edge type for 2 iterations:\n",
        "#     num_neighbors={\n",
        "#          ('Job', 'REQUIRES', 'Skill'):[1000,10], # [add x neighbors, add y neighbors for every x neighbor]\n",
        "#          ('Skill', 'rev_REQUIRES', 'Job'):[10,0],\n",
        "#         ('Skill', 'IS_SIMILAR_SKILL', 'Skill'):[10,10],\n",
        "#         ('Skill', 'rev_IS_SIMILAR_SKILL', 'Skill'):[0,0],\n",
        "#         ('Job', 'IS_SIMILAR_JOB', 'Job'):[0,20], # can't sample job-job in first iteration\n",
        "#         ('Job', 'rev_IS_SIMILAR_JOB', 'Job'):[0,20],\n",
        "#          },\n",
        "#     # num_neighbors = [10,10],\n",
        "#     # Use a batch size of 128 for sampling training nodes of type \"paper\":\n",
        "#     batch_size=200,\n",
        "#     input_nodes='Job', #if not set, we consider all nodes\n",
        "#     shuffle=True,\n",
        "#     drop_last=True,\n",
        "#     num_workers=4,\n",
        "#     directed=True,  # contains only edges which are followed randomly, False: contains full node induced subgraph\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1n7iu1YK9udb"
      },
      "outputs": [],
      "source": [
        "from itertools import cycle\n",
        "from typing import Tuple, List, Union\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from torch_geometric.sampler import NegativeSampling\n",
        "\n",
        "def create_loader(data:HeteroData, edge:Tuple[str,str,str], num_neighbors:List[int], batch_size:int, is_training:bool)->LinkNeighborLoader:\n",
        "\n",
        "    print('create mini-batches for', edge)\n",
        "\n",
        "    negative_sampling = NegativeSampling(\n",
        "        mode='binary',\n",
        "        amount=20  # ratio, like Graphsage\n",
        "        #weight=  # \"Probabilities\" of nodes to be sampled: Node degree follows power law distribution\n",
        "        )\n",
        "\n",
        "    loader = LinkNeighborLoader(\n",
        "        data,\n",
        "        num_neighbors={\n",
        "            ('Job', 'REQUIRES', 'Skill'):num_neighbors,\n",
        "            ('Skill', 'rev_REQUIRES', 'Job'):num_neighbors,\n",
        "            ('Skill', 'IS_SIMILAR_SKILL', 'Skill'):num_neighbors, # In this example, index 0 will never be used, since neighboring edge to a job node can't be a skill-skill edge\n",
        "            ('Skill', 'rev_IS_SIMILAR_SKILL', 'Skill'):num_neighbors,\n",
        "            ('Job', 'IS_SIMILAR_JOB', 'Job'):num_neighbors,\n",
        "            ('Job', 'rev_IS_SIMILAR_JOB', 'Job'):num_neighbors,\n",
        "        },\n",
        "        edge_label_index=(edge, None), # None means all edges are considered\n",
        "        #edge_label =train_data[edge].edge_label,\n",
        "        neg_sampling=negative_sampling, # adds negative samples\n",
        "        batch_size=batch_size,\n",
        "        shuffle=is_training,\n",
        "        #drop_last=True,\n",
        "        num_workers=2,\n",
        "        directed=True,  # contains only edges which are followed, False: contains full node induced subgraph\n",
        "        #disjoint=True # sampled seed node creates its own, disjoint from the rest, subgraph, will add \"batch vector\" to loader output\n",
        "    )\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "batch_size=256\n",
        "num_neighbors = [5,2]\n",
        "\n",
        "train_loaders, val_loaders, test_loaders = [], [], []\n",
        "for edge_type in train_data.edge_types:\n",
        "    # create mini-batches for each edge type, because LinkNeighborLoader only allows one target edge type\n",
        "\n",
        "    datasets = {\n",
        "        'train':train_data,\n",
        "        'val': val_data,\n",
        "        'test': test_data\n",
        "    }\n",
        "    loader = create_loader(\n",
        "        data=train_data,\n",
        "        edge=edge_type,\n",
        "        num_neighbors=num_neighbors,\n",
        "        batch_size=batch_size,\n",
        "        is_training=True\n",
        "    )\n",
        "    train_loaders.append(loader)\n",
        "\n",
        "    loader = create_loader(\n",
        "        data=val_data,\n",
        "        edge=edge_type,\n",
        "        num_neighbors=num_neighbors,\n",
        "        batch_size=batch_size,\n",
        "        is_training=False\n",
        "    )\n",
        "\n",
        "    val_loaders.append(loader)\n",
        "\n",
        "    loader = create_loader(\n",
        "        data=test_data,\n",
        "        edge=edge_type,\n",
        "        num_neighbors=num_neighbors,\n",
        "        batch_size=batch_size,\n",
        "        is_training=False\n",
        "    )\n",
        "\n",
        "    test_loaders.append(loader)\n",
        "\n",
        "def combined_iterator(iterables):\n",
        "  # creates an iterator which has as many elements as the longest iterable\n",
        "  # other iterables will be repeated until the longest is done\n",
        "  length = 0\n",
        "  index = 0\n",
        "  for i, iterable in enumerate(iterables):\n",
        "    l = len(iterable)\n",
        "    if l>length:\n",
        "      length = l\n",
        "      index = i\n",
        "\n",
        "  longest_iterable = iterables.pop(index)\n",
        "  iterators = [longest_iterable] + [cycle(it) for it in iterables]\n",
        "  return zip(*iterables)\n",
        "\n",
        "\n",
        "train_iterator = combined_iterator(train_loaders)\n",
        "val_iterator = combined_iterator(val_loaders)\n",
        "test_iterator = combined_iterator(test_loaders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnWJyyeC9udb"
      },
      "outputs": [],
      "source": [
        "# helpful article\n",
        "# https://medium.com/stanford-cs224w/a-tour-of-pygs-data-loaders-9f2384e48f8f\n",
        "\n",
        "# some info\n",
        "\n",
        "# HeteroData(\n",
        "#   Job={\n",
        "#     x=[9222, 128], # node features\n",
        "#     n_id=[9222] # the ids of the nodes in the original train_data set\n",
        "#   },\n",
        "#   (Job, REQUIRES, Skill)={\n",
        "#     edge_index=[2, 14498], # sampled edges\n",
        "#     edge_attr=[14498, 1],  # edge attributes of sampled edges\n",
        "#     edge_label=[509170], # 1 if it is a true edge, 0 if it is a false\n",
        "#     edge_label_index=[2, 509170], # all edges?\n",
        "#     e_id=[14498] # edge ids of edges in the original train_data set\n",
        "\n",
        "\n",
        "\n",
        "# if batchsize is 16 for the edge and we have neg_sampling=binary, we will have\n",
        "# this many jobs:\n",
        "#  Job={\n",
        "#     x=[64, 128],\n",
        "#     n_id=[64]\n",
        "#   },\n",
        "# since we sample a negative and a positive edge each, and each edge has 2 Job nodes (if our target is the job nodes)\n",
        "\n",
        "# LinkNeighborloader will sample negative edges for the target edges only, as we expect it\n",
        "# so for the \"neighbor\"-edges we get only positive ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI4AQ3kq9udc"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Union\n",
        "from torch import Tensor\n",
        "from torch_geometric.nn import to_hetero, HeteroDictLinear, Linear\n",
        "from torch_geometric.nn.conv import GraphConv, SAGEConv, SimpleConv\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PyG does not implement the exact max pooling aggregation as in the GraphSage paper\n",
        "# with GraphConvWithPool we manually extend it by adding a linear layer on x before .propagate\n",
        "# as our activation function is monotonically increasing, this modification corresponds to the max pooling aggregation\n",
        "\n",
        "class GraphConvWithPool(GraphConv):\n",
        "    def __init__(self, in_channels, out_channels: int, aggr: str = 'add', bias: bool = True, **kwargs):\n",
        "        super().__init__(in_channels, out_channels, aggr, bias, **kwargs)\n",
        "        self.linear = torch.nn.Linear(in_channels, in_channels, bias=False)\n",
        "\n",
        "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
        "                edge_weight: OptTensor = None, size: Size = None) -> Tensor:\n",
        "\n",
        "        if isinstance(x, Tensor):\n",
        "            x: OptPairTensor = (x, x)\n",
        "\n",
        "        x = self.linear(x) # added this\n",
        "\n",
        "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
        "                             size=size)\n",
        "        out = self.lin_rel(out)\n",
        "\n",
        "        x_r = x[1]\n",
        "        if x_r is not None:\n",
        "            out = out + self.lin_root(x_r)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class WeightedSkillSage(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels, aggregator='max'):\n",
        "        super().__init__()\n",
        "        #self.linear1 = Linear(-1,-1)\n",
        "        #self.conv1 = SimpleConv(aggr='sum')\n",
        "        self.conv1 = GraphConv(in_channels=-1, out_channels=hidden_channels)\n",
        "        self.conv2 = GraphConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
        "        self.linear3 = Linear(hidden_channels,out_channels)\n",
        "\n",
        "    def forward(self, x: HeteroData, edge_index, edge_weight):\n",
        "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        return x\n",
        "\n",
        "model = WeightedSkillSage(hidden_channels=64, out_channels=64)\n",
        "model = to_hetero(model, train_data.metadata(), aggr='sum')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQbpUFGwF-Zx"
      },
      "outputs": [],
      "source": [
        "from torcheval.metrics import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinaryAUPRC\n",
        "\n",
        "class GNNTrainer(Trainer):\n",
        "    def __init__(self, model, criterion, optimizer, device):\n",
        "        super().__init__(model, criterion, optimizer, device, metrics=['f1','accuracy','precision','recall', 'aucpr'])\n",
        "\n",
        "    def get_supervision_edge_type(self, heterodata):\n",
        "        for edge_type in heterodata.edge_types:\n",
        "            if 'input_id' in heterodata[edge_type].keys():\n",
        "                return edge_type\n",
        "\n",
        "    def calculate_metrics(self, split_name, y_hat, y):\n",
        "        y = y.to(torch.int)\n",
        "        acc, prec, rec, f1, aucpr = BinaryAccuracy(threshold=0.5).update(y_hat, y).compute().item(), BinaryPrecision(threshold=0.5).update(y_hat, y).compute().item(), BinaryRecall(threshold=0.5).update(y_hat, y).compute().item(), BinaryF1Score(threshold=0.5).update(y_hat, y).compute().item(), BinaryAUPRC().update(y_hat, y).compute().item()\n",
        "        '''  self.metrics_history[split_name]['accuracy'].append(acc)\n",
        "        self.metrics_history[split_name]['precision'].append(prec)\n",
        "        self.metrics_history[split_name]['recall'].append(rec)\n",
        "        self.metrics_history[split_name]['f1'].append(f1)\n",
        "        self.metrics_history[split_name]['aucpr'].append(aucpr) '''\n",
        "        print(f'{split_name}: F1: {f1}, AUC-PR: {aucpr}, (acc: {acc}, prec: {prec}, rec: {rec})')\n",
        "\n",
        "    def train(self, train_iterator, val_iterator, start_epoch, n_epochs, save_interval, save_path):\n",
        "        self.free_memory()\n",
        "\n",
        "        self.model.train()\n",
        "        for epoch in range(start_epoch, start_epoch+n_epochs):\n",
        "            print(f'=============== Epoch {epoch} ===============')\n",
        "            for batch_idx, edge_batches in enumerate(train_iterator):\n",
        "                self.optimizer.zero_grad()\n",
        "                minibatch_loss = 0\n",
        "\n",
        "                y_hat, y = [], []\n",
        "                for i,batch in enumerate(edge_batches):  # each batch here is one edge type, since we want to learn for all edge types\n",
        "                    print(i,end='\\r')\n",
        "                    batch = batch.to(self.device)\n",
        "                    hetero_out = model(batch.x_dict, batch.edge_index_dict, batch.edge_weight_dict)  # get model output\n",
        "\n",
        "                    # evaluate, calculate cosine sim and compute cross-entropy loss\n",
        "                    supervision_edge_type = self.get_supervision_edge_type(batch)\n",
        "                    src_type, dst_type = supervision_edge_type[0], supervision_edge_type[2]\n",
        "                    edge_label = batch[supervision_edge_type].edge_label\n",
        "                    edge_label_index = batch[supervision_edge_type].edge_label_index\n",
        "                    src_node_embeddings = hetero_out[src_type][edge_label_index[0]]\n",
        "                    dst_node_embeddings = hetero_out[dst_type][edge_label_index[1]]\n",
        "                    logits = F.cosine_similarity(src_node_embeddings, dst_node_embeddings, dim=-1)\n",
        "                    loss = self.criterion(logits, edge_label)\n",
        "                    minibatch_loss += loss\n",
        "\n",
        "                    y_hat.append(torch.sigmoid(logits))\n",
        "                    y.append(edge_label)\n",
        "\n",
        "                minibatch_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # save loss and metrics\n",
        "                self.metrics_history['train']['minibatch'].append(epoch+batch_idx)\n",
        "                self.metrics_history['train']['epoch'].append(epoch+batch_idx)\n",
        "                self.metrics_history['train']['loss'].append(minibatch_loss.item())\n",
        "\n",
        "                y_hat = torch.cat(y_hat)\n",
        "                y = torch.cat(y)\n",
        "                print(\"aa\")\n",
        "                self.calculate_metrics('train', y_hat, y)\n",
        "\n",
        "                if batch_idx % save_interval == 0:\n",
        "                    print('bb')\n",
        "                    self.free_memory()\n",
        "                    self.validate(val_iterator, epoch)\n",
        "                    self.save_checkpoint(batch_idx, save_path)\n",
        "\n",
        "                print(f'Mini-Batch {batch_idx}, Loss: {loss}')\n",
        "\n",
        "    def validate(self, val_iterator, epoch):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "          y_hat, y = [], []\n",
        "          for edge_batches in tqdm(val_iterator):\n",
        "\n",
        "              for batch in edge_batches:  # each batch here is one edge type, since we want to learn for all edge types\n",
        "\n",
        "                    batch = batch.to(self.device)\n",
        "                    hetero_out = model(batch.x_dict, batch.edge_index_dict, batch.edge_weight_dict)  # get model output\n",
        "\n",
        "                    # evaluate, calculate cosine sim and compute cross-entropy loss\n",
        "                    supervision_edge_type = self.get_supervision_edge_type(batch)\n",
        "                    src_type, dst_type = supervision_edge_type[0], supervision_edge_type[2]\n",
        "                    edge_label = batch[supervision_edge_type].edge_label\n",
        "                    edge_label_index = batch[supervision_edge_type].edge_label_index\n",
        "                    src_node_embeddings = hetero_out[src_type][edge_label_index[0]]\n",
        "                    dst_node_embeddings = hetero_out[dst_type][edge_label_index[1]]\n",
        "                    logits = F.cosine_similarity(src_node_embeddings, dst_node_embeddings, dim=-1)\n",
        "\n",
        "                    y_hat.append(torch.sigmoid(logits))\n",
        "                    y.append(edge_label)\n",
        "\n",
        "\n",
        "          # save loss and metrics\n",
        "          self.metrics_history['val']['epoch'].append(epoch)\n",
        "\n",
        "          y_hat = torch.cat(y_hat)\n",
        "          y = torch.cat(y)\n",
        "          self.calculate_metrics('val', y_hat, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsOdxQu-FgBD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "trainer = GNNTrainer(model, criterion, optimizer, device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainer.train(train_iterator, val_iterator, start_epoch=0, n_epochs=10, save_interval=1, save_path='./checkpoints')\n",
        "# trainer.validate(val_dataloader)\n",
        "# trainer.plot_losses()\n",
        "# trainer.load_checkpoint('./checkpoints/checkpoint_100.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9hY5T9f9udc"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
        "# with torch.no_grad():  # Initialize lazy modules.\n",
        "#      out = model(batch.x_dict, batch.edge_index_dict, batch.edge_weight_dict)\n",
        "def get_supervision_edge_type(heterodata):\n",
        "    for edge_type in heterodata.edge_types:\n",
        "        if 'input_id' in heterodata[edge_type].keys():\n",
        "            return edge_type\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "model.train()\n",
        "for edge_batches in train_iterator:\n",
        "    minibatch_loss = 0\n",
        "\n",
        "\n",
        "    # each batch here is one edge type, since we want to learn for all edge types\n",
        "    for batch in edge_batches:\n",
        "        batch = batch.to(device)\n",
        "        hetero_out = model(batch.x_dict, batch.edge_index_dict, batch.edge_weight_dict)\n",
        "\n",
        "        supervision_edge_type = get_supervision_edge_type(batch)\n",
        "        src_type, dst_type = supervision_edge_type[0], supervision_edge_type[2]\n",
        "        edge_label = batch[supervision_edge_type].edge_label\n",
        "        edge_label_index = batch[supervision_edge_type].edge_label_index\n",
        "        src_node_embeddings = hetero_out[src_type][edge_label_index[0]]\n",
        "        dst_node_embeddings = hetero_out[dst_type][edge_label_index[1]]\n",
        "        logits = F.cosine_similarity(src_node_embeddings, dst_node_embeddings, dim=-1)\n",
        "        loss = loss_fn(logits, edge_label)\n",
        "        minibatch_loss += loss\n",
        "\n",
        "    minibatch_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('mini-batch loss:',float(batch_loss))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rr-vuVB49udg"
      },
      "outputs": [],
      "source": [
        "supervision_edge_type = get_supervision_edge_type(batch)\n",
        "src_type, dst_type = supervision_edge_type[0], supervision_edge_type[2]\n",
        "edge_label = batch[supervision_edge_type].edge_label\n",
        "edge_label_index = batch[supervision_edge_type].edge_label_index\n",
        "src_node_embeddings = out[src_type][edge_label_index[0]]\n",
        "dst_node_embeddings = out[dst_type][edge_label_index[1]]\n",
        "torch.min(F.cosine_similarity(src_node_embeddings, dst_node_embeddings, dim=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iKpZ0709udg"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoCV-11H9udg"
      },
      "outputs": [],
      "source": [
        "batch[supervision_edge_type].edge_label_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgWVOJMI9udg"
      },
      "outputs": [],
      "source": [
        "J2S = ('Job','REQUIRES','Skill')\n",
        "batch[J2S].edge_label_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ok0V2Mz9udg"
      },
      "outputs": [],
      "source": [
        "batch['Job','REQUIRES','Skill']."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5nrqKUt9udh"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_loaders[0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKBtbRC49udh"
      },
      "outputs": [],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-95MPpfm9udh"
      },
      "outputs": [],
      "source": [
        "batch.edge_weight_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XauGYI6S9udh"
      },
      "outputs": [],
      "source": [
        "index = batch['Job','IS_SIMILAR_JOB','Job'].e_id\n",
        "labels = batch['Job','IS_SIMILAR_JOB','Job'].edge_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJYX09RM9udh"
      },
      "outputs": [],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import OGB_MAG\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "\n",
        "\n",
        "dataset = OGB_MAG(root='./data', preprocess='metapath2vec', transform=T.ToUndirected())\n",
        "data = dataset[0]\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(-1, hidden_channels)\n",
        "        self.conv2 = SAGEConv(-1, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = GNN(hidden_channels=64, out_channels=dataset.num_classes)\n",
        "model = to_hetero(model, data.metadata(), aggr='sum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65-tXuE59udh"
      },
      "outputs": [],
      "source": [
        "#https://colab.research.google.com/drive/1GrAxHyZCZ13jpTkMy9vVO_v_U9nHDdvB#scrollTo=wmiFKI0ovYN4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssoj9sM_9udh"
      },
      "outputs": [],
      "source": [
        "# intially we use this GraphConv layer and aggregate using mean\n",
        "# this layer allows the addition of edge weights: the adjacency matrix simply consists not of 1s and 0s but the corresponding weights\n",
        "#https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GraphConv.html\n",
        "\n",
        "# using max pool\n",
        "# https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.global_max_pool.html#torch_geometric.nn.pool.global_max_pool"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
