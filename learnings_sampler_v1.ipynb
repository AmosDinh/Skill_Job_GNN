{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "# 1. Sample using HGT Sampler as outlined in the paper, using pyg implementations\n",
    "# 2. The sampling is adapted to link prediction, by first sampling random supervision edges of which the nodes create the supervision nodes\n",
    "# a. Dataset is divided across multiple dimensions:\n",
    "#   a.1. Split into Train, Val, Test split (96, 2, 2)\n",
    "#   a.2. Training only: Edges are split into those which are used solely for message passing and those solely used for supervision (80, 20). \n",
    "#        Because an expressive model (HGT) is used, this prevents the model from memorizing supervision edges by their appearance as message passing edges\n",
    "#   a.3. This means Training consists of 96%*80% Message passing Edges, 96%*20% supervision edges, Val contains 2% Supervision Edges, Test contains 2% supervison Edges\n",
    "#   a.4. Validation and Test edges use the Training Message passing Edges as well.\n",
    "# b. For mini-batch sampling in the training phase, first x random edges are sampled as supervision edges. \n",
    "#    For the nodes of these supervision edges, we apply batch-wise HGT Sampling. Due to implementation limitations, for each supervision entity type, the hgt sampling is separate. \n",
    "#    This limitation does not apply for sampled neighbor entity types\n",
    "# during sampling, also the reverse edge of the supervision edge is removed to avoid data leakage\n",
    "\n",
    "\n",
    "# HGT Sampler (See Paper for further reference)\n",
    "# The probablity of a neighbor node s to be sampled depends on the normalized degree of all its edge types connecting it to all source nodes\n",
    "# If neighbor node s is connected to a and b by edge type r, and a has 2 neighbors through edge type r and b has 1 neighbor (node s) through edge type r, \n",
    "# then the sampling probablity of s is (1/2+1)**2 / 2**2, if it were connected through other edge types to the nodes as well, those degrees would be added to the numerator and denominator.\n",
    "# Nodes are sampled without replacement.\n",
    "# This sampling strategy creates more dense mini-batches, because neighbor nodes which are connected to multiple source nodes and by multiple relationship types are sampled more frequently.\n",
    "# Therefore, training is sped up since less node representations have to be computed. Furthermore, as stated in the paper, the sampling method allows to sample a \n",
    "# similar number of neighbors for each edge type, because high-count edge types and low-count edge types are weighted equally. For each neighbor node type T, a fixed number n of nodes is sampled.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler for Heterogeneous Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each node type, add a new edge type only consisting of self loops\n",
    "# this is done to allow HGT to attend to the previous node representations\n",
    "# for node_type in data.node_types:\n",
    "#     data[node_type, 'self_loop', node_type] = torch.cat((torch.arange(data[node_type].num_nodes),torch.arange(data[node_type].num_nodes)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m HeteroData\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mauto\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/__init__.py:1474\u001b[0m\n\u001b[1;32m   1470\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_classes\u001b[39;00m \u001b[39mimport\u001b[39;00m classes\n\u001b[1;32m   1472\u001b[0m \u001b[39m# quantization depends on torch.fx\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[39m# Import quantization\u001b[39;00m\n\u001b[0;32m-> 1474\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m quantization \u001b[39mas\u001b[39;00m quantization\n\u001b[1;32m   1476\u001b[0m \u001b[39m# Import the quasi random sampler\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m quasirandom \u001b[39mas\u001b[39;00m quasirandom\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/quantization/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mquantize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mobserver\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mqconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/quantization/quantize.py:10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flake8: noqa: F401\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mThis file is in the process of migration to `torch/ao/quantization`, and\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mis kept here for compatibility while the migration process is ongoing.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mhere.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mao\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantize\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     _add_observer_,\n\u001b[1;32m     12\u001b[0m     _convert,\n\u001b[1;32m     13\u001b[0m     _get_observer_dict,\n\u001b[1;32m     14\u001b[0m     _get_unique_devices_,\n\u001b[1;32m     15\u001b[0m     _is_activation_post_process,\n\u001b[1;32m     16\u001b[0m     _observer_forward_hook,\n\u001b[1;32m     17\u001b[0m     _propagate_qconfig_helper,\n\u001b[1;32m     18\u001b[0m     _register_activation_post_process_hook,\n\u001b[1;32m     19\u001b[0m     _remove_activation_post_process,\n\u001b[1;32m     20\u001b[0m     _remove_qconfig,\n\u001b[1;32m     21\u001b[0m     add_quant_dequant,\n\u001b[1;32m     22\u001b[0m     convert,\n\u001b[1;32m     23\u001b[0m     prepare,\n\u001b[1;32m     24\u001b[0m     prepare_qat,\n\u001b[1;32m     25\u001b[0m     propagate_qconfig_,\n\u001b[1;32m     26\u001b[0m     quantize,\n\u001b[1;32m     27\u001b[0m     quantize_dynamic,\n\u001b[1;32m     28\u001b[0m     quantize_qat,\n\u001b[1;32m     29\u001b[0m     swap_module,\n\u001b[1;32m     30\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/ao/quantization/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# flake8: noqa: F403\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfake_quantize\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfuse_modules\u001b[39;00m \u001b[39mimport\u001b[39;00m fuse_modules  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfuse_modules\u001b[39;00m \u001b[39mimport\u001b[39;00m fuse_modules_qat  \u001b[39m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mao\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mobserver\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     MovingAverageMinMaxObserver,\n\u001b[1;32m     10\u001b[0m     HistogramObserver,\n\u001b[1;32m     11\u001b[0m     MovingAveragePerChannelMinMaxObserver,\n\u001b[1;32m     12\u001b[0m     FixedQParamsObserver,\n\u001b[1;32m     13\u001b[0m     default_fixed_qparams_range_0to1_observer,\n\u001b[1;32m     14\u001b[0m     default_fixed_qparams_range_neg1to1_observer,\n\u001b[1;32m     15\u001b[0m     _with_args,\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m ABC, abstractmethod\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/ao/quantization/observer.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mao\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     check_min_max_valid, calculate_qmin_qmax, is_per_tensor, is_per_channel, validate_qmin_qmax)\n\u001b[1;32m     18\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdefault_affine_fixed_qparams_observer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mdefault_debug_observer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mUniformQuantizationObserverBase\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m ]\n\u001b[1;32m     52\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39m_PartialWrapper\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/ao/quantization/utils.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mao\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantization\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquant_type\u001b[39;00m \u001b[39mimport\u001b[39;00m QuantType\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfx\u001b[39;00m \u001b[39mimport\u001b[39;00m Node\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparametrize\u001b[39;00m \u001b[39mimport\u001b[39;00m is_parametrized\n\u001b[1;32m     15\u001b[0m NodePattern \u001b[39m=\u001b[39m Union[Tuple[Node, Node], Tuple[Node, Tuple[Node, Node]], Any]\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/fx/__init__.py:83\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mFX is a toolkit for developers to use to transform ``nn.Module``\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39minstances. FX consists of three main components: a **symbolic tracer,**\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mrepository.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgraph_module\u001b[39;00m \u001b[39mimport\u001b[39;00m GraphModule\n\u001b[1;32m     84\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_symbolic_trace\u001b[39;00m \u001b[39mimport\u001b[39;00m symbolic_trace, Tracer, wrap, PH, ProxyableClassMeta\n\u001b[1;32m     85\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m Graph, CodeGen\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/fx/graph_module.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlinecache\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Type, Dict, List, Any, Union, Optional, Set\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgraph\u001b[39;00m \u001b[39mimport\u001b[39;00m Graph, _PyTreeCodeGen, _is_from_torch, _custom_builtins, PythonCode\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_compatibility\u001b[39;00m \u001b[39mimport\u001b[39;00m compatibility\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpackage\u001b[39;00m \u001b[39mimport\u001b[39;00m Importer, sys_importer\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/fx/graph.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mnode\u001b[39;00m \u001b[39mimport\u001b[39;00m Node, Argument, Target, map_arg, _type_repr, _get_qualified_name\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_pytree\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpytree\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _pytree \u001b[39mas\u001b[39;00m fx_pytree\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/fx/node.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfx\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moperator_schemas\u001b[39;00m \u001b[39mimport\u001b[39;00m normalize_function, normalize_module, ArgsKwargsPair\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m ops \u001b[39mas\u001b[39;00m _ops\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/fx/operator_schemas.py:250\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39missubclass\u001b[39m(argument_type, signature_type)\n\u001b[1;32m    244\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m@compatibility\u001b[39m(is_backward_compatible\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalize_function\u001b[39m(\n\u001b[1;32m    248\u001b[0m         target: Callable, args: Tuple[Any], kwargs : Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, arg_types : Optional[Tuple[Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    249\u001b[0m         kwarg_types : Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m--> 250\u001b[0m         normalize_to_only_use_kwargs : \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[ArgsKwargsPair]:\n\u001b[1;32m    251\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[39m    Returns normalized arguments to PyTorch functions. This means that\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[39m    `args/kwargs` will be matched up to the functional's\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m        Returns normalized_args_and_kwargs, or `None` if not successful.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/typing.py:309\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    308\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m         \u001b[39mreturn\u001b[39;00m cached(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    310\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/typing.py:403\u001b[0m, in \u001b[0;36m_SpecialForm.__getitem__\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m@_tp_cache\u001b[39m\n\u001b[1;32m    402\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, parameters):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\u001b[39mself\u001b[39;49m, parameters)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/typing.py:530\u001b[0m, in \u001b[0;36mOptional\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Optional type.\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \n\u001b[1;32m    527\u001b[0m \u001b[39mOptional[X] is equivalent to Union[X, None].\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    529\u001b[0m arg \u001b[39m=\u001b[39m _type_check(parameters, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m requires a single type.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m Union[arg, \u001b[39mtype\u001b[39;49m(\u001b[39mNone\u001b[39;49;00m)]\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/typing.py:309\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    308\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 309\u001b[0m         \u001b[39mreturn\u001b[39;00m cached(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    310\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/typing.py:403\u001b[0m, in \u001b[0;36m_SpecialForm.__getitem__\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39m@_tp_cache\u001b[39m\n\u001b[1;32m    402\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, parameters):\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(\u001b[39mself\u001b[39;49m, parameters)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/typing.py:515\u001b[0m, in \u001b[0;36mUnion\u001b[0;34m(self, parameters)\u001b[0m\n\u001b[1;32m    513\u001b[0m     parameters \u001b[39m=\u001b[39m (parameters,)\n\u001b[1;32m    514\u001b[0m msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUnion[arg, ...]: each arg must be a type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 515\u001b[0m parameters \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(_type_check(p, msg) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m parameters)\n\u001b[1;32m    516\u001b[0m parameters \u001b[39m=\u001b[39m _remove_dups_flatten(parameters)\n\u001b[1;32m    517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(parameters) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "def add_reverse_edge_original_attributes_and_label_inplace(original_edge, reverse_edge):\n",
    "    # add edge label and index and edge attr to reverse edge \n",
    "    if 'edge_attr' in original_edge:\n",
    "        reverse_edge['edge_attr'] = original_edge['edge_attr']\n",
    "    reverse_edge['edge_label'] = original_edge['edge_label']\n",
    "    reverse_edge['edge_label_index'] = original_edge['edge_label_index'].index_select(0, torch.LongTensor([1, 0]))\n",
    "    for key in original_edge.keys():\n",
    "        if key not in ['edge_index', 'edge_attr', 'edge_label', 'edge_label_index']:\n",
    "            reverse_edge[key] = original_edge[key]\n",
    "                \n",
    "    return reverse_edge\n",
    "\n",
    "def get_datasets(get_edge_attr=False, filename=None, filter_top_k=False, top_k=50, remove_text_attr=True):\n",
    "    if filename is None:\n",
    "        filename = 'HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt'\n",
    "    size = os.path.getsize(filename)\n",
    "    print('size of dataset on disk: ', size/1e9, 'gb')\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        data = HeteroData.from_dict(torch.load(filename))\n",
    "        print('loading saved heterodata object')\n",
    "\n",
    "\n",
    "    \n",
    "    def top_k_mask(scores, indices, top_k ):\n",
    "        # Make sure we are using the GPU\n",
    "        scores = scores.cuda()\n",
    "        indices = indices.cuda()\n",
    "        \n",
    "        # Create an empty mask with the same shape as scores\n",
    "        mask = torch.zeros_like(scores, dtype=torch.bool)\n",
    "        # Get the unique indices and their counts\n",
    "        unique_indices, counts = torch.unique(indices, return_counts=True)\n",
    "    \n",
    "        # Indices where count > top_k\n",
    "        large_indices = unique_indices[counts > top_k]\n",
    "    \n",
    "        # Set mask for indices where count <= top_k\n",
    "        mask[~torch.isin(indices,large_indices)] = True\n",
    "        # For indices where count > 50, we only keep top 50 scores\n",
    "        for idx in tqdm(large_indices):\n",
    "            idx_mask = (indices == idx)\n",
    "            values, idxs = scores[idx_mask].topk(top_k)\n",
    "            a = mask[idx_mask]\n",
    "            a[idxs] = True\n",
    "            mask[idx_mask] = a\n",
    "            \n",
    "        return mask.cpu()\n",
    "\n",
    "    \n",
    "   \n",
    "    if filter_top_k:\n",
    "        print('for skill job edges keep top k edges per job, k is ',top_k)\n",
    "        e = ('skills', 'job_skill', 'jobs')\n",
    "        rev_e = (e[2],'rev_'+e[1],e[0])\n",
    "        cache_dir = 'cache'\n",
    "        if not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "        \n",
    "        mask_path = os.path.join(cache_dir, f'mask{top_k}.pt') \n",
    "        \n",
    "        if os.path.isfile(mask_path):\n",
    "            mask = torch.load(mask_path)\n",
    "        else:\n",
    "            mask = top_k_mask(data[e].edge_attr.squeeze(1), data[e].edge_index[1,:], top_k)\n",
    "            torch.save(mask, mask_path) \n",
    "        \n",
    "        data[e].edge_attr = data[e].edge_attr[mask]\n",
    "        data[rev_e].edge_attr = data[rev_e].edge_attr[mask]\n",
    "        data[e].edge_index = data[e].edge_index[:,mask]\n",
    "        data[rev_e].edge_index = data[rev_e].edge_index[:,mask]\n",
    "        print('keep',torch.sum(mask), 'of total',mask.shape[0])\n",
    "    \n",
    "    \n",
    "    from torch_geometric import seed_everything\n",
    "    import torch_geometric.transforms as T\n",
    "    from torch_geometric.utils import sort_edge_index\n",
    "\n",
    "\n",
    "    \n",
    "    edge_types = []\n",
    "    rev_edge_types = []\n",
    "    for edge_type in data.edge_types:\n",
    "        if edge_type[1].startswith('rev_'):\n",
    "            rev_edge_types.append(edge_type)\n",
    "        else:\n",
    "            edge_types.append(edge_type)\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        is_undirected=True,\n",
    "        edge_types=edge_types,\n",
    "        rev_edge_types=rev_edge_types,\n",
    "        num_val=0.02,\n",
    "        num_test=0.05,\n",
    "        add_negative_train_samples=False, # only adds neg samples for val and test, neg train are added by LinkNeighborLoader. This means for each train batch, negs. are different, for val and train they stay the same\n",
    "        neg_sampling_ratio=1.0,\n",
    "        disjoint_train_ratio=0.3, #  training edges are shared for message passing and supervision\n",
    "        )\n",
    "\n",
    "    seed_everything(14)\n",
    "    # sort by col to speed up sampling later (we can sepcify is_sorted=True in link neighbor loader)\n",
    "    # we actually dont use the sort, because it seems to mess up things, but have not checked if everything works without sorting, so we leave it here\n",
    "    def sort_edges(data):\n",
    "        for edge_type in data.edge_types:\n",
    "            if 'edge_attr' in data[edge_type].keys():\n",
    "                data[edge_type].edge_index, data[edge_type].edge_attr = sort_edge_index(data[edge_type].edge_index, data[edge_type].edge_attr, sort_by_row=False) \n",
    "            else:\n",
    "                data[edge_type].edge_index = sort_edge_index(data[edge_type].edge_index, sort_by_row=False) \n",
    "        return data\n",
    "    \n",
    "\n",
    "    def preprocess(data):\n",
    "        if not get_edge_attr:\n",
    "            # delete edge_attr of every edge type\n",
    "            for edge_type in data.edge_types:\n",
    "                del data[edge_type].edge_attr \n",
    "\n",
    "        # delete all keys for every node type except 'x' (e.g. description and title)\n",
    "        for node_type in data.node_types:\n",
    "            keys = list(data[node_type].keys())\n",
    "            for key in keys:\n",
    "                if key != 'x':\n",
    "                    del data[node_type][key]\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    # change all types to float32 and normalize the triangle columns\n",
    "    for node_type in data.node_types:\n",
    "        for i in range(data[node_type].x.shape[1]):\n",
    "            if data[node_type].x[:,i].max()>5:\n",
    "                #normalize\n",
    "                print('normalizing column ', i, ' of node type ', node_type)\n",
    "                data[node_type].x[:,i] = data[node_type].x[:,i]/data[node_type].x[:,i].max()\n",
    "        \n",
    "        data[node_type].x = data[node_type].x.to(torch.float32)\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    #train_data = sort_edges(train_data)\n",
    "    #val_data = sort_edges(val_data)\n",
    "    #test_data = sort_edges(test_data)\n",
    "    if remove_text_attr:\n",
    "        train_data = preprocess(train_data)\n",
    "        val_data = preprocess(val_data)\n",
    "        test_data = preprocess(test_data)\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.loader import HGTLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "\n",
    "def get_hgt_linkloader(data, target_edge, batch_size, is_training, sampling_mode, neg_ratio, num_neighbors_hgtloader, num_workers, prefetch_factor, pin_memory):\n",
    "    # first sample some edges in linkNeighborLoader\n",
    "    # use the nodes of the sampled edges to sample from hgt loader\n",
    "    \n",
    "    \n",
    "    num_neighbors_linkloader = [0]\n",
    "    #for edge_type in data.edge_types:\n",
    "    #    num_neighbors_linkloader[edge_type] = [0,0]\n",
    "    \n",
    "    negative_sampling = NegativeSampling(\n",
    "        mode=sampling_mode, # binary or triplet\n",
    "        amount=neg_ratio  # ratio, like Graphsage # 10\n",
    "        #weight=  # \"Probabilities\" of nodes to be sampled: Node degree follows power law distribution\n",
    "        )\n",
    "    \n",
    "    if sampling_mode == 'triplet':\n",
    "        data[target_edge].edge_label = None\n",
    "        \n",
    "\n",
    "    linkNeighborLoader = LinkNeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=num_neighbors_linkloader,\n",
    "            edge_label_index=(target_edge, data[target_edge].edge_label_index), # if (edge, None), None means all edges are considered\n",
    "        \n",
    "            neg_sampling=negative_sampling, # adds negative samples\n",
    "            batch_size=batch_size,\n",
    "            shuffle=is_training, #is_training\n",
    "            subgraph_type='directional', # contains only sampled edges\n",
    "            #drop_last=True,\n",
    "            num_workers=num_workers,\n",
    "            #disjoint=True # sampled seed node creates its own, disjoint from the rest, subgraph, will add \"batch vector\" to loader output\n",
    "        \n",
    "            #num_workers=2,\n",
    "            #prefetch_factor=2\n",
    "            is_sorted = False,\n",
    "            pin_memory=pin_memory,\n",
    "            prefetch_factor=prefetch_factor,\n",
    "    )\n",
    "   \n",
    "   \n",
    "    def get_hgt(data, input_nodetype, input_mask):\n",
    "        return next(iter(HGTLoader(\n",
    "                data,\n",
    "                # Sample 512 nodes per type and per iteration for 4 iterations\n",
    "                num_samples=num_neighbors_hgtloader,\n",
    "                batch_size=input_mask.shape[0],\n",
    "                input_nodes=(input_nodetype, input_mask),\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=pin_memory,\n",
    "                prefetch_factor=prefetch_factor,\n",
    "            )))\n",
    "        \n",
    "    \n",
    "    def add_self_loops(data):\n",
    "        for node_type in data.node_types:\n",
    "            data[node_type, 'self_loop', node_type].edge_index = torch.arange(data[node_type].num_nodes).repeat(2,1)\n",
    "        return data \n",
    "\n",
    "            \n",
    "    def get_hgt_with_selfloops(loader):\n",
    "        \n",
    "        \n",
    "        for batch in loader:   \n",
    "            if sampling_mode=='triplet':      \n",
    "                # original edge_label_index from the whole data object\n",
    "                unmapped_batchids = torch.cat((batch[target_edge[0]].src_index,batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index.flatten()))\n",
    "                original_node_ids = batch[target_edge[0]].n_id[unmapped_batchids]\n",
    "                original_edge_label_nodes = torch.LongTensor(original_node_ids.unique())\n",
    "\n",
    "                #remapping or sorting is not needed, since nodes are sorted, also in the htg batch, the edges will be the same\n",
    "                src = batch[target_edge[0]].src_index.unsqueeze(0)\n",
    "                src_total = src\n",
    "                for i in range(neg_ratio):\n",
    "                    src_total = torch.cat((src_total,src), dim=1)\n",
    "                dst = torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index.flatten()),dim=0).unsqueeze(0)\n",
    "                \n",
    "                local_edge_label_index = torch.cat((src_total, dst),dim=0)\n",
    "                edge_label = torch.cat((torch.ones(batch[target_edge[2]].dst_pos_index.shape[0]), torch.zeros(batch[target_edge[2]].dst_neg_index.flatten().shape[0])))\n",
    "                \n",
    "            elif sampling_mode=='binary':\n",
    "                unmapped_batchids = batch[target_edge].edge_label_index.flatten()\n",
    "                original_node_ids = batch[target_edge[0]].n_id[unmapped_batchids]\n",
    "                original_edge_label_nodes = torch.LongTensor(original_node_ids.unique())\n",
    "\n",
    "            else:\n",
    "                raise Exception('binary or triplet sampling mode')\n",
    "                \n",
    "                \n",
    "            hgt_batch = get_hgt(data, target_edge[0], original_edge_label_nodes) # 0,1,3,4,5,6,7,8,9,\n",
    "          \n",
    "            if sampling_mode=='triplet':\n",
    "                \n",
    "                # return message passing edges, and supervision edges/labels, ignore labels/label_indices in the message passing edges\n",
    "                yield add_self_loops(hgt_batch), local_edge_label_index, edge_label, batch[target_edge].input_id, original_node_ids\n",
    "            else: # sampling_mode=='binary':\n",
    "                # return message passing edges, and supervision edges/labels, ignore labels/label_indices in the message passing edges, as well as original edge indices\n",
    "                yield add_self_loops(hgt_batch), batch[target_edge].edge_label_index, batch[target_edge].edge_label, batch[target_edge].input_id, original_node_ids\n",
    "    \n",
    "    def get_hgt_2types_with_selfloops(loader):\n",
    "        for batch in loader:\n",
    "            if sampling_mode=='triplet':   \n",
    "                original_src_nodes = batch[target_edge[0]].n_id[batch[target_edge[0]].src_index]\n",
    "                original_edge_label_nodes_class1 = torch.LongTensor(original_src_nodes.unique())\n",
    "                \n",
    "                original_dst_nodes = batch[target_edge[2]].n_id[torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index.flatten()))]\n",
    "                original_edge_label_nodes_class2 = torch.LongTensor(original_dst_nodes.unique())\n",
    "                \n",
    "                \n",
    "                src = batch[target_edge[0]].src_index.unsqueeze(0)\n",
    "                src_total = src\n",
    "                for i in range(neg_ratio):\n",
    "                    src_total = torch.cat((src_total,src), dim=1)\n",
    "                \n",
    "                dst = torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index.flatten()),dim=0).unsqueeze(0)\n",
    "                \n",
    "                local_edge_label_index = torch.cat((src_total, dst),dim=0)\n",
    "                edge_label = torch.cat((torch.ones(batch[target_edge[2]].dst_pos_index.shape[0]), torch.zeros(batch[target_edge[2]].dst_neg_index.flatten().shape[0])))\n",
    "\n",
    "            elif sampling_mode=='binary':\n",
    "                original_src_nodes = batch[target_edge[0]].n_id[batch[target_edge].edge_label_index[0,:]]\n",
    "                original_edge_label_nodes_class1 = original_src_nodes.unique()\n",
    "                original_dst_nodes = batch[target_edge[2]].n_id[batch[target_edge].edge_label_index[1,:]]\n",
    "                original_edge_label_nodes_class2 = original_dst_nodes.unique()\n",
    "\n",
    "            else:\n",
    "                raise Exception('binary or triplet sampling mode')\n",
    "\n",
    "            # batch the start and end supervision nodes separately\n",
    "            hgt_batch1 = get_hgt(data, target_edge[0], original_edge_label_nodes_class1)\n",
    "            hgt_batch2 = get_hgt(data, target_edge[2], original_edge_label_nodes_class2)\n",
    "            \n",
    "            \n",
    "            # ** We dont need to remove any edges ** since the supervision edges wont be sampled by hgt\n",
    "            if sampling_mode=='triplet':\n",
    "                yield add_self_loops(hgt_batch1), add_self_loops(hgt_batch2), local_edge_label_index, edge_label, batch[target_edge].input_id, original_src_nodes, original_dst_nodes\n",
    "            else: # sampling_mode=='binary':\n",
    "                # we can access the corresponding nodes of edge_label_index[0,:] in hgt_batch1[target_edge[0]], those of [1,:] in hgt_batch2...\n",
    "                yield add_self_loops(hgt_batch1), add_self_loops(hgt_batch2), batch[target_edge].edge_label_index, batch[target_edge].edge_label, batch[target_edge].input_id, original_src_nodes, original_dst_nodes\n",
    "\n",
    "        \n",
    "    if target_edge[0] == target_edge[2]:\n",
    "        # same edge type, only need to sample once\n",
    "        return get_hgt_with_selfloops(linkNeighborLoader)\n",
    "    else:\n",
    "        return get_hgt_2types_with_selfloops(linkNeighborLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, val_data, test_data = get_datasets(get_edge_attr=False, filter_top_k=True, top_k=50)\n",
    "# testing\n",
    "#input_edgetype = ('jobs', 'job_job', 'jobs')\n",
    "\n",
    "#loader = get_hgt_linkloader(train_data, input_edgetype, 4, True, 'triplet', 1, [10], num_workers=0, prefetch_factor=None, pin_memory=False)\n",
    "#minibatch, edge_label_index, edge_label, input_edge_ids = next(iter(loader))\n",
    "#minibatch\n",
    "#input_edgetype = ('skills', 'qualification_skill', 'qualifications')\n",
    "#loader = get_hgt_linkloader(train_data, input_edgetype, 10, True, 'triplet', 1, [10], num_workers=0)\n",
    "#minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id = next(iter(loader))\n",
    "#input_edge_id\n",
    "\n",
    "\n",
    "# add reverse edge\n",
    "#add_reverse_edge_original_attributes_and_label_inplace(train_data['courses_and_programs', 'course_and_programs_student', 'people'], reverse_edge=train_data['people', 'rev_course_and_programs_student', 'courses_and_programs'] )\n",
    "#from tqdm.auto import tqdm\n",
    "#input_edgetype = ('people', 'rev_course_and_programs_student', 'courses_and_programs')\n",
    "#loader = get_hgt_linkloader(train_data, input_edgetype, 4, True, 'triplet', 1, [10], num_workers=0, prefetch_factor=None, pin_memory=False)\n",
    "#for batch in tqdm(loader, total=get_single_minibatch_count(train_data, 4, input_edgetype)):\n",
    "#    \n",
    "#    pass # courses_and_programs, course_and_programs_student, people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_minibatch_count(data, batch_size):\n",
    "    batches = []\n",
    "    for edge_type in data.edge_types:\n",
    "        if edge_type[1].startswith('rev_'):\n",
    "            continue\n",
    "        batches.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
    "        \n",
    "    return len(batches)\n",
    "\n",
    "def get_single_minibatch_count(data, batch_size, edge_type):\n",
    "    return (data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size\n",
    "\n",
    "def uniform_hgt_sampler(data, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory):\n",
    "    # return batches from all edgetypes with each \"edge\" being drawn uniformly at random (but we translate the probabilities to batches), last batches of each edge type may be smaller than batch_size\n",
    "    batches = []\n",
    "    loaders = {}\n",
    "    # only the non-reverse edge types for now\n",
    "    for edge_type in data.edge_types:\n",
    "        if edge_type[1].startswith('rev_'):\n",
    "            continue\n",
    "        batches.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
    "        loaders[edge_type]=get_hgt_linkloader(data, edge_type, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory)\n",
    "        \n",
    "    random.seed(14)\n",
    "    random.shuffle(batches)\n",
    "    # set a random random seed again (may affect creating the loaders later for a second epoch)\n",
    "    random.seed()\n",
    "    \n",
    "    print('total batches:', len(batches))\n",
    "    \n",
    "    for target_edge_type in batches:\n",
    "        if target_edge_type[0] == target_edge_type[2]:\n",
    "            same_nodetype = True\n",
    "        else:\n",
    "            same_nodetype = False\n",
    "        try:\n",
    "            minibatch = next(loaders[target_edge_type])\n",
    "        except StopIteration: # \"reinit\" iterator\n",
    "            loaders[target_edge_type] = iter(loaders[target_edge_type])\n",
    "        yield same_nodetype, target_edge_type, minibatch\n",
    "\n",
    "def equal_edgeweight_hgt_sampler(data, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers, prefetch_factor, pin_memory):\n",
    "    batchcount=[]\n",
    "    batches=[]\n",
    "    for edge_type in data.edge_types:\n",
    "        if edge_type[1].startswith('rev_'):\n",
    "            continue\n",
    "        batchcount.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
    "        batches.append(edge_type)\n",
    "    print('total batches:', len(batchcount))\n",
    "\n",
    "    batches = random.choices(batches, weights=weights, k=len(batchcount)) \n",
    "    # set a random random seed again (may affect creating the loaders later for a second epoch)\n",
    "    random.seed()\n",
    "    \n",
    "   \n",
    "    \n",
    "    for target_edge_type in batches:\n",
    "        if target_edge_type[0] == target_edge_type[2]:\n",
    "            same_nodetype = True\n",
    "        else:\n",
    "            same_nodetype = False\n",
    "        try:\n",
    "            minibatch = next(loaders[target_edge_type])\n",
    "        except StopIteration: # \"reinit\" iterator\n",
    "            loaders[target_edge_type] = iter(loaders[target_edge_type])\n",
    "        yield same_nodetype, target_edge_type, minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb Cell 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# num neighbors: sample 25 one hop neighbors and for each one hop neighbor again 10 neighbors\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m num_relationships \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_data\u001b[39m.\u001b[39medge_types)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m one_hop_neighbors \u001b[39m=\u001b[39m (\u001b[39m25\u001b[39m \u001b[39m*\u001b[39m batch_size)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mnum_relationships \u001b[39m# per relationship type\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m two_hop_neighbors \u001b[39m=\u001b[39m (\u001b[39m25\u001b[39m \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m batch_size)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mnum_relationships \u001b[39m# per relationship type\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import datetime\n",
    "    import time\n",
    "\n",
    "    # num neighbors: sample 25 one hop neighbors and for each one hop neighbor again 10 neighbors\n",
    "\n",
    "    batch_size = 32\n",
    "    num_relationships = len(train_data.edge_types)\n",
    "    one_hop_neighbors = (25 * batch_size)//num_relationships # per relationship type\n",
    "    two_hop_neighbors = (25 * 10 * batch_size)//num_relationships # per relationship type\n",
    "    num_neighbors = [one_hop_neighbors, two_hop_neighbors]\n",
    "    print('num_neighbors', num_neighbors)\n",
    "\n",
    "    sampler = uniform_hgt_sampler(train_data, batch_size, True, 'binary', 1, num_neighbors, num_workers=0)\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    print()\n",
    "    for i,(same_nodetype, target_edge_type, batch) in enumerate(sampler):\n",
    "        \n",
    "        # batching is different depending on if node types in edge are same or different\n",
    "        edge_type = batch[-1]\n",
    "        if same_nodetype:\n",
    "            minibatch, edge_label_index, edge_label, input_edge_ids = batch\n",
    "            print(minibatch)\n",
    "        else:\n",
    "            minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id = batch\n",
    "            print(minibatchpart1)\n",
    "            \n",
    "        print(i,target_edge_type)\n",
    "        \n",
    "        break\n",
    "        time.sleep(5)\n",
    "        \n",
    "    end = datetime.datetime.now()\n",
    "    print()\n",
    "    print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
