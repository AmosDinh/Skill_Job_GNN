{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling\n",
    "# 1. Sample using HGT Sampler as outlined in the paper, using pyg implementations\n",
    "# 2. The sampling is adapted to link prediction, by first sampling random supervision edges of which the nodes create the supervision nodes\n",
    "# a. Dataset is divided across multiple dimensions:\n",
    "#   a.1. Split into Train, Val, Test split (96, 2, 2)\n",
    "#   a.2. Training only: Edges are split into those which are used solely for message passing and those solely used for supervision (80, 20). \n",
    "#        Because an expressive model (HGT) is used, this prevents the model from memorizing supervision edges by their appearance as message passing edges\n",
    "#   a.3. This means Training consists of 96%*80% Message passing Edges, 96%*20% supervision edges, Val contains 2% Supervision Edges, Test contains 2% supervison Edges\n",
    "#   a.4. Validation and Test edges use the Training Message passing Edges as well.\n",
    "# b. For mini-batch sampling in the training phase, first x random edges are sampled as supervision edges. \n",
    "#    For the nodes of these supervision edges, we apply batch-wise HGT Sampling. Due to implementation limitations, for each supervision entity type, the hgt sampling is separate. \n",
    "#    This limitation does not apply for sampled neighbor entity types\n",
    "# during sampling, also the reverse edge of the supervision edge is removed to avoid data leakage\n",
    "\n",
    "\n",
    "# HGT Sampler (See Paper for further reference)\n",
    "# The probablity of a neighbor node s to be sampled depends on the normalized degree of all its edge types connecting it to all source nodes\n",
    "# If neighbor node s is connected to a and b by edge type r, and a has 2 neighbors through edge type r and b has 1 neighbor (node s) through edge type r, \n",
    "# then the sampling probablity of s is (1/2+1)**2 / 2**2, if it were connected through other edge types to the nodes as well, those degrees would be added to the numerator and denominator.\n",
    "# Nodes are sampled without replacement.\n",
    "# This sampling strategy creates more dense mini-batches, because neighbor nodes which are connected to multiple source nodes and by multiple relationship types are sampled more frequently.\n",
    "# Therefore, training is sped up since less node representations have to be computed. Furthermore, as stated in the paper, the sampling method allows to sample a \n",
    "# similar number of neighbors for each edge type, because high-count edge types and low-count edge types are weighted equally. For each neighbor node type T, a fixed number n of nodes is sampled.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler for Heterogeneous Graph Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each node type, add a new edge type only consisting of self loops\n",
    "# this is done to allow HGT to attend to the previous node representations\n",
    "# for node_type in data.node_types:\n",
    "#     data[node_type, 'self_loop', node_type] = torch.cat((torch.arange(data[node_type].num_nodes),torch.arange(data[node_type].num_nodes)), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#Z3251sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#Z3251sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#Z3251sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch_geometric\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m HeteroData\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/amos/programming/create_graphds/learnings_sampler_v1.ipynb#Z3251sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_datasets\u001b[39m(get_edge_attr\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/__init__.py:1504\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdlpack\u001b[39;00m \u001b[39mimport\u001b[39;00m from_dlpack, to_dlpack\n\u001b[1;32m   1501\u001b[0m \u001b[39m# Import experimental masked operations support. See\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[39m# [RFC-0016](https://github.com/pytorch/rfcs/pull/27) for more\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[39m# information.\u001b[39;00m\n\u001b[0;32m-> 1504\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m masked\n\u001b[1;32m   1506\u001b[0m \u001b[39m# Import removed ops with error message about removal\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_linalg_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m     matrix_rank,\n\u001b[1;32m   1509\u001b[0m     eig,\n\u001b[1;32m   1510\u001b[0m     solve,\n\u001b[1;32m   1511\u001b[0m     lstsq,\n\u001b[1;32m   1512\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/masked/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmaskedtensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m is_masked_tensor, MaskedTensor\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmaskedtensor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcreation\u001b[39;00m \u001b[39mimport\u001b[39;00m as_masked_tensor, masked_tensor\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     _canonical_dim,\n\u001b[1;32m      5\u001b[0m     _generate_docstring,\n\u001b[1;32m      6\u001b[0m     _reduction_identity,\n\u001b[1;32m      7\u001b[0m     _where,\n\u001b[1;32m      8\u001b[0m     _input_mask,\n\u001b[1;32m      9\u001b[0m     _output_mask,\n\u001b[1;32m     10\u001b[0m     _combine_input_and_mask,\n\u001b[1;32m     11\u001b[0m     \u001b[39msum\u001b[39m,\n\u001b[1;32m     12\u001b[0m     prod,\n\u001b[1;32m     13\u001b[0m     cumsum,\n\u001b[1;32m     14\u001b[0m     cumprod,\n\u001b[1;32m     15\u001b[0m     amax,\n\u001b[1;32m     16\u001b[0m     amin,\n\u001b[1;32m     17\u001b[0m     argmax,\n\u001b[1;32m     18\u001b[0m     argmin,\n\u001b[1;32m     19\u001b[0m     mean,\n\u001b[1;32m     20\u001b[0m     median,\n\u001b[1;32m     21\u001b[0m     logsumexp,\n\u001b[1;32m     22\u001b[0m     logaddexp,\n\u001b[1;32m     23\u001b[0m     norm,\n\u001b[1;32m     24\u001b[0m     var,\n\u001b[1;32m     25\u001b[0m     std,\n\u001b[1;32m     26\u001b[0m     softmax,\n\u001b[1;32m     27\u001b[0m     log_softmax,\n\u001b[1;32m     28\u001b[0m     softmin,\n\u001b[1;32m     29\u001b[0m     normalize,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mas_masked_tensor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_masked_tensor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmasked_tensor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mMaskedTensor\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m ]\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/masked/_ops.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmasked\u001b[39;00m \u001b[39mimport\u001b[39;00m as_masked_tensor, is_masked_tensor, MaskedTensor\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _docs\n\u001b[0;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_prims_common\u001b[39;00m \u001b[39mimport\u001b[39;00m corresponding_real_dtype\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m sym_float\n\u001b[1;32m     14\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/torch/_prims_common/__init__.py:23\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfunctools\u001b[39;00m \u001b[39mimport\u001b[39;00m cmp_to_key, reduce\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     Any,\n\u001b[1;32m     12\u001b[0m     Callable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     Union,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msympy\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m sym_float, sym_int, sym_max\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/__init__.py:74\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mlogic\u001b[39;00m \u001b[39mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[1;32m     68\u001b[0m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[1;32m     69\u001b[0m         true, false, satisfiable)\n\u001b[1;32m     71\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39massumptions\u001b[39;00m \u001b[39mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[1;32m     72\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpolys\u001b[39;00m \u001b[39mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[1;32m     75\u001b[0m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[1;32m     76\u001b[0m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[1;32m     77\u001b[0m         subresultants, resultant, discriminant, cofactors, gcd_list, gcd,\n\u001b[1;32m     78\u001b[0m         lcm_list, lcm, terms_gcd, trunc, monic, content, primitive, compose,\n\u001b[1;32m     79\u001b[0m         decompose, sturm, gff_list, gff, sqf_norm, sqf_part, sqf_list, sqf,\n\u001b[1;32m     80\u001b[0m         factor_list, factor, intervals, refine_root, count_roots, real_roots,\n\u001b[1;32m     81\u001b[0m         nroots, ground_roots, nth_power_roots_poly, cancel, reduced, groebner,\n\u001b[1;32m     82\u001b[0m         is_zero_dimensional, GroebnerBasis, poly, symmetrize, horner,\n\u001b[1;32m     83\u001b[0m         interpolate, rational_interpolate, viete, together,\n\u001b[1;32m     84\u001b[0m         BasePolynomialError, ExactQuotientFailed, PolynomialDivisionFailed,\n\u001b[1;32m     85\u001b[0m         OperationNotSupported, HeuristicGCDFailed, HomomorphismFailed,\n\u001b[1;32m     86\u001b[0m         IsomorphismFailed, ExtraneousFactors, EvaluationFailed,\n\u001b[1;32m     87\u001b[0m         RefinementFailed, CoercionFailed, NotInvertible, NotReversible,\n\u001b[1;32m     88\u001b[0m         NotAlgebraic, DomainError, PolynomialError, UnificationFailed,\n\u001b[1;32m     89\u001b[0m         GeneratorsError, GeneratorsNeeded, ComputationFailed,\n\u001b[1;32m     90\u001b[0m         UnivariatePolynomialError, MultivariatePolynomialError,\n\u001b[1;32m     91\u001b[0m         PolificationFailed, OptionError, FlagError, minpoly,\n\u001b[1;32m     92\u001b[0m         minimal_polynomial, primitive_element, field_isomorphism,\n\u001b[1;32m     93\u001b[0m         to_number_field, isolate, round_two, prime_decomp, prime_valuation,\n\u001b[1;32m     94\u001b[0m         galois_group, itermonomials, Monomial, lex, grlex,\n\u001b[1;32m     95\u001b[0m         grevlex, ilex, igrlex, igrevlex, CRootOf, rootof, RootOf,\n\u001b[1;32m     96\u001b[0m         ComplexRootOf, RootSum, roots, Domain, FiniteField, IntegerRing,\n\u001b[1;32m     97\u001b[0m         RationalField, RealField, ComplexField, PythonFiniteField,\n\u001b[1;32m     98\u001b[0m         GMPYFiniteField, PythonIntegerRing, GMPYIntegerRing, PythonRational,\n\u001b[1;32m     99\u001b[0m         GMPYRationalField, AlgebraicField, PolynomialRing, FractionField,\n\u001b[1;32m    100\u001b[0m         ExpressionDomain, FF_python, FF_gmpy, ZZ_python, ZZ_gmpy, QQ_python,\n\u001b[1;32m    101\u001b[0m         QQ_gmpy, GF, FF, ZZ, QQ, ZZ_I, QQ_I, RR, CC, EX, EXRAW,\n\u001b[1;32m    102\u001b[0m         construct_domain, swinnerton_dyer_poly, cyclotomic_poly,\n\u001b[1;32m    103\u001b[0m         symmetric_poly, random_poly, interpolating_poly, jacobi_poly,\n\u001b[1;32m    104\u001b[0m         chebyshevt_poly, chebyshevu_poly, hermite_poly, hermite_prob_poly,\n\u001b[1;32m    105\u001b[0m         legendre_poly, laguerre_poly, apart, apart_list, assemble_partfrac_list,\n\u001b[1;32m    106\u001b[0m         Options, ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[1;32m    108\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mseries\u001b[39;00m \u001b[39mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[1;32m    109\u001b[0m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[1;32m    110\u001b[0m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[1;32m    112\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[1;32m    113\u001b[0m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[1;32m    114\u001b[0m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[1;32m    134\u001b[0m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/polys/__init__.py:123\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39morthopolys\u001b[39;00m \u001b[39mimport\u001b[39;00m (jacobi_poly, chebyshevt_poly, chebyshevu_poly,\n\u001b[1;32m    118\u001b[0m         hermite_poly, hermite_prob_poly, legendre_poly, laguerre_poly)\n\u001b[1;32m    120\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mappellseqs\u001b[39;00m \u001b[39mimport\u001b[39;00m (bernoulli_poly, bernoulli_c_poly, genocchi_poly,\n\u001b[1;32m    121\u001b[0m         euler_poly, andre_poly)\n\u001b[0;32m--> 123\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpartfrac\u001b[39;00m \u001b[39mimport\u001b[39;00m apart, apart_list, assemble_partfrac_list\n\u001b[1;32m    125\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpolyoptions\u001b[39;00m \u001b[39mimport\u001b[39;00m Options\n\u001b[1;32m    127\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mrings\u001b[39;00m \u001b[39mimport\u001b[39;00m ring, xring, vring, sring\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/polys/partfrac.py:15\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolys\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolytools\u001b[39;00m \u001b[39mimport\u001b[39;00m parallel_poly_from_expr\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m \u001b[39mimport\u001b[39;00m numbered_symbols, take, xthreaded, public\n\u001b[1;32m     13\u001b[0m \u001b[39m@xthreaded\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[39m@public\u001b[39;49m\n\u001b[0;32m---> 15\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mapart\u001b[39;49m(f, x\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, full\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions):\n\u001b[1;32m     16\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m    Compute partial fraction decomposition of a rational function.\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m    apart_list, assemble_partfrac_list\u001b[39;49;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     allowed_flags(options, [])\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/utilities/decorator.py:76\u001b[0m, in \u001b[0;36mxthreaded\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mxthreaded\u001b[39m(func):\n\u001b[1;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply ``func`` to sub--elements of an object, excluding :class:`~.Add`.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[39m    This decorator is intended to make it uniformly possible to apply a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m threaded_factory(func, \u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/utilities/decorator.py:13\u001b[0m, in \u001b[0;36mthreaded_factory\u001b[0;34m(func, use_add)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A factory for ``threaded`` decorators. \"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m sympify\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatrices\u001b[39;00m \u001b[39mimport\u001b[39;00m MatrixBase\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miterables\u001b[39;00m \u001b[39mimport\u001b[39;00m iterable\n\u001b[1;32m     16\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mthreaded_func\u001b[39m(expr, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/matrices/__init__.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m MutableSparseMatrix\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39msparsetools\u001b[39;00m \u001b[39mimport\u001b[39;00m banded\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mimmutable\u001b[39;00m \u001b[39mimport\u001b[39;00m ImmutableDenseMatrix, ImmutableSparseMatrix\n\u001b[1;32m     23\u001b[0m ImmutableMatrix \u001b[39m=\u001b[39m ImmutableDenseMatrix\n\u001b[1;32m     24\u001b[0m SparseMatrix \u001b[39m=\u001b[39m MutableSparseMatrix\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/matrices/immutable.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msympify\u001b[39;00m \u001b[39mimport\u001b[39;00m _sympy_converter \u001b[39mas\u001b[39;00m sympify_converter, _sympify\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatrices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdense\u001b[39;00m \u001b[39mimport\u001b[39;00m DenseMatrix\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatrices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexpressions\u001b[39;00m \u001b[39mimport\u001b[39;00m MatrixExpr\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatrices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatrices\u001b[39;00m \u001b[39mimport\u001b[39;00m MatrixBase\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatrices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrepmatrix\u001b[39;00m \u001b[39mimport\u001b[39;00m RepMatrix\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/matrices/expressions/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeterminant\u001b[39;00m \u001b[39mimport\u001b[39;00m Determinant, det, Permanent, per\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtranspose\u001b[39;00m \u001b[39mimport\u001b[39;00m Transpose\n\u001b[0;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39madjoint\u001b[39;00m \u001b[39mimport\u001b[39;00m Adjoint\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mhadamard\u001b[39;00m \u001b[39mimport\u001b[39;00m hadamard_product, HadamardProduct, hadamard_power, HadamardPower\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdiagonal\u001b[39;00m \u001b[39mimport\u001b[39;00m DiagonalMatrix, DiagonalOf, DiagMatrix, diagonalize_vector\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/matrices/expressions/adjoint.py:7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatrices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexpressions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtranspose\u001b[39;00m \u001b[39mimport\u001b[39;00m transpose\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msympy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatrices\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexpressions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmatexpr\u001b[39;00m \u001b[39mimport\u001b[39;00m MatrixExpr\n\u001b[0;32m----> 7\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAdjoint\u001b[39;00m(MatrixExpr):\n\u001b[1;32m      8\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m    The Hermitian adjoint of a matrix expression.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m    True\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     is_Adjoint \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/core/basic.py:121\u001b[0m, in \u001b[0;36mBasic.__init_subclass__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init_subclass__\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[1;32m    117\u001b[0m     \u001b[39m# Initialize the default_assumptions FactKB and also any assumptions\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# property methods. This method will only be called for subclasses of\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39m# Basic but not for Basic itself so we call\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[39m# _prepare_class_assumptions(Basic) below the class definition.\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     _prepare_class_assumptions(\u001b[39mcls\u001b[39;49m)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/core/assumptions.py:638\u001b[0m, in \u001b[0;36m_prepare_class_assumptions\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    635\u001b[0m defs\u001b[39m.\u001b[39mupdate(local_defs)\n\u001b[1;32m    637\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_explicit_class_assumptions \u001b[39m=\u001b[39m defs\n\u001b[0;32m--> 638\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdefault_assumptions \u001b[39m=\u001b[39m StdFactKB(defs)\n\u001b[1;32m    640\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_prop_handler \u001b[39m=\u001b[39m {}\n\u001b[1;32m    641\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m _assume_defined:\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/core/assumptions.py:479\u001b[0m, in \u001b[0;36mStdFactKB.__init__\u001b[0;34m(self, facts)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generator \u001b[39m=\u001b[39m facts\u001b[39m.\u001b[39mgenerator\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m facts:\n\u001b[0;32m--> 479\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdeduce_all_facts(facts)\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/core/facts.py:625\u001b[0m, in \u001b[0;36mFactKB.deduce_all_facts\u001b[0;34m(self, facts)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[39m# lookup routing tables\u001b[39;00m\n\u001b[1;32m    624\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m full_implications[k, v]:\n\u001b[0;32m--> 625\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tell(key, value)\n\u001b[1;32m    627\u001b[0m     beta_maytrigger\u001b[39m.\u001b[39mupdate(beta_triggers[k, v])\n\u001b[1;32m    629\u001b[0m \u001b[39m# --- beta chains ---\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/pyg_torch21/lib/python3.10/site-packages/sympy/core/facts.py:587\u001b[0m, in \u001b[0;36mFactKB._tell\u001b[0;34m(self, k, v)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_tell\u001b[39m(\u001b[39mself\u001b[39m, k, v):\n\u001b[1;32m    583\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Add fact k=v to the knowledge base.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \n\u001b[1;32m    585\u001b[0m \u001b[39m    Returns True if the KB has actually been updated, False otherwise.\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 587\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m[k] \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    588\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m[k] \u001b[39m==\u001b[39m v:\n\u001b[1;32m    589\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "def get_datasets(get_edge_attr=False, filename=None):\n",
    "    if filename is None:\n",
    "        filename = 'HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt'\n",
    "    size = os.path.getsize(filename)\n",
    "    print('size of dataset on disk: ', size/1e9, 'gb')\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        data = HeteroData.from_dict(torch.load(filename))\n",
    "        print('loading saved heterodata object')\n",
    "\n",
    "    from torch_geometric import seed_everything\n",
    "    import torch_geometric.transforms as T\n",
    "    from torch_geometric.utils import sort_edge_index\n",
    "\n",
    "    edge_types = []\n",
    "    rev_edge_types = []\n",
    "    for edge_type in data.edge_types:\n",
    "        if edge_type[1].startswith('rev_'):\n",
    "            rev_edge_types.append(edge_type)\n",
    "        else:\n",
    "            edge_types.append(edge_type)\n",
    "\n",
    "    transform = T.RandomLinkSplit(\n",
    "        is_undirected=True,\n",
    "        edge_types=edge_types,\n",
    "        rev_edge_types=rev_edge_types,\n",
    "        num_val=0.02,\n",
    "        num_test=0.05,\n",
    "        add_negative_train_samples=False, # only adds neg samples for val and test, neg train are added by LinkNeighborLoader. This means for each train batch, negs. are different, for val and train they stay the same\n",
    "        neg_sampling_ratio=1.0,\n",
    "        disjoint_train_ratio=0.3, #  training edges are shared for message passing and supervision\n",
    "        )\n",
    "\n",
    "    seed_everything(14)\n",
    "    # sort by col to speed up sampling later (we can sepcify is_sorted=True in link neighbor loader)\n",
    "    # we actually dont use the sort, because it seems to mess up things, but have not checked if everything works without sorting, so we leave it here\n",
    "    def sort_edges(data):\n",
    "        for edge_type in data.edge_types:\n",
    "            if 'edge_attr' in data[edge_type].keys():\n",
    "                data[edge_type].edge_index, data[edge_type].edge_attr = sort_edge_index(data[edge_type].edge_index, data[edge_type].edge_attr, sort_by_row=False) \n",
    "            else:\n",
    "                data[edge_type].edge_index = sort_edge_index(data[edge_type].edge_index, sort_by_row=False) \n",
    "        return data\n",
    "    \n",
    "\n",
    "    def preprocess(data):\n",
    "        if not get_edge_attr:\n",
    "            # delete edge_attr of every edge type\n",
    "            for edge_type in data.edge_types:\n",
    "                del data[edge_type].edge_attr \n",
    "\n",
    "        # delete all keys for every node type except 'x' (e.g. description and title)\n",
    "        for node_type in train_data.node_types:\n",
    "            keys = list(train_data[node_type].keys())\n",
    "            for key in keys:\n",
    "                if key != 'x':\n",
    "                    del train_data[node_type][key]\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    # change all types to float32 and normalize the triangle columns\n",
    "    for node_type in data.node_types:\n",
    "        for i in range(data[node_type].x.shape[1]):\n",
    "            if data[node_type].x[:,i].max()>5:\n",
    "                #normalize\n",
    "                print('normalizing column ', i, ' of node type ', node_type)\n",
    "                data[node_type].x[:,i] = data[node_type].x[:,i]/data[node_type].x[:,i].max()\n",
    "        \n",
    "        data[node_type].x = data[node_type].x.to(torch.float32)\n",
    "        \n",
    "    \n",
    "    \n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    #train_data = sort_edges(train_data)\n",
    "    #val_data = sort_edges(val_data)\n",
    "    #test_data = sort_edges(test_data)\n",
    "    train_data = preprocess(train_data)\n",
    "    val_data = preprocess(val_data)\n",
    "    test_data = preprocess(test_data)\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.loader import HGTLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "\n",
    "def get_hgt_linkloader(data, target_edge, batch_size, is_training, sampling_mode, neg_ratio, num_neighbors_hgtloader, num_workers):\n",
    "    # first sample some edges in linkNeighborLoader\n",
    "    # use the nodes of the sampled edges to sample from hgt loader\n",
    "    \n",
    "    \n",
    "    num_neighbors_linkloader = [0]\n",
    "    #for edge_type in data.edge_types:\n",
    "    #    num_neighbors_linkloader[edge_type] = [0,0]\n",
    "    \n",
    "    negative_sampling = NegativeSampling(\n",
    "        mode=sampling_mode, # binary or triplet\n",
    "        amount=neg_ratio  # ratio, like Graphsage # 10\n",
    "        #weight=  # \"Probabilities\" of nodes to be sampled: Node degree follows power law distribution\n",
    "        )\n",
    "    \n",
    "    if sampling_mode == 'triplet':\n",
    "        data[target_edge].edge_label = None\n",
    "        \n",
    "\n",
    "    linkNeighborLoader = LinkNeighborLoader(\n",
    "            data,\n",
    "            num_neighbors=num_neighbors_linkloader,\n",
    "            edge_label_index=(target_edge, data[target_edge].edge_label_index), # if (edge, None), None means all edges are considered\n",
    "        \n",
    "            neg_sampling=negative_sampling, # adds negative samples\n",
    "            batch_size=batch_size,\n",
    "            shuffle=is_training, #is_training\n",
    "            subgraph_type='directional', # contains only sampled edges\n",
    "            #drop_last=True,\n",
    "            num_workers=num_workers,\n",
    "            #disjoint=True # sampled seed node creates its own, disjoint from the rest, subgraph, will add \"batch vector\" to loader output\n",
    "            pin_memory=True, # faster data transfer to gpu\n",
    "            #num_workers=2,\n",
    "            #prefetch_factor=2\n",
    "            is_sorted = False,\n",
    "            prefetch_factor=3\n",
    "    )\n",
    "   \n",
    "   \n",
    "    def get_hgt(data, input_nodetype, input_mask):\n",
    "        return next(iter(HGTLoader(\n",
    "                data,\n",
    "                # Sample 512 nodes per type and per iteration for 4 iterations\n",
    "                num_samples=num_neighbors_hgtloader,\n",
    "                batch_size=input_mask.shape[0],\n",
    "                input_nodes=(input_nodetype, input_mask),\n",
    "                num_workers=num_workers,\n",
    "                pin_memory=True,\n",
    "                prefetch_factor=3\n",
    "            )))\n",
    "        \n",
    "    \n",
    "    def add_self_loops(data):\n",
    "        for node_type in data.node_types:\n",
    "            data[node_type, 'self_loop', node_type].edge_index = torch.arange(data[node_type].num_nodes).repeat(2,1)\n",
    "        return data \n",
    "\n",
    "            \n",
    "    def get_hgt_with_selfloops(loader):\n",
    "        \n",
    "        \n",
    "        for batch in loader:   \n",
    "            if sampling_mode=='triplet':      \n",
    "                # original edge_label_index from the whole data object\n",
    "                unmapped_batchids = torch.cat((batch[target_edge[0]].src_index,batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index)).unique()\n",
    "                original_edge_label_nodes = torch.LongTensor(batch[target_edge[0]].n_id[unmapped_batchids])\n",
    "\n",
    "                #remapping or sorting is not needed, since nodes are sorted, also in the htg batch, the edges will be the same\n",
    "        \n",
    "                src = batch[target_edge[0]].src_index.unsqueeze(0)\n",
    "                src = torch.cat((src,src), dim=1)\n",
    "                dst = torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index),dim=0).unsqueeze(0)\n",
    "                local_edge_label_index = torch.cat((src, dst),dim=0)\n",
    "                edge_label = torch.cat((torch.ones(batch[target_edge[2]].dst_pos_index.shape[0]), torch.zeros(batch[target_edge[2]].dst_neg_index.shape[0])))\n",
    "                \n",
    "            elif sampling_mode=='binary':\n",
    "                unmapped_batchids = batch[target_edge].edge_label_index.flatten().unique()\n",
    "                original_edge_label_nodes = torch.LongTensor(batch[target_edge[0]].n_id[unmapped_batchids])\n",
    "\n",
    "            else:\n",
    "                raise Exception('binary or triplet sampling mode')\n",
    "                \n",
    "                \n",
    "            hgt_batch = get_hgt(data, target_edge[0], original_edge_label_nodes) # 0,1,3,4,5,6,7,8,9,\n",
    "          \n",
    "            if sampling_mode=='triplet':\n",
    "                \n",
    "                # return message passing edges, and supervision edges/labels, ignore labels/label_indices in the message passing edges\n",
    "                yield add_self_loops(hgt_batch), local_edge_label_index, edge_label, batch[target_edge].input_id\n",
    "            else: # sampling_mode=='binary':\n",
    "                # return message passing edges, and supervision edges/labels, ignore labels/label_indices in the message passing edges, as well as original edge indices\n",
    "                yield add_self_loops(hgt_batch), batch[target_edge].edge_label_index, batch[target_edge].edge_label, batch[target_edge].input_id\n",
    "    \n",
    "    def get_hgt_2types_with_selfloops(loader):\n",
    "        for batch in loader:\n",
    "            if sampling_mode=='triplet':   \n",
    "                original_edge_label_index_class1 = torch.LongTensor(batch[target_edge[0]].n_id[batch[target_edge[0]].src_index.unique()])\n",
    "                original_edge_label_index_class2 = torch.LongTensor(batch[target_edge[2]].n_id[torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index)).unique()])\n",
    "                \n",
    "                src = batch[target_edge[0]].src_index.unsqueeze(0)\n",
    "                src = torch.cat((src,src), dim=1)\n",
    "                dst = torch.cat((batch[target_edge[2]].dst_pos_index, batch[target_edge[2]].dst_neg_index),dim=0).unsqueeze(0)\n",
    "                \n",
    "                local_edge_label_index = torch.cat((src, dst),dim=0)\n",
    "                edge_label = torch.cat((torch.ones(batch[target_edge[2]].dst_pos_index.shape[0]), torch.zeros(batch[target_edge[2]].dst_neg_index.shape[0])))\n",
    "\n",
    "            elif sampling_mode=='binary':\n",
    "                original_edge_label_index_class1 = batch[target_edge[0]].n_id[batch[target_edge].edge_label_index[0,:].unique()]\n",
    "                original_edge_label_index_class2 = batch[target_edge[2]].n_id[batch[target_edge].edge_label_index[1,:].unique()]\n",
    "\n",
    "            else:\n",
    "                raise Exception('binary or triplet sampling mode')\n",
    "\n",
    "            # batch the start and end supervision nodes separately\n",
    "            hgt_batch1 = get_hgt(data, target_edge[0], original_edge_label_index_class1)\n",
    "            hgt_batch2 = get_hgt(data, target_edge[2], original_edge_label_index_class2)\n",
    "            \n",
    "            \n",
    "            # ** We dont need to remove any edges ** since the supervision edges wont be sampled by hgt\n",
    "            if sampling_mode=='triplet':\n",
    "                yield add_self_loops(hgt_batch1), add_self_loops(hgt_batch2), local_edge_label_index, edge_label, batch[target_edge].input_id\n",
    "            else: # sampling_mode=='binary':\n",
    "                # we can access the corresponding nodes of edge_label_index[0,:] in hgt_batch1[target_edge[0]], those of [1,:] in hgt_batch2...\n",
    "                yield add_self_loops(hgt_batch1), add_self_loops(hgt_batch2), batch[target_edge].edge_label_index, batch[target_edge].edge_label, batch[target_edge].input_id\n",
    "\n",
    "        \n",
    "    if target_edge[0] == target_edge[2]:\n",
    "        # same edge type, only need to sample once\n",
    "        return get_hgt_with_selfloops(linkNeighborLoader)\n",
    "    else:\n",
    "        return get_hgt_2types_with_selfloops(linkNeighborLoader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dataset on disk:  2.27811871 gb\n",
      "loading saved heterodata object\n",
      "normalizing column  773  of node type  skills\n",
      "normalizing column  774  of node type  skills\n",
      "normalizing column  24  of node type  people\n",
      "normalizing column  773  of node type  jobs\n",
      "normalizing column  774  of node type  jobs\n",
      "HeteroData(\n",
      "  courses_and_programs={\n",
      "    x=[0, 815],\n",
      "    n_id=[0],\n",
      "    num_sampled_nodes=[2],\n",
      "  },\n",
      "  qualifications={\n",
      "    x=[0, 786],\n",
      "    n_id=[0],\n",
      "    num_sampled_nodes=[2],\n",
      "  },\n",
      "  skills={\n",
      "    x=[0, 775],\n",
      "    n_id=[0],\n",
      "    num_sampled_nodes=[2],\n",
      "  },\n",
      "  people={\n",
      "    x=[0, 25],\n",
      "    n_id=[0],\n",
      "    num_sampled_nodes=[2],\n",
      "  },\n",
      "  jobs={\n",
      "    x=[12, 775],\n",
      "    n_id=[12],\n",
      "    num_sampled_nodes=[2],\n",
      "    src_index=[4],\n",
      "    dst_pos_index=[4],\n",
      "    dst_neg_index=[4],\n",
      "  },\n",
      "  organizations={\n",
      "    x=[0, 3],\n",
      "    n_id=[0],\n",
      "    num_sampled_nodes=[2],\n",
      "  },\n",
      "  (skills, qualification_skill, qualifications)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[431],\n",
      "    edge_label_index=[2, 431],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (skills, course_and_program_skill, courses_and_programs)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[69687],\n",
      "    edge_label_index=[2, 69687],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (courses_and_programs, course_qualification, qualifications)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[567],\n",
      "    edge_label_index=[2, 567],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (courses_and_programs, course_and_programs_student, people)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[149433],\n",
      "    edge_label_index=[2, 149433],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (jobs, job_student, people)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[79230],\n",
      "    edge_label_index=[2, 79230],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (people, supervisor_supervisee, people)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[58839],\n",
      "    edge_label_index=[2, 58839],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (people, organization_student, organizations)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[78856],\n",
      "    edge_label_index=[2, 78856],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (jobs, job_job, jobs)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "    input_id=[4],\n",
      "  },\n",
      "  (skills, job_skill, jobs)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[4398188],\n",
      "    edge_label_index=[2, 4398188],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (jobs, broader_job_job, jobs)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[14738],\n",
      "    edge_label_index=[2, 14738],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (skills, skill_skill, skills)={\n",
      "    edge_index=[2, 0],\n",
      "    edge_label=[610044],\n",
      "    edge_label_index=[2, 610044],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (qualifications, rev_qualification_skill, skills)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (courses_and_programs, rev_course_and_program_skill, skills)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (qualifications, rev_course_qualification, courses_and_programs)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (people, rev_course_and_programs_student, courses_and_programs)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (people, rev_job_student, jobs)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (people, rev_supervisor_supervisee, people)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (organizations, rev_organization_student, people)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (jobs, rev_job_job, jobs)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (jobs, rev_job_skill, skills)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (jobs, rev_broader_job_job, jobs)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  },\n",
      "  (skills, rev_skill_skill, skills)={\n",
      "    edge_index=[2, 0],\n",
      "    e_id=[0],\n",
      "    num_sampled_edges=[1],\n",
      "  }\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  courses_and_programs={\n",
       "    x=[0, 815],\n",
       "    n_id=[0],\n",
       "  },\n",
       "  qualifications={\n",
       "    x=[0, 786],\n",
       "    n_id=[0],\n",
       "  },\n",
       "  skills={\n",
       "    x=[10, 775],\n",
       "    n_id=[10],\n",
       "  },\n",
       "  people={\n",
       "    x=[10, 25],\n",
       "    n_id=[10],\n",
       "  },\n",
       "  jobs={\n",
       "    x=[22, 775],\n",
       "    n_id=[22],\n",
       "    input_id=[12],\n",
       "    batch_size=12,\n",
       "  },\n",
       "  organizations={\n",
       "    x=[0, 3],\n",
       "    n_id=[0],\n",
       "  },\n",
       "  (skills, qualification_skill, qualifications)={\n",
       "    edge_index=[2, 0],\n",
       "    edge_label=[431],\n",
       "    edge_label_index=[2, 431],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (skills, course_and_program_skill, courses_and_programs)={\n",
       "    edge_index=[2, 0],\n",
       "    edge_label=[69687],\n",
       "    edge_label_index=[2, 69687],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (courses_and_programs, course_qualification, qualifications)={\n",
       "    edge_index=[2, 0],\n",
       "    edge_label=[567],\n",
       "    edge_label_index=[2, 567],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (courses_and_programs, course_and_programs_student, people)={\n",
       "    edge_index=[2, 0],\n",
       "    edge_label=[149433],\n",
       "    edge_label_index=[2, 149433],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (jobs, job_student, people)={\n",
       "    edge_index=[2, 10],\n",
       "    edge_label=[79230],\n",
       "    edge_label_index=[2, 79230],\n",
       "    e_id=[10],\n",
       "  },\n",
       "  (people, supervisor_supervisee, people)={\n",
       "    edge_index=[2, 0],\n",
       "    edge_label=[58839],\n",
       "    edge_label_index=[2, 58839],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (people, organization_student, organizations)={\n",
       "    edge_index=[2, 0],\n",
       "    edge_label=[78856],\n",
       "    edge_label_index=[2, 78856],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (jobs, job_job, jobs)={\n",
       "    edge_index=[2, 20],\n",
       "    edge_label_index=[2, 4963],\n",
       "    e_id=[20],\n",
       "  },\n",
       "  (skills, job_skill, jobs)={\n",
       "    edge_index=[2, 4],\n",
       "    edge_label=[4398188],\n",
       "    edge_label_index=[2, 4398188],\n",
       "    e_id=[4],\n",
       "  },\n",
       "  (jobs, broader_job_job, jobs)={\n",
       "    edge_index=[2, 4],\n",
       "    edge_label=[14738],\n",
       "    edge_label_index=[2, 14738],\n",
       "    e_id=[4],\n",
       "  },\n",
       "  (skills, skill_skill, skills)={\n",
       "    edge_index=[2, 0],\n",
       "    edge_label=[610044],\n",
       "    edge_label_index=[2, 610044],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (qualifications, rev_qualification_skill, skills)={\n",
       "    edge_index=[2, 0],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (courses_and_programs, rev_course_and_program_skill, skills)={\n",
       "    edge_index=[2, 0],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (qualifications, rev_course_qualification, courses_and_programs)={\n",
       "    edge_index=[2, 0],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (people, rev_course_and_programs_student, courses_and_programs)={\n",
       "    edge_index=[2, 0],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (people, rev_job_student, jobs)={\n",
       "    edge_index=[2, 10],\n",
       "    e_id=[10],\n",
       "  },\n",
       "  (people, rev_supervisor_supervisee, people)={\n",
       "    edge_index=[2, 0],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (organizations, rev_organization_student, people)={\n",
       "    edge_index=[2, 0],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (jobs, rev_job_job, jobs)={\n",
       "    edge_index=[2, 20],\n",
       "    e_id=[20],\n",
       "  },\n",
       "  (jobs, rev_job_skill, skills)={\n",
       "    edge_index=[2, 3],\n",
       "    e_id=[3],\n",
       "  },\n",
       "  (jobs, rev_broader_job_job, jobs)={\n",
       "    edge_index=[2, 5],\n",
       "    e_id=[5],\n",
       "  },\n",
       "  (skills, rev_skill_skill, skills)={\n",
       "    edge_index=[2, 0],\n",
       "    e_id=[0],\n",
       "  },\n",
       "  (courses_and_programs, self_loop, courses_and_programs)={ edge_index=[2, 0] },\n",
       "  (qualifications, self_loop, qualifications)={ edge_index=[2, 0] },\n",
       "  (skills, self_loop, skills)={ edge_index=[2, 10] },\n",
       "  (people, self_loop, people)={ edge_index=[2, 10] },\n",
       "  (jobs, self_loop, jobs)={ edge_index=[2, 22] },\n",
       "  (organizations, self_loop, organizations)={ edge_index=[2, 0] }\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data, val_data, test_data = get_datasets(get_edge_attr=False)\n",
    "# testing\n",
    "#input_edgetype = ('jobs', 'job_job', 'jobs')\n",
    "#loader = get_hgt_linkloader(train_data, input_edgetype, 4, True, 'triplet', 1, [10], num_workers=0)\n",
    "#minibatch, edge_label_index, edge_label, input_edge_ids = next(iter(loader))\n",
    "#minibatch\n",
    "#input_edgetype = ('skills', 'qualification_skill', 'qualifications')\n",
    "#loader = get_hgt_linkloader(train_data, input_edgetype, 10, True, 'triplet', 1, [10], num_workers=0)\n",
    "#minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id = next(iter(loader))\n",
    "#input_edge_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_minibatch_count(data, batch_size):\n",
    "    batches = []\n",
    "    for edge_type in data.edge_types:\n",
    "        if edge_type[1].startswith('rev_'):\n",
    "            continue\n",
    "        batches.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
    "        \n",
    "    return len(batches)\n",
    "\n",
    "def uniform_hgt_sampler(data, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers):\n",
    "    # return batches from all edgetypes with each \"edge\" being drawn uniformly at random (but we translate the probabilities to batches), last batches of each edge type may be smaller than batch_size\n",
    "    batches = []\n",
    "    loaders = {}\n",
    "    # only the non-reverse edge types for now\n",
    "    for edge_type in data.edge_types:\n",
    "        if edge_type[1].startswith('rev_'):\n",
    "            continue\n",
    "        batches.extend([edge_type for _ in range((data[edge_type].edge_label_index.shape[1]+batch_size)//batch_size)])\n",
    "        loaders[edge_type]=get_hgt_linkloader(data, edge_type, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors, num_workers)\n",
    "        \n",
    "    random.seed(14)\n",
    "    random.shuffle(batches)\n",
    "    # set a random random seed again (may affect creating the loaders later for a second epoch)\n",
    "    random.seed()\n",
    "    \n",
    "    print('total batches:', len(batches))\n",
    "    \n",
    "    for target_edge_type in batches:\n",
    "        if target_edge_type[0] == target_edge_type[2]:\n",
    "            same_nodetype = True\n",
    "        else:\n",
    "            same_nodetype = False\n",
    "        yield same_nodetype, target_edge_type, next(loaders[target_edge_type])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_neighbors [36, 363]\n",
      "2023-11-01 14:21:24.226340\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_hgt_linkloader() missing 1 required positional argument: 'num_workers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(start)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,(same_nodetype, target_edge_type, batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sampler):\n\u001b[1;32m     19\u001b[0m     \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# batching is different depending on if node types in edge are same or different\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     edge_type \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m same_nodetype:\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36muniform_hgt_sampler\u001b[0;34m(data, batch_size, is_training, sampling_mode, neg_sampling_ratio, num_neighbors)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     batches\u001b[38;5;241m.\u001b[39mextend([edge_type \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m((data[edge_type]\u001b[38;5;241m.\u001b[39medge_label_index\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mbatch_size)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mbatch_size)])\n\u001b[0;32m---> 21\u001b[0m     loaders[edge_type]\u001b[38;5;241m=\u001b[39m\u001b[43mget_hgt_linkloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sampling_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_neighbors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m14\u001b[39m)\n\u001b[1;32m     24\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(batches)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_hgt_linkloader() missing 1 required positional argument: 'num_workers'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import datetime\n",
    "    import time\n",
    "\n",
    "    # num neighbors: sample 25 one hop neighbors and for each one hop neighbor again 10 neighbors\n",
    "\n",
    "    batch_size = 32\n",
    "    num_relationships = len(train_data.edge_types)\n",
    "    one_hop_neighbors = (25 * batch_size)//num_relationships # per relationship type\n",
    "    two_hop_neighbors = (25 * 10 * batch_size)//num_relationships # per relationship type\n",
    "    num_neighbors = [one_hop_neighbors, two_hop_neighbors]\n",
    "    print('num_neighbors', num_neighbors)\n",
    "\n",
    "    sampler = uniform_hgt_sampler(train_data, batch_size, True, 'binary', 1, num_neighbors, num_workers=0)\n",
    "    start = datetime.datetime.now()\n",
    "    print(start)\n",
    "    print()\n",
    "    for i,(same_nodetype, target_edge_type, batch) in enumerate(sampler):\n",
    "        \n",
    "        # batching is different depending on if node types in edge are same or different\n",
    "        edge_type = batch[-1]\n",
    "        if same_nodetype:\n",
    "            minibatch, edge_label_index, edge_label, input_edge_ids = batch\n",
    "            print(minibatch)\n",
    "        else:\n",
    "            minibatchpart1, minibatchpart2, edge_label_index, edge_label, input_edge_id = batch\n",
    "            print(minibatchpart1)\n",
    "            \n",
    "        print(i,target_edge_type)\n",
    "        \n",
    "        break\n",
    "        time.sleep(5)\n",
    "        \n",
    "    end = datetime.datetime.now()\n",
    "    print()\n",
    "    print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg_torch21",
   "language": "python",
   "name": "pyg_torch21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
