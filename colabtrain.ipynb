{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "  from google.colab import drive\n",
    "  colab_path = '/content/'\n",
    "  drive.mount('/content/drive',force_remount=True)\n",
    "  DRIVE_FOLDER = Path('/content/drive/MyDrive/DataExplorationProject/Skill_Ontology_GNN')\n",
    "  colab = True\n",
    "else:\n",
    "  colab_path = ''\n",
    "  colab = False\n",
    "  \n",
    "  \n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import gc\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, device, metrics=[]):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.metrics_history = self.create_metrics_history(metrics)\n",
    "        self.epoch = 0\n",
    "\n",
    "    def create_metrics_history(self, metrics):\n",
    "        metrics = set(metrics)\n",
    "        metrics.add('epoch')\n",
    "        metrics.add('minibatch')\n",
    "        metrics.add('accuracy')\n",
    "        metrics.add('loss')\n",
    "\n",
    "        metrics = list(metrics)\n",
    "        metrics_history={}\n",
    "        for split in ['train','val']:\n",
    "            metrics_history[split]={}\n",
    "            for metric in metrics:\n",
    "                metrics_history[split][metric]=[]\n",
    "        return metrics_history\n",
    "\n",
    "    def free_memory(self):\n",
    "        \"\"\"Clears the GPU cache and triggers garbage collection, to reduce OOMs.\"\"\"\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    def train(self, dataloader, n_epochs, save_interval, save_path):\n",
    "        self.free_memory()\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epoch, self.epoch+n_epochs):\n",
    "            print(f'=============== Epoch {epoch} ===============')\n",
    "            for batch_idx, (data, target) in enumerate(dataloader):\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.train_losses.append(loss.item())\n",
    "                if batch_idx % save_interval == 0:\n",
    "                    self.save_checkpoint(epoch, batch_idx, save_path)\n",
    "\n",
    "                print(f'Mini-Batch {batch_idx}, Loss: {loss}')\n",
    "\n",
    "    def validate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in dataloader:\n",
    "                data, target = data.to(self.device), target.to(self.device)\n",
    "                output = self.model(data)\n",
    "                loss = self.criterion(output, target)\n",
    "                self.val_losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, epoch, batch_idx, save_path):\n",
    "        print('save')\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'metrics_history': self.metrics_history,\n",
    "        }, f'{save_path}/checkpoint_{epoch}_{batch_idx}.pt')\n",
    "\n",
    "    def load_checkpoint(self, load_path):\n",
    "        checkpoint = torch.load(load_path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "       \n",
    "\n",
    "    def plot_losses(self):\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.title(\"Training and Validation Loss\")\n",
    "        plt.plot(self.train_losses,label=\"train\")\n",
    "        plt.plot(self.val_losses,label=\"val\")\n",
    "        plt.xlabel(\"iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "if colab:\n",
    "    # Install required packages.\n",
    "    import os\n",
    "    import torch\n",
    "    os.environ['TORCH'] = torch.__version__\n",
    "    print(torch.__version__)\n",
    "    # !pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "    # !pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "    # !pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
    "    # !pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "    !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "    !pip install torch_geometric\n",
    "    !pip install sentence-transformers\n",
    "    !pip install torcheval\n",
    "    !pip install matplotlib\n",
    "    !pip install pandas\n",
    "    # unpack datasets\n",
    "    if not 'unzipped' in globals():\n",
    "        !unzip /content/drive/MyDrive/DataExplorationProject/Skill_Ontology_GNN/neo4jgraph.zip\n",
    "        unzipped =True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "filename = 'Job_Skill_HeteroData_v1.pt'\n",
    "if os.path.exists('./'+filename):\n",
    "    data = torch.load('./'+filename)\n",
    "    print('loading saved heterodata object')\n",
    "else:\n",
    "    torch.save(data, './'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import seed_everything\n",
    "import torch_geometric.transforms as T\n",
    "seed_everything(4)\n",
    "\n",
    "transform = T.RandomLinkSplit(\n",
    "    is_undirected=True,\n",
    "    edge_types=[\n",
    "        ('Job', 'REQUIRES', 'Skill'),\n",
    "        ('Skill', 'IS_SIMILAR_SKILL', 'Skill'),\n",
    "        ('Job', 'IS_SIMILAR_JOB', 'Job')\n",
    "        ],\n",
    "    rev_edge_types=[\n",
    "        ('Skill', 'rev_REQUIRES', 'Job'),\n",
    "        ('Skill', 'rev_IS_SIMILAR_SKILL', 'Skill'),\n",
    "        ('Job', 'rev_IS_SIMILAR_JOB', 'Job')\n",
    "    ],\n",
    "    num_val=0.008,\n",
    "    num_test=0.80,\n",
    "    add_negative_train_samples=False, # only adds neg samples for val and test, neg train are added by LinkNeighborLoader. This means for each train batch, negs. are different, for val and train they stay the same\n",
    "    neg_sampling_ratio=1.0,\n",
    "    disjoint_train_ratio=0, #  training edges are shared for message passing and supervision\n",
    "    \n",
    "\n",
    "    )\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.sampler import NegativeSampling\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "def create_loader(data:HeteroData, edge_type:Tuple[str,str,str], num_neighbors:List[int], negative_sampling_amount:int, batch_size:int, is_training:bool)->LinkNeighborLoader:\n",
    "\n",
    "    #print('create mini-batches for', edge)\n",
    "\n",
    "    negative_sampling = NegativeSampling(\n",
    "        mode='binary',\n",
    "        amount=negative_sampling_amount  # ratio, like Graphsage\n",
    "        #weight=  # \"Probabilities\" of nodes to be sampled: Node degree follows power law distribution\n",
    "        )\n",
    "\n",
    "    loader = LinkNeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        # {\n",
    "        #     ('Job', 'REQUIRES', 'Skill'):num_neighbors,\n",
    "        #     ('Skill', 'rev_REQUIRES', 'Job'):num_neighbors,\n",
    "        #     ('Skill', 'IS_SIMILAR_SKILL', 'Skill'):num_neighbors, # In this example, index 0 will never be used, since neighboring edge to a job node can't be a skill-skill edge\n",
    "        #     ('Skill', 'rev_IS_SIMILAR_SKILL', 'Skill'):num_neighbors,\n",
    "        #     ('Job', 'IS_SIMILAR_JOB', 'Job'):num_neighbors,\n",
    "        #     ('Job', 'rev_IS_SIMILAR_JOB', 'Job'):num_neighbors,\n",
    "        # },\n",
    "        edge_label_index=(edge_type, data[edge_type].edge_label_index), # if (edge, None), None means all edges are considered\n",
    "        #  =train_data[edge].edge_label,\n",
    "        neg_sampling=negative_sampling, # adds negative samples\n",
    "        batch_size=batch_size,\n",
    "        shuffle=is_training,\n",
    "        #drop_last=True,\n",
    "        #num_workers=0,\n",
    "        directed=True,  # contains only edges which are followed, False: contains full node induced subgraph\n",
    "        #disjoint=True # sampled seed node creates its own, disjoint from the rest, subgraph, will add \"batch vector\" to loader output\n",
    "        pin_memory=True # faster data transfer to gpu\n",
    "    )\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "num_neighbors = [5,4]\n",
    "\n",
    "def create_iterator(data, is_training:bool):\n",
    "    loaders = []\n",
    "    for edge_type in [train_data.edge_types[0]]:\n",
    "        if 'rev_' in edge_type[1]:\n",
    "            continue    # we dont need rev_ target edges, since they are the same\n",
    "                        # rev edges are only needed in the later step for the gnn traversal\n",
    "        # create mini-batches for each edge type, because LinkNeighborLoader only allows one target edge type\n",
    "     \n",
    "        loader = create_loader(\n",
    "            data=data,\n",
    "            edge_type=edge_type,\n",
    "            num_neighbors=num_neighbors,\n",
    "            batch_size=batch_size,\n",
    "            is_training=is_training,\n",
    "            negative_sampling_amount=(20 if is_training else 1)\n",
    "        )\n",
    "        loaders.append(loader)\n",
    "    \n",
    "    \n",
    "    # creates an iterator which has as many elements as the longest iterable\n",
    "    # other iterables will be repeated until the longest is done\n",
    "    length = 0\n",
    "    index = 0\n",
    "    for i, iterable in enumerate(loaders):\n",
    "        l = len(iterable)\n",
    "        if l>length:\n",
    "            length = l\n",
    "            index = i\n",
    "\n",
    "    longest_loader = loaders.pop(index)\n",
    "    \n",
    "    \n",
    "    # create a list of iterators\n",
    "    iterators = [iter(loader) for loader in loaders]\n",
    "    \n",
    "    def iterator():\n",
    "        for batch in longest_loader:\n",
    "            batches = [batch]\n",
    "            for i in range(len(iterators)):\n",
    "                try:\n",
    "                    batches.append(next(iterators[i]))\n",
    "                except StopIteration:\n",
    "                    iterators[i] = iter(loaders[i])\n",
    "                    batches.append(next(iterators[i]))\n",
    "            yield tuple(batches)\n",
    "\n",
    "    return iterator, len(longest_loader)\n",
    "    \n",
    "    \n",
    "\n",
    "# watch -n 1 df -h /dev/shm\n",
    "gc.collect()\n",
    "train_iterator, train_batch_len = create_iterator(train_data, is_training=True)\n",
    "val_iterator, val_batch_len = create_iterator(val_data, is_training=False)\n",
    "test_iterator, test_batch_len = create_iterator(test_data, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Union\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import to_hetero, HeteroDictLinear, Linear\n",
    "from torch_geometric.nn.conv import GraphConv, SAGEConv, SimpleConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.typing import Adj, OptPairTensor, OptTensor, Size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PyG does not implement the exact max pooling aggregation as in the GraphSage paper\n",
    "# with GraphConvWithPool we manually extend it by adding a linear layer on x before .propagate\n",
    "# as our activation function is monotonically increasing, this modification corresponds to the max pooling aggregation\n",
    "\n",
    "class GraphConvWithPool(GraphConv):\n",
    "    def __init__(self, in_channels, out_channels: int, aggr: str = 'add', bias: bool = True, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, aggr, bias, **kwargs)\n",
    "        self.linear = torch.nn.Linear(in_channels, in_channels, bias=False)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj,\n",
    "                edge_weight: OptTensor = None, size: Size = None) -> Tensor:\n",
    "\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)\n",
    "\n",
    "        x = self.linear(x) # added this\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, edge_weight=edge_weight,\n",
    "                             size=size)\n",
    "        out = self.lin_rel(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if x_r is not None:\n",
    "            out = out + self.lin_root(x_r)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class WeightedSkillSage(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        #self.linear1 = Linear(-1,-1)\n",
    "        #self.conv1 = SimpleConv(aggr='sum')\n",
    "        self.linear1 = Linear(-1,hidden_channels)\n",
    "        self.linear2 = Linear(-1,hidden_channels)\n",
    "        self.conv1 = GraphConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
    "        self.conv2 = GraphConv(in_channels=hidden_channels, out_channels=hidden_channels)\n",
    "        self.linear3 = Linear(hidden_channels,out_channels)\n",
    "\n",
    "    def forward(self, x: HeteroData, edge_index, edge_weight):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "model = WeightedSkillSage(hidden_channels=64, out_channels=64)\n",
    "model = to_hetero(model, train_data.metadata(), aggr='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinaryAUPRC\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class GNNTrainer(Trainer):\n",
    "    def __init__(self, model, criterion, optimizer, device):\n",
    "        super().__init__(model, criterion, optimizer, device, metrics=['f1','accuracy','precision','recall', 'aucpr'])\n",
    "        self.train_batch_len = 0\n",
    "        self.val_batch_len = 0\n",
    "        \n",
    "    def get_supervision_edge_type(self, heterodata):\n",
    "        for edge_type in heterodata.edge_types:\n",
    "            if 'input_id' in heterodata[edge_type].keys():\n",
    "                return edge_type\n",
    "\n",
    "    def calculate_metrics(self, split_name, y_hat, y, print_=True):\n",
    "        y = y.to(torch.int).cpu()\n",
    "        y_hat = y_hat.cpu()\n",
    "        acc, prec, rec, f1, aucpr = BinaryAccuracy(threshold=0.5).update(y_hat, y).compute().item(), BinaryPrecision(threshold=0.5).update(y_hat, y).compute().item(), BinaryRecall(threshold=0.5).update(y_hat, y).compute().item(), BinaryF1Score(threshold=0.5).update(y_hat, y).compute().item(), BinaryAUPRC().update(y_hat, y).compute().item()\n",
    "        self.metrics_history[split_name]['accuracy'].append(acc)\n",
    "        self.metrics_history[split_name]['precision'].append(prec)\n",
    "        self.metrics_history[split_name]['recall'].append(rec)\n",
    "        self.metrics_history[split_name]['f1'].append(f1)\n",
    "        self.metrics_history[split_name]['aucpr'].append(aucpr) \n",
    "        if print_:\n",
    "            print(f'{split_name}: F1: {f1}, AUC-PR: {aucpr}, (acc: {acc}, prec: {prec}, rec: {rec})')\n",
    "\n",
    "    def train(self, train_iterator, val_iterator, start_epoch, n_epochs, batch_save_interval, save_path):\n",
    "        self.free_memory()\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(start_epoch, start_epoch+n_epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss = 0\n",
    "            for batch_idx, edge_batches in tqdm(enumerate(train_iterator()), total=self.train_batch_len):\n",
    "                minibatch_loss = 0\n",
    "                \n",
    "                y_hat, y = [], []\n",
    "                for i, batch in enumerate(edge_batches):  # each batch here is one edge type, since we want to learn for all edge types\n",
    "                    batch = batch.to(self.device)\n",
    "                    hetero_out = model(batch.x_dict, batch.edge_index_dict, batch.edge_weight_dict)  # get model output\n",
    "\n",
    "                    # evaluate, calculate cosine sim and compute cross-entropy loss\n",
    "                    supervision_edge_type = self.get_supervision_edge_type(batch)\n",
    "                    src_type, dst_type = supervision_edge_type[0], supervision_edge_type[2]\n",
    "                    edge_label = batch[supervision_edge_type].edge_label\n",
    "                    edge_label_index = batch[supervision_edge_type].edge_label_index\n",
    "                    src_node_embeddings = hetero_out[src_type][edge_label_index[0]]\n",
    "                    dst_node_embeddings = hetero_out[dst_type][edge_label_index[1]]\n",
    "                    logits = F.cosine_similarity(src_node_embeddings, dst_node_embeddings, dim=-1)\n",
    "                    loss = self.criterion(logits, edge_label)\n",
    "                    minibatch_loss += loss\n",
    "\n",
    "                    y_hat.append(torch.sigmoid(logits).cpu().detach())\n",
    "                    y.append(edge_label.to(torch.int).cpu().detach())\n",
    "\n",
    "                minibatch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += minibatch_loss.item()\n",
    "\n",
    "                # save loss and metrics\n",
    "                self.metrics_history['train']['minibatch'].append(epoch+batch_idx)\n",
    "                self.metrics_history['train']['epoch'].append(epoch+batch_idx)\n",
    "                self.metrics_history['train']['loss'].append(minibatch_loss.item())\n",
    "\n",
    "                y_hat = torch.cat(y_hat)\n",
    "                y = torch.cat(y)\n",
    "                \n",
    "                self.calculate_metrics('train', y_hat, y, print_=False)\n",
    "                if batch_idx % batch_save_interval == 0:\n",
    "                    self.validate(val_iterator, epoch)\n",
    "                    self.save_checkpoint(epoch, batch_idx, save_path)\n",
    "                    \n",
    "            print(f'ep{epoch-n_epochs}, Loss: {total_loss}')\n",
    "\n",
    "    def validate(self, val_iterator, epoch):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_hat, y = [], []\n",
    "            \n",
    "            for i,edge_batches in enumerate(val_iterator()):\n",
    "                print(f'{i}/{self.val_batch_len-1}', end='\\r')\n",
    "                for batch in edge_batches:  # each batch here is one edge type, since we want to learn for all edge types\n",
    "\n",
    "                    batch = batch.to(self.device)\n",
    "                    hetero_out = model(batch.x_dict, batch.edge_index_dict, batch.edge_weight_dict)  # get model output\n",
    "\n",
    "                    # evaluate, calculate cosine sim and compute cross-entropy loss\n",
    "                    supervision_edge_type = self.get_supervision_edge_type(batch)\n",
    "                    src_type, dst_type = supervision_edge_type[0], supervision_edge_type[2]\n",
    "                    edge_label = batch[supervision_edge_type].edge_label\n",
    "                    edge_label_index = batch[supervision_edge_type].edge_label_index\n",
    "                    src_node_embeddings = hetero_out[src_type][edge_label_index[0]]\n",
    "                    dst_node_embeddings = hetero_out[dst_type][edge_label_index[1]]\n",
    "                    logits = F.cosine_similarity(src_node_embeddings, dst_node_embeddings, dim=-1)\n",
    "                    y_hat.append(torch.sigmoid(logits).cpu().detach())\n",
    "                    y.append(edge_label.to(torch.int).cpu().detach())\n",
    "            print('')\n",
    "\n",
    "            # save loss and metrics\n",
    "            self.metrics_history['val']['epoch'].append(epoch)\n",
    "            y_hat = torch.cat(y_hat)\n",
    "            y = torch.cat(y)\n",
    "            self.calculate_metrics('val', y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001) #2e-15\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "trainer = GNNTrainer(model, criterion, optimizer, device)\n",
    "#trainer.load_checkpoint('./checkpoints/checkpoint_0_300.pt')\n",
    "\n",
    "\n",
    "# for tqdm\n",
    "trainer.train_batch_len = train_batch_len\n",
    "trainer.val_batch_len = val_batch_len\n",
    "\n",
    "trainer.train(train_iterator, val_iterator, start_epoch=300, n_epochs=200, batch_save_interval=100, save_path='checkpoints')\n",
    "# trainer.validate(val_dataloader)\n",
    "# trainer.plot_losses()\n",
    "# trainer.load_checkpoint('./checkpoints/checkpoint_100.pt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
