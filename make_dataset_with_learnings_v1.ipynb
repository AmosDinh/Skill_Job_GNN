{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf5b79b-08ce-4a17-a680-d84616874a72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is the base datadaset.\n",
    "It includes categorical, numerical and (date, to numerical) features.\n",
    "text is encoded to embeddings.\n",
    "Throughout the notebook, .pt (pickles) are used, which need to be deleted in case of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f1e8de5-e4c5-48ad-ab17-0fffb6be9ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d89251-d276-44cd-b97d-5c017821ecc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def is_running_in_databricks() -> bool:\n",
    "    return \"DATABRICKS_RUNTIME_VERSION\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f049aac1-da04-440c-b744-3216edb68c98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if \"DATABRICKS_RUNTIME_VERSION\" in os.environ:\n",
    "  #CUDA = 'cu121' \n",
    "  \n",
    "  import os\n",
    "  \n",
    "  !pip install torch==2.1.0  torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "  import torch\n",
    "  #os.environ['TORCH'] = torch.__version__\n",
    "  #print(torch.__version__)\n",
    "  #torch_version = '2.0.0+cu118'\n",
    "  \n",
    "  #!pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html # torch_spline_conv\n",
    "  !pip install torch_geometric\n",
    "  !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "  #!pip install torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  #!pip install torch_scatter -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  #!pip install pyg_lib -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  !pip install sentence-transformers\n",
    "  !pip install torcheval\n",
    "  !pip install matplotlib\n",
    "  !pip install pandas\n",
    "  !pip install tensorboard\n",
    "  ROOT_FOLDER = 'dbfs:/FileStore/GraphNeuralNetworks/'\n",
    "else:\n",
    "  ROOT_FOLDER = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfff8fe5-9911-47c3-b4da-d35ff8dba790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if is_running_in_databricks():\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe9344d-c3a8-4fa4-9ae6-f901196eebb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def is_running_in_databricks() -> bool:\n",
    "    return \"DATABRICKS_RUNTIME_VERSION\" in os.environ\n",
    "\n",
    "if is_running_in_databricks():\n",
    "  ROOT_FOLDER = '/dbfs/FileStore/GraphNeuralNetworks/'\n",
    "else:\n",
    "  ROOT_FOLDER = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a4bd4f-cf5b-4e77-bb19-25b00019895b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ce2dd9-9b0f-4e26-bbda-289d32268ce4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "p1oM_TnG9udQ",
    "outputId": "094d85e4-269c-4906-ee4b-396d85fec6d1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cddd5d7-8118-424b-96c1-45eb99fbd404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_files = [\n",
    "    'final_courses_and_programs',\n",
    "    'final_qualifications',\n",
    "    'final_skills',\n",
    "\n",
    "    'final_qualification_skill_edges',\n",
    "    'final_course_and_program_skill_edges',\n",
    "    'final_course_qualification_edges',\n",
    "\n",
    "    'final_course_and_programs_student_edges',\n",
    "\n",
    "    'final_people',\n",
    "    'final_jobs',\n",
    "    'final_organizations',\n",
    "    'final_job_student_edges',\n",
    "    'final_supervisor_supervisee_edges',\n",
    "    'final_organization_student_edges',\n",
    "    'fixed_job_job_edges',\n",
    "    'fixed_job_skill_edges',\n",
    "    'fixed_broader_job_job_edges',\n",
    "    'final_skill_skill_edges'\n",
    "    #'job_skill_edges_tfidf_100mio_50kid_800kskill'\n",
    "]\n",
    "df_names= []\n",
    "for name in dataset_files:\n",
    "    df = pd.read_csv(ROOT_FOLDER+'final_dataset_courseprograms_joined/'+name+'.csv', lineterminator='\\n')\n",
    "    print(name, df.shape[0])\n",
    "    globals()[name.replace('final_','').replace('fixed_','')] = df\n",
    "    df_names.append(name.replace('final_','').replace('fixed_',''))\n",
    "    \n",
    "import string\n",
    "word_list = [\n",
    "    \"or\", \"up\", \"it\", \"us\", \"race\", \"location\", \"systems\", \"tools\",\n",
    "    \"so\", \"addition\", \"id\", \"am\", \"edge\"\n",
    "] + list(set(list(string.ascii_lowercase))-set(['r']))\n",
    "job_skill_edges = job_skill_edges.loc[~job_skill_edges['skill'].isna()]\n",
    "job_skill_edges = job_skill_edges.loc[~job_skill_edges['skill'].isin(word_list)]\n",
    "\n",
    "\n",
    "# filter out all skills if the whole skill is a number only\n",
    "job_skill_edges = job_skill_edges.loc[~ job_skill_edges['skill'].str.isnumeric()]\n",
    "print('job_skill_edges', job_skill_edges.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "course_and_program_skill_edges = course_and_program_skill_edges.loc[~course_and_program_skill_edges['Skill'].isna()]\n",
    "skills = skills.loc[~skills['SKILL'].isna()]\n",
    "\n",
    "# convert create_dte to timestamp if it is not already\n",
    "x  = pd.to_datetime(courses_and_programs['CREATE_DTE'], unit='s', errors='coerce')\n",
    "courses_and_programs.loc[~x.isna(), 'CREATE_DTE'] = x\n",
    "\n",
    "\n",
    "\n",
    "courses_and_programs.loc[courses_and_programs['TITLE'].isna(), 'TITLE'] = ''\n",
    "courses_and_programs.loc[courses_and_programs['DESCRIPTION'].isna(), 'DESCRIPTION'] = ''\n",
    "qualifications.loc[qualifications['TITLE'].isna(), 'TITLE'] = ''\n",
    "qualifications.loc[qualifications['DESCRIPTION'].isna(), 'DESCRIPTION'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49058b7-031c-4abe-8437-2e6c4bf6df68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_skill_edges = skill_skill_edges[['skill','related_skill','cosine_sim_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d554e1-9e5e-4eca-9929-8ecb505bf9c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_skill_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9f1c95-df16-4555-a276-24984cd1fd5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get only skill edges where skills are in the skill list\n",
    "skill_skill_edges = skill_skill_edges.loc[skill_skill_edges['skill'].isin(skills['SKILL'])]\n",
    "skill_skill_edges = skill_skill_edges.loc[skill_skill_edges['related_skill'].isin(skills['SKILL'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23838a0-ff9e-493f-aecb-7ab9e7a58d46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_skill_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fdbbcb-2282-45a3-8861-7622b6aa18c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add some skills which are only in the job_skill_edges but not in the skills csv\n",
    "a = set(job_skill_edges['skill'].unique())\n",
    "b = set(skills['SKILL'].unique())\n",
    "\n",
    "skills_to_add = list(a-b)\n",
    "skills_to_add = pd.DataFrame({'SKILL':skills_to_add})\n",
    "skills = pd.concat([skills, skills_to_add], axis=0)\n",
    "skills = skills.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d6090f-10d7-45fa-93f9-8367c5e583b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# group by JOB_ID\n",
    "job_skill_edges_grouped = job_skill_edges.groupby('JOB_ID').median()\n",
    "job_skill_edges_grouped\n",
    "\n",
    "# plot hist for n_jobdesc_used\n",
    "job_skill_edges_grouped['n_jobdesc_used'].hist(bins=50, range=(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cdf80cb-72f5-44eb-8060-73200b910525",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter out jobs with less than 10 job descriptions\n",
    "#job_skill_edges = job_skill_edges.loc[job_skill_edges['n_jobdesc_used']>=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3293503c-d770-4c1b-826d-484a07872b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming 'job_skill_edges' is your DataFrame with the column 'scaled_tfidf'\n",
    "\n",
    "# # Calculate the threshold value (95th percentile)\n",
    "# threshold = np.percentile(job_skill_edges['scaled_tfidf'], 5)\n",
    "\n",
    "# # Replace values above the threshold with the threshold value\n",
    "# #job_skill_edges.loc[job_skill_edges['scaled_tfidf'] > threshold, 'scaled_tfidf'] = threshold\n",
    "\n",
    "# # Plot the KDE and threshold\n",
    "# sns.kdeplot(data=job_skill_edges['scaled_tfidf'])\n",
    "# plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "# plt.legend()\n",
    "# plt.xlabel('scaled_tfidf')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Kernel Density Estimation')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75da72f-2725-4d0a-a7cc-c4da68c4879b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for each df in df_names print the name and print the columns containing nans\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    print(globals()[name].columns[globals()[name].isna().any()].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80280340-d481-4edb-a4f1-077dc9b0d49d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del broader_job_job_edges['Short Title']\n",
    "broader_job_job_edges = broader_job_job_edges[['index_fixed','onet_index_fixed']]\n",
    "broader_job_job_edges['SRC_ID'] = broader_job_job_edges['index_fixed']\n",
    "broader_job_job_edges['DST_ID'] = broader_job_job_edges['onet_index_fixed']\n",
    "del broader_job_job_edges['index_fixed']\n",
    "del broader_job_job_edges['onet_index_fixed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f51d82-385b-4bf9-bb29-673d5fb413e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for each column which contains DTE DATE or Date or date, convert the column to timestamp, for all dfs in df_names, save the highest timestamp overall to timestamp_max\n",
    "cols_dte = []\n",
    "timestamp_max = 0\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if 'DTE' in col or 'DATE' in col or 'Date' in col or 'date' in col:\n",
    "            try:\n",
    "\n",
    "                globals()[name][col] = (pd.to_datetime(globals()[name][col]) - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "                timestamp_max = max(timestamp_max, globals()[name][col].max())\n",
    "                cols_dte.append(col)\n",
    "                print('>>> converted',col)\n",
    "            except:\n",
    "                print('could not convert', col)\n",
    "                pass\n",
    "\n",
    "# normalize all timestamps by timestamp max\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if 'DTE' in col or 'DATE' in col or 'Date' in col or 'date' in col:\n",
    "            try:\n",
    "                globals()[name][col] = globals()[name][col] / timestamp_max\n",
    "            except:\n",
    "                print('could not convert', col)\n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02b6f3d-2017-4e73-b189-1f4712da46ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get all columns not in DTE DATE or Date or date, for all df_names, if the first 20 rows of the column are not numeric, normalize the column by the max of the column\n",
    "cols_numeric = []\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if col not in cols_dte and 'DTE' not in col and 'DATE' not in col and 'Date' not in col and 'date' not in col and 'ID' not in col and 'index' not in col and 'alt_title' not in col:\n",
    "            if 'TOTAL' in col:\n",
    "                print(col)\n",
    "            try:\n",
    "                if not pd.to_numeric(globals()[name][col], errors='coerce').isna().any():\n",
    "                    globals()[name][col] = pd.to_numeric(globals()[name][col]) / pd.to_numeric(globals()[name][col]).max()\n",
    "                    print('>>> converted', col)\n",
    "                    cols_numeric.append(col)\n",
    "                    \n",
    "                else:\n",
    "                    print('--- did not convert', col)\n",
    "            except:\n",
    "                print('could not convert', col)\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec262c8-0655-4cd9-838a-8a5360a61105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for all columns not in col_dte and cols_numeric, if the column only contains unique values, save it to cols_id\n",
    "cols_id = []\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if col not in cols_dte and col not in cols_numeric:\n",
    "            if len(globals()[name][col].unique()) == globals()[name].shape[0] or '_ID' in col or 'index_x' in col or 'index_y' in col:\n",
    "                cols_id.append(col)\n",
    "                print('>>> ID col', col)\n",
    "            else:\n",
    "                print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9ee835-63a5-49ac-82b2-5cf9bc85ec23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for all columns not in col_dte and cols_numeric and cols_id, if the column only contains 2 different values, save it to cols_binary and encode the column to 0 and 1\n",
    "cols_binary = []\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if col not in cols_dte and col not in cols_numeric and col not in cols_id:\n",
    "            if len(globals()[name][col].unique()) == 2:\n",
    "                cols_binary.append(col)\n",
    "                print('>>> binary col', col)\n",
    "                globals()[name][col] = globals()[name][col].astype('category').cat.codes\n",
    "            else:\n",
    "                print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486e8d83-1870-4050-ad46-fa80b5745997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        print('  ', col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0130372c-2c50-4eb0-bd05-c54e12c9a1a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_domains = set()\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if 'DMN_ID' in col:\n",
    "            print(col)\n",
    "            unique_domains.update(globals()[name][col].unique())\n",
    "            print(name, globals()[name][col].unique())\n",
    "            \n",
    "print('unique domain ids' , unique_domains)\n",
    "\n",
    "# create onehot mapping for all domain ids\n",
    "domain_id_to_onehot = {}\n",
    "for i, domain_id in enumerate(unique_domains):\n",
    "    onehot = [0] * len(unique_domains)\n",
    "    onehot[i] = 1\n",
    "    domain_id_to_onehot[domain_id] = onehot\n",
    "\n",
    "# for all columns, if they are DMN_ID columns, replace the column with the onehot mapping\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if 'DMN_ID' in col:\n",
    "            print(col)\n",
    "            globals()[name][col] = globals()[name][col].apply(lambda x: domain_id_to_onehot[x])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e7f488-cece-4d6f-99e8-3bd266ac5190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in the qualification program edges, remap the qual_id to the learning item id of the qualification\n",
    "# so that we dont have to remap the skill qualification edges, which use the learning item id of the qualification\n",
    "\n",
    "qual_to_learning_id = {row.QUAL_ID:row.LEARNING_ITEM_ID for i, row in qualifications.iterrows()}\n",
    "\n",
    "\n",
    "course_qualification_edges['QUAL_LEARNING_ID'] = course_qualification_edges['QUAL_ID'].apply(lambda x: qual_to_learning_id[x])\n",
    "del course_qualification_edges['QUAL_ID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbec107-7c99-4ca6-80f4-5e8795a0a6a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cols = {\n",
    "    'courses_and_programs': {\n",
    "        'categorical': [\n",
    "            'CPNT_TYP_ID', 'CPNT_SRC_ID', 'LEARNING_TYPE', 'CPNT_CLASSIFICATION' #, 'DMN_ID' handle dmn id separately\n",
    "        ],\n",
    "        'remove':[\n",
    "            'REAL_ID', \n",
    "            'THUMBNAIL_URI',\n",
    "            #'TITLE_TRANSLATION_NEEDED',\n",
    "            #'DESCRIPTION_TRANSLATION_NEEDED',\n",
    "            #'DESC_TRANSLATION_NEEDED',\n",
    "            #\n",
    "        ],\n",
    "        'keepadditional':[\n",
    "            'TITLE_TRANSLATION_NEEDED',\n",
    "            'DESCRIPTION_TRANSLATION_NEEDED',\n",
    "            'DESC_TRANSLATION_NEEDED',\n",
    "            'NOTACTIVE'\n",
    "        ],\n",
    "        'text': [\n",
    "            'TITLE', 'DESCRIPTION'\n",
    "        ]\n",
    "    },\n",
    "    'qualifications': {\n",
    "        'categorical': [\n",
    "            'QUAL_TYP_ID', # 'DMN_ID' handle dmn id separately\n",
    "        ],\n",
    "        'remove':[\n",
    "            'REAL_ID', \n",
    "            #'TITLE_TRANSLATION_NEEDED',\n",
    "            #'DESCRIPTION_TRANSLATION_NEEDED',\n",
    "            #'DESC_TRANSLATION_NEEDED',\n",
    "            #'NOTACTIVE',\n",
    "            #\"LEARNING_ITEM_ID\"\n",
    "            'QUAL_ID'\n",
    "        ],\n",
    "        'keepadditional':[\n",
    "            'TITLE_TRANSLATION_NEEDED',\n",
    "            'DESCRIPTION_TRANSLATION_NEEDED',\n",
    "            'DESC_TRANSLATION_NEEDED',\n",
    "            'NOTACTIVE'\n",
    "        ],\n",
    "        'text': [\n",
    "            'TITLE', 'DESCRIPTION'\n",
    "        ]\n",
    "    },\n",
    "    'skills':{\n",
    "        'text':['SKILL']\n",
    "    },\n",
    "    'jobs':{\n",
    "        'remove':[\n",
    "            'O*NET-SOC Code', \n",
    "        ],\n",
    "        'text':['TITLE']\n",
    "    },\n",
    "     'job_job_edges':{\n",
    "        'remove':[\n",
    "            'O*NET-SOC Code', \n",
    "            'Related O*NET-SOC Code',\n",
    "            'Title',\n",
    "            'Related Title'\n",
    "            \n",
    "        ],\n",
    "    },\n",
    "     'job_skill_edges':{\n",
    "        'remove':[\n",
    "            'n_jobdesc_used'\n",
    "            \n",
    "        ],\n",
    "    },\n",
    "     'qualification_skill_edges':{\n",
    "        'remove':[\n",
    "            'IS_APPROXIMATE'\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "      'course_and_program_skill_edges':{\n",
    "        'remove':[\n",
    "            'IS_APPROXIMATE'\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "      'people':{\n",
    "          #'remove':[],\n",
    "          'kepadditional':['IS_ACTIVE_INFERRED'],\n",
    "          'categorical':['EMP_TYP_ID']\n",
    "      }\n",
    "    \n",
    "}\n",
    "# for all columns in cols, if the column is in the df, remove the column from the df\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if name in cols.keys() and 'remove' in cols[name].keys() and col in cols[name]['remove']:\n",
    "            print('>>> remove', col)\n",
    "            \n",
    "            globals()[name] = globals()[name].drop(col, axis=1)\n",
    "\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    print(globals()[name].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0387b50-b564-460b-b336-86825f4c0d90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for all dfs for all columns, if col in cols and categorical in cols[name] and col in df, convert the column to onehot categories\n",
    "onehot_and_numeric_mappings = {}\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    onehot_and_numeric_mappings[name] = {}\n",
    "    for col in globals()[name].columns:\n",
    "        if name in cols.keys() and 'categorical' in cols[name].keys() and col in cols[name]['categorical']:\n",
    "            print('>>> categorical', col)\n",
    "            unique_categories = globals()[name][col].unique()\n",
    "            to_onehot = {}\n",
    "            for i, category in enumerate(unique_categories):\n",
    "                onehot = [0] * len(unique_categories)\n",
    "                onehot[i] = 1\n",
    "                to_onehot[category] = onehot\n",
    "                \n",
    "            globals()[name][col] = globals()[name][col].apply(lambda x: to_onehot[x])\n",
    "            print(name, col, 'categories:',str(to_onehot))\n",
    "            onehot_and_numeric_mappings[name][col] = to_onehot\n",
    "            \n",
    "            \n",
    "job_job_edges['Relatedness Tier'] = job_job_edges['Relatedness Tier'].apply(lambda x: {'Primary-Short':1,'Primary-Long':0.75, 'Supplemental':0.5}[x])\n",
    "onehot_and_numeric_mappings['job_job_edges'] = {'Relatedness Tier':{'Primary-Short':1,'Primary-Long':0.75, 'Supplemental':0.5}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f326b4cc-224a-48fa-959b-6cea35b6619f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for all text columns apply all-mpnet-base-v2 sentence embeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "# for name in df_names:\n",
    "#     print(name)\n",
    "#     for col in globals()[name].columns:\n",
    "#         if name in cols.keys() and 'text' in cols[name].keys() and col in cols[name]['text']:\n",
    "#             if col =='DESCRIPTION':\n",
    "#                 # skip \n",
    "#                 continue \n",
    "            \n",
    "#             print('>>> text col, apply mpnet', col)\n",
    "            \n",
    "#             # save to pickle and if it already exists, load from pickle \n",
    "#             pickle_path = 'final_dataset_courseprograms_joined/'+name+'_'+col+'_mpnet.pkl'\n",
    "#             if os.path.exists(pickle_path):\n",
    "#                 if col == 'TITLE':\n",
    "#                     globals()[name]['TEXT_EMBEDDING'] = pd.read_pickle(pickle_path)\n",
    "#                 else:\n",
    "#                     globals()[name][col] = pd.read_pickle(pickle_path)\n",
    "#             else:\n",
    "#                 if col == 'TITLE': # combine Title and Description\n",
    "#                     globals()[name]['TEXT_EMBEDDING'] = globals()[name]['TITLE'] + '\\n' + globals()[name]['DESCRIPTION']\n",
    "#                     globals()[name]['TEXT_EMBEDDING'] = globals()[name]['TEXT_EMBEDDING'].apply(lambda x: model.encode(x))\n",
    "#                 else:\n",
    "#                     globals()[name][col] = globals()[name][col].apply(lambda x: model.encode(x))\n",
    "                    \n",
    "#                 globals()[name][col].to_pickle(pickle_path)\n",
    "       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96127cd3-8c30-4ef5-8d29-1c99e5f7071d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "\n",
    "batch_size = 1000  # Specify your batch size\n",
    "\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if name in cols.keys() and 'text' in cols[name].keys() and col in cols[name]['text']:\n",
    "            if col =='DESCRIPTION':\n",
    "                # skip \n",
    "                continue \n",
    "            \n",
    "            print('>>> text col, apply mpnet', col)\n",
    "            \n",
    "            # save to pickle and if it already exists, load from pickle \n",
    "            pickle_path = ROOT_FOLDER+'final_dataset_courseprograms_joined/'+name+'_'+col+'_mpnet.pkl'\n",
    "            if os.path.exists(pickle_path):\n",
    "                # if col == 'TITLE':\n",
    "                #     globals()[name]['TEXT_EMBEDDING'] = pd.read_pickle(pickle_path)\n",
    "                # else:\n",
    "                globals()[name]['TEXT_EMBEDDING'] = pd.read_pickle(pickle_path)\n",
    "            else:\n",
    "                num_batches = int(np.ceil(len(globals()[name][col]) / batch_size))\n",
    "                embeddings = []\n",
    "                \n",
    "                if col == 'TITLE' and name !='jobs':\n",
    "                    texts = globals()[name]['TITLE'] + '\\n' + globals()[name]['DESCRIPTION']\n",
    "                else:\n",
    "                    texts = globals()[name][col]\n",
    "                \n",
    "                for i in tqdm(range(num_batches)):\n",
    "                    batch_texts = texts[i * batch_size: (i + 1) * batch_size]\n",
    "                    batch_embeddings = model.encode(batch_texts.tolist())\n",
    "                    embeddings.extend(batch_embeddings)\n",
    "                    \n",
    "                # if col == 'TITLE' and name !='jobs':\n",
    "                #     globals()[name]['TEXT_EMBEDDING'] = embeddings\n",
    "                # else:\n",
    "                globals()[name]['TEXT_EMBEDDING'] = embeddings\n",
    "\n",
    "                pd.to_pickle(embeddings, pickle_path)\n",
    "                \n",
    "                \n",
    "skills = skills.drop_duplicates(subset=['SKILL']) # don't want to recompute with the dropped list, so we drop afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84428fe1-7c7a-47d4-a2f9-0e2d89a626d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# job job remapping\n",
    "# job skill remapping\n",
    "# job broader job remapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb93d67-69bb-4a15-8500-744b511acd26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# plot job_skill_edges scaled_tfidf, histogram, from 0 to 1, 100 bins\n",
    "plt.hist(job_skill_edges['scaled_tfidf'], bins=100, range=(0,0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8694a22f-6d83-4a19-b0c7-aa14d1a9f719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print all dfs in df_name if they have a name with edge in it\n",
    "for name in df_names:\n",
    "    if 'edge' in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440feab0-5b14-4122-9b0b-10d8a835fe7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# rename edges so we can do automatic mapping\n",
    "# for all of the below:\n",
    "# qualification_skill_edges\n",
    "# course_and_program_skill_edges\n",
    "# course_qualification_edges\n",
    "# course_and_programs_student_edges\n",
    "# job_student_edges\n",
    "# supervisor_supervisee_edges\n",
    "# organization_student_edges\n",
    "# job_job_edges\n",
    "# job_skill_edges\n",
    "# broader_job_job_edges\n",
    "# create a dictionary which has the column renamed for each df\n",
    "rename_dict = {}\n",
    "rename_dict['qualification_skill_edges'] = {'LEARNING_ITEM_ID':'qualifications_id', 'Skill':'skills_id'}\n",
    "rename_dict['course_and_program_skill_edges'] = {'Skill':'skills_id','LEARNING_ITEM_ID':'courses_and_programs_id'}\n",
    "rename_dict['course_qualification_edges'] = {'LEARNING_ITEM_ID':'courses_and_programs_id', 'QUAL_LEARNING_ID':'qualifications_id'}\n",
    "rename_dict['course_and_programs_student_edges'] = {'LEARNING_ITEM_ID':'courses_and_programs_id', 'STUD_ID':'people_id'}\n",
    "rename_dict['job_student_edges'] = {'ONET_ID':'jobs_id','STUD_ID':'people_id'}\n",
    "rename_dict['supervisor_supervisee_edges'] = {'STUD_ID':'people_id1', 'SUPER_ID':'people_id2'}\n",
    "rename_dict['organization_student_edges'] = {'ORG_ID':'organizations_id', 'STUD_ID':'people_id'}\n",
    "rename_dict['job_job_edges'] = {'SRC_ID':'jobs_id1','DST_ID':'jobs_id2'}\n",
    "rename_dict['job_skill_edges'] = {'skill':'skills_id','JOB_ID':'jobs_id'}\n",
    "rename_dict['broader_job_job_edges'] = {'SRC_ID':'jobs_id1','DST_ID':'jobs_id2'}\n",
    "rename_dict['skill_skill_edges'] = {'skill':'skills_id1','related_skill':'skills_id2'}\n",
    "\n",
    "# same for the node dfs\n",
    "rename_dict['courses_and_programs'] = {'LEARNING_ITEM_ID':'courses_and_programs_id'}\n",
    "rename_dict['qualifications'] = {'LEARNING_ITEM_ID':'qualifications_id'}\n",
    "#rename_dict['skills'] = {'SKILL':'skills_id'}\n",
    "rename_dict['jobs'] = {'ID':'jobs_id'}\n",
    "rename_dict['people'] = {'STUD_ID':'people_id'}\n",
    "rename_dict['organizations'] = {'ORG_ID':'organizations_id'}\n",
    "\n",
    "\n",
    "# rename all the columns correspondingly\n",
    "for name in df_names:\n",
    "    if name in rename_dict.keys():\n",
    "        globals()[name] = globals()[name].rename(columns=rename_dict[name])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f47f19-1943-486c-96ad-63f858c33a20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print out columns for all edge types:\n",
    "for name in df_names:\n",
    "    if 'edge' in name:\n",
    "        print(name)\n",
    "        print(globals()[name].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0485f46-56a1-4d5c-bcab-49993e85a21c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in the skills df, rename the SKILL column to skill_id and create a mapping which maps the skill to the skill_id\n",
    "# add the mapping to the onehot_and_numeric_mappings\n",
    "# change the values in the skill_id column to the skill_id mapping\n",
    "# add the original skill name as TITLE\n",
    "if 'skills' in onehot_and_numeric_mappings.keys() and 'skills_id' not in onehot_and_numeric_mappings['skills'].keys() or 'skills' not in onehot_and_numeric_mappings.keys():\n",
    "    skills = skills.rename(columns={'SKILL':'skills_id'})\n",
    "    skills['TITLE'] = skills['skills_id']\n",
    "    skill_to_skill_id = {}\n",
    "    for i, skill in enumerate(skills['skills_id'].unique()):\n",
    "        skill_to_skill_id[skill] = i+1\n",
    "    skills['skills_id'] = skills['skills_id'].apply(lambda x: skill_to_skill_id[x])\n",
    "    onehot_and_numeric_mappings['skills'] = {'skills_id':skill_to_skill_id}\n",
    "\n",
    "# for all edge dfs in df_names, if the df has a column called skill_id, change the values in the skill_id column to the skill_id mapping\n",
    "for name in df_names:\n",
    "    if 'edge' in name:\n",
    "        for postfix in ['', '1', '2']:\n",
    "            if 'skills_id'+postfix in globals()[name].columns:\n",
    "                print(name)\n",
    "                globals()[name]['skills_id'+postfix] = globals()[name]['skills_id'+postfix].apply(lambda x: skill_to_skill_id[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd173c81-532d-467a-810f-5e8a3733050f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# substract 1 from each id column\n",
    "# for all df_names, if the df has a column called id, substract 1 from the column\n",
    "if 'runonce1' not in globals().keys():\n",
    "    runonce1 = True\n",
    "    for name in df_names:\n",
    "        #if 'edge' in name:\n",
    "        if 'skills_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['skills_id'] = globals()[name]['skills_id'] - 1\n",
    "        if 'skills_id1' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['skills_id1'] = globals()[name]['skills_id1'] - 1\n",
    "        if 'skills_id2' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['skills_id2'] = globals()[name]['skills_id2'] - 1\n",
    "        if 'qualifications_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['qualifications_id'] = globals()[name]['qualifications_id'] - 1\n",
    "#        if name =='qualification_skill_edges':\n",
    "#            globals()[name]['LEARNING_ITEM_ID'] =  globals()[name]['LEARNING_ITEM_ID'] - 1\n",
    "        if 'courses_and_programs_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['courses_and_programs_id'] = globals()[name]['courses_and_programs_id'] - 1\n",
    "        if 'jobs_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['jobs_id'] = globals()[name]['jobs_id'] - 1\n",
    "        if 'jobs_id1' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['jobs_id1'] = globals()[name]['jobs_id1'] - 1\n",
    "        if 'jobs_id2' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['jobs_id2'] = globals()[name]['jobs_id2'] - 1\n",
    "        if 'people_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['people_id'] = globals()[name]['people_id'] - 1\n",
    "        if 'people_id1' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['people_id1'] = globals()[name]['people_id1'] - 1\n",
    "        if 'people_id2' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['people_id2'] = globals()[name]['people_id2'] - 1\n",
    "        if 'organizations_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['organizations_id'] = globals()[name]['organizations_id'] - 1\n",
    "\n",
    "    # do the same for people, skills, jobs, organizations, qualifications, courses_and_programs\n",
    "    # up top\n",
    "\n",
    "                \n",
    "    # people['people_id'] = people['people_id'] - 1\n",
    "    # skills['skills_id'] = skills['skills_id'] - 1\n",
    "    # jobs['jobs_id1'] = jobs['jobs_id1'] - 1\n",
    "    # jobs['jobs_id2'] = jobs['jobs_id2'] - 1\n",
    "    # organizations['organizations_id'] = organizations['organizations_id'] - 1\n",
    "    # qualifications['qualifications_id'] = qualifications['qualifications_id'] - 1\n",
    "    # #qualifications['LEARNING_ITEM_ID' ] = qualifications['LEARNING_ITEM_ID'] - 1\n",
    "    # courses_and_programs['courses_and_programs_id'] = courses_and_programs['courses_and_programs_id'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc322137-6a7c-420d-9f0c-ca2e8e20d7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "data = HeteroData()\n",
    "\n",
    "# for all dfs which have no \"edges\" in the name, add the df to the data object as a node, but ignore all columns containing id or ID\n",
    "for name in df_names:\n",
    "    if 'edge' not in name:\n",
    "        for col in globals()[name].columns:\n",
    "            if col.endswith('_id'):\n",
    "                # sort by the id column\n",
    "                globals()[name] = globals()[name].sort_values(by=[col])\n",
    "\n",
    "\n",
    "\n",
    "for name in df_names:\n",
    "    if 'edge' not in name:\n",
    "        \n",
    "            \n",
    "            tensor_obj = None\n",
    "            print(name)\n",
    "            \n",
    "            \n",
    "            for col in globals()[name].columns:\n",
    "                \n",
    "                if '_id' not in col: # lowercase only!\n",
    "                    if type(globals()[name][col].values[0]) == np.ndarray or type(globals()[name][col].values[0]) == list:\n",
    "                        temp_obj = torch.tensor(globals()[name][col].to_numpy().tolist())\n",
    "                        print('>>> ', col, 'list')\n",
    "                        print('    ', len(globals()[name][col].values[0]))\n",
    "                    elif type(globals()[name][col].values[0]) == str or ('keepadditional' in cols[name] and col in cols[name]['keepadditional']):\n",
    "                        data[name][col] = globals()[name][col].values\n",
    "                        print('>>> ', col, 'text')\n",
    "                        print('    ', type(globals()[name][col].values[0]))\n",
    "                        continue # important\n",
    "                        \n",
    "                    else:\n",
    "                        temp_obj = torch.tensor(globals()[name][col].values).reshape(-1,1)\n",
    "                        print('>>> ', col, 'number')\n",
    "                        print('    ', type(globals()[name][col].values[0]))\n",
    "                        \n",
    "                    if tensor_obj is None:\n",
    "                        tensor_obj = temp_obj\n",
    "                    else:\n",
    "                        tensor_obj = torch.cat((tensor_obj, temp_obj), dim=1)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            if tensor_obj is not None:\n",
    "                print('===', tensor_obj.shape)\n",
    "                data[name].x = tensor_obj\n",
    "            else:\n",
    "                data[name].num_nodes = globals()[name].shape[0]\n",
    "\n",
    "# for all edge dfs, take the two columns with _id in name and add them as edge to the data object\n",
    "for name in df_names:\n",
    "    if 'edge' in name:\n",
    "        print(name)\n",
    "        print(globals()[name].columns)\n",
    "        edge_cols = []\n",
    "        non_id_cols = []\n",
    "        for col in globals()[name].columns:\n",
    "            if '_id' in col:\n",
    "                edge_cols.append(col)\n",
    "            else:\n",
    "                non_id_cols.append(col)\n",
    "                \n",
    "        assert len(edge_cols) == 2, f'{name} needs 2 _id cols'\n",
    "        col_name = edge_cols[0].replace('_id1','').replace('_id2','').replace('_id',''), name.replace('_edges',''), edge_cols[1].replace('_id1','').replace('_id2','').replace('_id','')\n",
    "        print(name, col_name)\n",
    "        data[col_name].edge_index = torch.tensor(globals()[name][edge_cols].to_numpy().T)\n",
    "        if len(non_id_cols):\n",
    "            data[col_name].edge_attr = torch.tensor(globals()[name][non_id_cols].to_numpy())\n",
    "            \n",
    "        print('>>> edge_index', edge_cols)\n",
    "        print('>>> edge_attr', non_id_cols)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f055f27d-acc5-496b-a07c-6a8b20073a78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for typ in data.edge_types:\n",
    "    print()\n",
    "    print(typ)\n",
    "    continue  \n",
    "    for i in range(10000):\n",
    "        \n",
    "        a = data[typ].edge_index[0,i]\n",
    "        b = data[typ].edge_index[1,i]\n",
    "        if 'TITLE' in data[typ[0]].keys():\n",
    "            print(typ[0], data[typ[0]].TITLE[a])\n",
    "        if 'DESCRIPTION' in data[typ[0]].keys():\n",
    "            print(typ[0], data[typ[0]].DESCRIPTION[a])\n",
    "            \n",
    "            \n",
    "        print(typ[2])\n",
    "        if 'TITLE' in data[typ[2]].keys():\n",
    "            print(typ[2], data[typ[2]].TITLE[b])\n",
    "        if 'DESCRIPTION' in data[typ[2]].keys():\n",
    "            print(typ[2], data[typ[2]].DESCRIPTION[b])\n",
    "        \n",
    "        print('---')\n",
    "        \n",
    "        \n",
    "    print('======')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e85cb2-7d37-4eca-a0d9-04d73cfbb968",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.has_self_loops(), data.has_isolated_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46267660-d925-4558-b0df-63fe520a7d80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# gc collect\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c907bad3-6008-4a55-97a3-4ef8367fdf48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "       T.RemoveIsolatedNodes(),\n",
    "       #T.ToUndirected(merge=False), # don't merge reversed edges into the original edge type\n",
    "       T.RemoveDuplicatedEdges(),\n",
    "])\n",
    "data_before = data\n",
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b385bba4-98ee-4b7f-b2ac-045f4093792e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for edge_type in data.edge_types:\n",
    "    print(edge_type)\n",
    "    print(data_before[edge_type].edge_index.shape)\n",
    "    print(data[edge_type].edge_index.shape)\n",
    "    print('---')\n",
    "\n",
    "for node_type in data.node_types:\n",
    "    print(node_type)\n",
    "    print(data_before[node_type].num_nodes)\n",
    "    print(data[node_type].num_nodes)\n",
    "    print('---')\n",
    "    \n",
    "# ignore the ones where we have duplicate edge, idk why they are there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec63383-fed9-4e6b-af76-01079b172457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from utils.plot_degree import plot_degree\n",
    "# add features:\n",
    "# node degrees\n",
    "from torch_geometric.utils import to_dense_adj, degree\n",
    "degrees = {}\n",
    "for node_type in data.node_types:\n",
    "    degrees[node_type] = {}\n",
    "    \n",
    "for edge_type in data.edge_types:\n",
    "    if edge_type[0] == edge_type[2]:\n",
    "        degrees[edge_type[0]][edge_type] = degree(data[edge_type].edge_index.flatten(), num_nodes=data[edge_type[0]].num_nodes)\n",
    "    else:\n",
    "        degrees[edge_type[0]][edge_type] = degree(data[edge_type].edge_index[0], num_nodes=data[edge_type[0]].num_nodes)\n",
    "        degrees[edge_type[2]][edge_type] = degree(data[edge_type].edge_index[1], num_nodes=data[edge_type[2]].num_nodes)\n",
    "\n",
    "# for each node type, add the degrees of all edge types and append it as 'total_degree' to the degrees dict\n",
    "for node_type in data.node_types:\n",
    "    degrees[node_type]['total_degree'] = torch.zeros(data[node_type].num_nodes)\n",
    "    for edge_type in degrees[node_type]:\n",
    "        if edge_type != 'total_degree':\n",
    "            degrees[node_type]['total_degree'] += degrees[node_type][edge_type] \n",
    "\n",
    "# for each node type print degree statistics for each edge type, and the total degree, max, min, mean, median. Also make a small degree plot\n",
    "for node_type in data.node_types:\n",
    "    print(node_type)\n",
    "    # create empty torch array with length of nodes\n",
    "    node_degrees = None\n",
    "    \n",
    "    for edge_type in degrees[node_type]:\n",
    "        print(edge_type, '\\nmean',degrees[node_type][edge_type].mean(), '\\nstd',degrees[node_type][edge_type].std(), '\\nmedian',degrees[node_type][edge_type].median(),'\\nmax', degrees[node_type][edge_type].max(),'\\nmin', degrees[node_type][edge_type].min())\n",
    "        print('We normalize by max degree')\n",
    "        #smaller figure\n",
    "        x = degrees[node_type]['total_degree'].numpy()\n",
    "        if node_type == edge_type[0]:\n",
    "            title_ = f'{node_type}, {\" \".join(edge_type[0].split(\"_\"))}--{\" \".join(edge_type[2].split(\"_\"))}'\n",
    "            save_file_ = f'{node_type}__{\" \".join(edge_type[0].split(\"_\"))}_{\" \".join(edge_type[2].split(\"_\"))}.png'\n",
    "        elif edge_type == 'total_degree':\n",
    "            title_ = f'{node_type}, total degree'\n",
    "            save_file_ = f'{node_type}__total_degree.png'\n",
    "        else:\n",
    "            title_ = f'{node_type}, {\" \".join(edge_type[2].split(\"_\"))}-{\" \".join(edge_type[0].split(\"_\"))}'\n",
    "            save_file_ = f'{node_type}__{\" \".join(edge_type[2].split(\"_\"))}_{\" \".join(edge_type[0].split(\"_\"))}.png'\n",
    "            \n",
    "        if not os.path.exists(ROOT_FOLDER+'plot_images'):\n",
    "            os.mkdir(ROOT_FOLDER+'plot_images')\n",
    "        plot_degree(x, title_, 'Degree', 'Count', ROOT_FOLDER+'plot_images/'+save_file_, savefig=True, xlim=[0, np.mean(x)+np.std(x)*1], color='#b8cce4')\n",
    "        \n",
    "        maximum = degrees[node_type][edge_type].max()\n",
    "        if node_degrees is None:\n",
    "            node_degrees = (degrees[node_type][edge_type]/maximum).unsqueeze(1)\n",
    "        else:\n",
    "            node_degrees = torch.cat((node_degrees, (degrees[node_type][edge_type]/maximum).unsqueeze(1)), dim=1)\n",
    "            \n",
    "        print('node degree shape',node_type, node_degrees.shape)\n",
    "        \n",
    "    if 'x' in data[node_type].keys():\n",
    "        data[node_type].x = torch.cat((data[node_type].x, node_degrees), dim=1)\n",
    "    else:\n",
    "        data[node_type].x = node_degrees\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91b381f-7ab1-4b9f-83be-4e379f56c09e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import gc\n",
    "from torch_geometric.utils import is_undirected\n",
    "\n",
    "\n",
    "''' def triangle_count(adj_matrix:SparseTensor):\n",
    "    # block wise triangle count, to avoid memory issues\n",
    "    diags = []\n",
    "    \n",
    "    row_block_size = 40\n",
    "    rows = adj_matrix.size(0)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print('using',device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        adj_matrix = adj_matrix.to(device)\n",
    "        counter = 0\n",
    "        for block in tqdm(range(0,rows, row_block_size), desc='blockwise sparse matrix-multiplication'):\n",
    "            if counter % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            start = block\n",
    "            end = min(block+row_block_size, rows)\n",
    "            adj_pow_3_block = adj_matrix[start:end].spspmm(adj_matrix).spspmm(adj_matrix)\n",
    "            diag_block = get_diag(adj_pow_3_block[:,start:]).cpu()\n",
    "            diags.append(diag_block)\n",
    "            counter +=1\n",
    "\n",
    "    \n",
    "    return 1/2 * torch.cat(diags, dim=0) '''\n",
    "def triangle_count(adj_matrix:SparseTensor):\n",
    "    # adj_matmul, blockwise, so kernel does not crash\n",
    "    # diag1((row1 to rowS) * full_matrix * full_matrix)\n",
    "    # diag2((rowS+1 to rowR) * full_matrix * full_matrix)\n",
    "    # ....\n",
    "    diags = []\n",
    "    \n",
    "    row_block_size = 20\n",
    "    rows = adj_matrix.size(0)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        adj_matrix = adj_matrix.to(device)\n",
    "        count = 0\n",
    "        for block in tqdm(range(0,rows, row_block_size), desc='blockwise sparse matrix-multiplication'):\n",
    "            if count % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            start = block\n",
    "            end = min(block+row_block_size, rows)\n",
    "            adj_pow_2_block = adj_matrix[start:end].spspmm(adj_matrix)\n",
    "            # get only the diag part now\n",
    "            for i in range(0, adj_pow_2_block.size(dim=0)):\n",
    "                diag_i = adj_pow_2_block[i,:].spspmm(adj_matrix[:,i]).to_dense().cpu()\n",
    "                diags.append(diag_i[0,0].item())\n",
    "            \n",
    "            count +=1\n",
    "    \n",
    "    return 1/2 * torch.tensor(diags)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_sparse.diag import get_diag\n",
    "\n",
    "def undirected_triangle_counts(edge_index, max_num_nodes): \n",
    "    \"\"\"Get triangles **per node**, to get count for whole graph, divide by 3\"\"\"\n",
    "    \n",
    "    if not is_undirected(edge_index):\n",
    "        print('converting edge index to undirected')\n",
    "        edge_index = to_undirected(edge_index)\n",
    "    \n",
    "    adj_matrix = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index[1].shape[0]), sparse_sizes=(max_num_nodes, max_num_nodes))\n",
    "    triangles = triangle_count(adj_matrix) \n",
    "    return triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02be1439-551b-4b4d-9c2d-a10a0f95f63b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for edge types where both ends are the same node type, add the triangle count for the node\n",
    "import torch \n",
    "if not os.path.exists(ROOT_FOLDER+'precomputed_triangles'):\n",
    "    os.mkdir(ROOT_FOLDER+'precomputed_triangles')\n",
    "for edge_type in data.edge_types:\n",
    "    if edge_type[1] == 'broader_job_job' or edge_type[1] == 'supervisor_supervisee':\n",
    "        continue # there are no triangles\n",
    "\n",
    "    if edge_type[0] == edge_type[2]:\n",
    "        print('')\n",
    "        print('add triangles to', edge_type[0])\n",
    "        print(edge_type)\n",
    "        # if file already in pickle, load from pickle, folder is precomputed_triangles\n",
    "        pickle_path = ROOT_FOLDER+'precomputed_triangles/'+edge_type[0]+'_'+('_'.join(edge_type))+'.pt'\n",
    "        if os.path.exists(pickle_path):\n",
    "            triangles = torch.load(pickle_path)\n",
    "        else:\n",
    "            triangles = undirected_triangle_counts(data[edge_type].edge_index, data[edge_type[0]].num_nodes)\n",
    "            torch.save(triangles, pickle_path)\n",
    "            \n",
    "        print(edge_type[0], 'triangles', '\\nmean', triangles.mean(),'\\nstd', triangles.std(), '\\nmedian',triangles.median(), '\\nmax',triangles.max(), '\\nmin',triangles.min())\n",
    "        \n",
    "        if 'x' in data[edge_type[0]].keys():\n",
    "            data[edge_type[0]].x = torch.cat([data[edge_type[0]].x, triangles.unsqueeze(1)/triangles.max()], dim=1)\n",
    "        else:\n",
    "            data[edge_type[0]].x = triangles.unsqueeze(1)\n",
    "            \n",
    "\n",
    "\n",
    "print('add homogeneous triangles')\n",
    "homogeneous_data = data.to_homogeneous()\n",
    "if os.path.exists(ROOT_FOLDER+'precomputed_triangles/homogeneous.pt'):\n",
    "    homogenous_triangles = torch.load(ROOT_FOLDER+'precomputed_triangles/homogeneous.pt')\n",
    "else:\n",
    "    homogenous_triangles = undirected_triangle_counts(homogeneous_data.edge_index, homogeneous_data.num_nodes)\n",
    "    torch.save(homogenous_triangles, ROOT_FOLDER+'precomputed_triangles/homogeneous.pt')\n",
    "    \n",
    "print('homogeneous triangles', '\\nmean', homogenous_triangles.mean(),'\\nstd', homogenous_triangles.std(), '\\nmedian',homogenous_triangles.median(), '\\nmax',homogenous_triangles.max(), '\\nmin',homogenous_triangles.min())\n",
    "\n",
    "max_triangles = homogenous_triangles.max()\n",
    "for i, node_type in zip(homogeneous_data.node_type.unique(), data.node_types):\n",
    "    mask =  homogeneous_data.node_type == i.item()\n",
    "    \n",
    "    if 'x' in data[node_type].keys():\n",
    "        data[node_type].x = torch.cat([data[node_type].x, homogenous_triangles[mask].unsqueeze(1)/max_triangles], dim=1)\n",
    "    else:\n",
    "        data[node_type].x = homogenous_triangles[mask].unsqueeze(1)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6488d0c4-7dc8-4477-bfa9-a485c6d264b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdd607d-d434-40ad-ab75-f9d9d65fa51a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "homogeneous_data.node_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85812a3b-feff-45f8-b6f0-5bb9472d2c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "homogeneous_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8830d3-2d1a-4045-9694-514cc55c8382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b45406-041b-4967-b4c9-2c6aa1f2fb74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['courses_and_programs'].x.shape, torch.sum(homogeneous_data.node_type==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1909bdcf-f2a3-4929-9b33-5c998261ad23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "triangles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d9053f-fcc4-4d50-883f-2edfd16cabd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = T.ToUndirected(merge=False)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98787bca-cbbf-4b52-94b4-87377b916dae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "\n",
    "filename = ROOT_FOLDER+'HeteroData_Learnings_normalized_triangles_withadditionaldata_v1.pt'\n",
    "if os.path.exists(filename):\n",
    "    # data = HeteroData.from_dict(torch.load('./'+filename))\n",
    "    raise Exception('File already exists')\n",
    "else:\n",
    "    torch.save(data.to_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82247447-261c-4d71-9bf9-bf2cbb2d856d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b28dc6b-1d51-4021-862a-c4e3543ca09a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#data['jobs','job_job','jobs']['abc'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f49e79-612e-4dd2-ab37-b7b1c619115a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "make_dataset_with_learnings_v1",
   "widgets": {}
  },
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyg_torch21",
   "language": "python",
   "name": "pyg_torch21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
