{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cf5b79b-08ce-4a17-a680-d84616874a72",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This is the base datadaset.\n",
    "It includes categorical, numerical and (date, to numerical) features.\n",
    "text is encoded to embeddings.\n",
    "Throughout the notebook, .pt (pickles) are used, which need to be deleted in case of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f1e8de5-e4c5-48ad-ab17-0fffb6be9ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d89251-d276-44cd-b97d-5c017821ecc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def is_running_in_databricks() -> bool:\n",
    "    return \"DATABRICKS_RUNTIME_VERSION\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f049aac1-da04-440c-b744-3216edb68c98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if is_running_in_databricks():\n",
    "  CUDA = 'cu121' \n",
    "  \n",
    "  import os\n",
    "  \n",
    "  !pip install torch==2.1.0  torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "  import torch\n",
    "  #os.environ['TORCH'] = torch.__version__\n",
    "  #print(torch.__version__)\n",
    "  #torch_version = '2.0.0+cu118'\n",
    "  \n",
    "  #!pip install pyg_lib torch_scatter torch_sparse torch_cluster -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html # torch_spline_conv\n",
    "  !pip install torch_geometric\n",
    "  !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.1.0+cu121.html\n",
    "  #!pip install torch_sparse -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  #!pip install torch_scatter -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  #!pip install pyg_lib -f https://data.pyg.org/whl/torch-2.1.0+${CUDA}.html\n",
    "  !pip install sentence-transformers\n",
    "  !pip install torcheval\n",
    "  !pip install matplotlib\n",
    "  !pip install pandas\n",
    "  !pip install tensorboard\n",
    "  ROOT_FOLDER = 'dbfs:/FileStore/GraphNeuralNetworks/'\n",
    "else:\n",
    "  ROOT_FOLDER = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfff8fe5-9911-47c3-b4da-d35ff8dba790",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if is_running_in_databricks():\n",
    "    dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fe9344d-c3a8-4fa4-9ae6-f901196eebb8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def is_running_in_databricks() -> bool:\n",
    "    return \"DATABRICKS_RUNTIME_VERSION\" in os.environ\n",
    "\n",
    "if is_running_in_databricks():\n",
    "  ROOT_FOLDER = '/dbfs/FileStore/GraphNeuralNetworks/'\n",
    "else:\n",
    "  ROOT_FOLDER = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a4bd4f-cf5b-4e77-bb19-25b00019895b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ce2dd9-9b0f-4e26-bbda-289d32268ce4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "p1oM_TnG9udQ",
    "outputId": "094d85e4-269c-4906-ee4b-396d85fec6d1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cddd5d7-8118-424b-96c1-45eb99fbd404",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_files = [\n",
    "    'final_courses_and_programs',\n",
    "    'final_qualifications',\n",
    "    'final_skills',\n",
    "\n",
    "    'final_qualification_skill_edges',\n",
    "    'final_course_and_program_skill_edges',\n",
    "    'final_course_qualification_edges',\n",
    "\n",
    "    'final_course_and_programs_student_edges',\n",
    "\n",
    "    'final_people',\n",
    "    'final_jobs',\n",
    "    'final_organizations',\n",
    "    'final_job_student_edges',\n",
    "    'final_supervisor_supervisee_edges',\n",
    "    'final_organization_student_edges',\n",
    "    'fixed_job_job_edges',\n",
    "    'fixed_job_skill_edges',\n",
    "    'fixed_broader_job_job_edges',\n",
    "    'final_skill_skill_edges'\n",
    "    #'job_skill_edges_tfidf_100mio_50kid_800kskill'\n",
    "]\n",
    "df_names= []\n",
    "for name in dataset_files:\n",
    "    df = pd.read_csv(ROOT_FOLDER+'final_dataset_courseprograms_joined/'+name+'.csv', lineterminator='\\n')\n",
    "    print(name, df.shape[0])\n",
    "    globals()[name.replace('final_','').replace('fixed_','')] = df\n",
    "    df_names.append(name.replace('final_','').replace('fixed_',''))\n",
    "    \n",
    "import string\n",
    "word_list = [\n",
    "    \"or\", \"up\", \"it\", \"us\", \"race\", \"location\", \"systems\", \"tools\",\n",
    "    \"so\", \"addition\", \"id\", \"am\", \"edge\"\n",
    "] + list(set(list(string.ascii_lowercase))-set(['r']))\n",
    "job_skill_edges = job_skill_edges.loc[~job_skill_edges['skill'].isna()]\n",
    "job_skill_edges = job_skill_edges.loc[~job_skill_edges['skill'].isin(word_list)]\n",
    "\n",
    "\n",
    "# filter out all skills if the whole skill is a number only\n",
    "job_skill_edges = job_skill_edges.loc[~ job_skill_edges['skill'].str.isnumeric()]\n",
    "print('job_skill_edges', job_skill_edges.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "course_and_program_skill_edges = course_and_program_skill_edges.loc[~course_and_program_skill_edges['Skill'].isna()]\n",
    "skills = skills.loc[~skills['SKILL'].isna()]\n",
    "\n",
    "# convert create_dte to timestamp if it is not already\n",
    "x  = pd.to_datetime(courses_and_programs['CREATE_DTE'], unit='s', errors='coerce')\n",
    "courses_and_programs.loc[~x.isna(), 'CREATE_DTE'] = x\n",
    "\n",
    "\n",
    "\n",
    "courses_and_programs.loc[courses_and_programs['TITLE'].isna(), 'TITLE'] = ''\n",
    "courses_and_programs.loc[courses_and_programs['DESCRIPTION'].isna(), 'DESCRIPTION'] = ''\n",
    "qualifications.loc[qualifications['TITLE'].isna(), 'TITLE'] = ''\n",
    "qualifications.loc[qualifications['DESCRIPTION'].isna(), 'DESCRIPTION'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e49058b7-031c-4abe-8437-2e6c4bf6df68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_skill_edges = skill_skill_edges[['skill','related_skill','cosine_sim_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53d554e1-9e5e-4eca-9929-8ecb505bf9c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_skill_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9f1c95-df16-4555-a276-24984cd1fd5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get only skill edges where skills are in the skill list\n",
    "skill_skill_edges = skill_skill_edges.loc[skill_skill_edges['skill'].isin(skills['SKILL'])]\n",
    "skill_skill_edges = skill_skill_edges.loc[skill_skill_edges['related_skill'].isin(skills['SKILL'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23838a0-ff9e-493f-aecb-7ab9e7a58d46",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_skill_edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fdbbcb-2282-45a3-8861-7622b6aa18c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add some skills which are only in the job_skill_edges but not in the skills csv\n",
    "a = set(job_skill_edges['skill'].unique())\n",
    "b = set(skills['SKILL'].unique())\n",
    "\n",
    "skills_to_add = list(a-b)\n",
    "skills_to_add = pd.DataFrame({'SKILL':skills_to_add})\n",
    "skills = pd.concat([skills, skills_to_add], axis=0)\n",
    "skills = skills.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d6090f-10d7-45fa-93f9-8367c5e583b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# group by JOB_ID\n",
    "job_skill_edges_grouped = job_skill_edges.groupby('JOB_ID').median()\n",
    "job_skill_edges_grouped\n",
    "\n",
    "# plot hist for n_jobdesc_used\n",
    "job_skill_edges_grouped['n_jobdesc_used'].hist(bins=50, range=(0,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cdf80cb-72f5-44eb-8060-73200b910525",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# filter out jobs with less than 10 job descriptions\n",
    "#job_skill_edges = job_skill_edges.loc[job_skill_edges['n_jobdesc_used']>=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3293503c-d770-4c1b-826d-484a07872b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming 'job_skill_edges' is your DataFrame with the column 'scaled_tfidf'\n",
    "\n",
    "# # Calculate the threshold value (95th percentile)\n",
    "# threshold = np.percentile(job_skill_edges['scaled_tfidf'], 5)\n",
    "\n",
    "# # Replace values above the threshold with the threshold value\n",
    "# #job_skill_edges.loc[job_skill_edges['scaled_tfidf'] > threshold, 'scaled_tfidf'] = threshold\n",
    "\n",
    "# # Plot the KDE and threshold\n",
    "# sns.kdeplot(data=job_skill_edges['scaled_tfidf'])\n",
    "# plt.axvline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "# plt.legend()\n",
    "# plt.xlabel('scaled_tfidf')\n",
    "# plt.ylabel('Density')\n",
    "# plt.title('Kernel Density Estimation')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75da72f-2725-4d0a-a7cc-c4da68c4879b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for each df in df_names print the name and print the columns containing nans\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    print(globals()[name].columns[globals()[name].isna().any()].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80280340-d481-4edb-a4f1-077dc9b0d49d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "del broader_job_job_edges['Short Title']\n",
    "broader_job_job_edges = broader_job_job_edges[['index_fixed','onet_index_fixed']]\n",
    "broader_job_job_edges['SRC_ID'] = broader_job_job_edges['index_fixed']\n",
    "broader_job_job_edges['DST_ID'] = broader_job_job_edges['onet_index_fixed']\n",
    "del broader_job_job_edges['index_fixed']\n",
    "del broader_job_job_edges['onet_index_fixed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f51d82-385b-4bf9-bb29-673d5fb413e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# for each column which contains DTE DATE or Date or date, convert the column to timestamp, for all dfs in df_names, save the highest timestamp overall to timestamp_max\n",
    "cols_dte = []\n",
    "timestamp_max = 0\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if 'DTE' in col or 'DATE' in col or 'Date' in col or 'date' in col:\n",
    "            try:\n",
    "\n",
    "                globals()[name][col] = (pd.to_datetime(globals()[name][col]) - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "                timestamp_max = max(timestamp_max, globals()[name][col].max())\n",
    "                cols_dte.append(col)\n",
    "                print('>>> converted',col)\n",
    "            except:\n",
    "                print('could not convert', col)\n",
    "                pass\n",
    "\n",
    "# normalize all timestamps by timestamp max\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if 'DTE' in col or 'DATE' in col or 'Date' in col or 'date' in col:\n",
    "            try:\n",
    "                globals()[name][col] = globals()[name][col] / timestamp_max\n",
    "            except:\n",
    "                print('could not convert', col)\n",
    "                pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02b6f3d-2017-4e73-b189-1f4712da46ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get all columns not in DTE DATE or Date or date, for all df_names, if the first 20 rows of the column are not numeric, normalize the column by the max of the column\n",
    "cols_numeric = []\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if col not in cols_dte and 'DTE' not in col and 'DATE' not in col and 'Date' not in col and 'date' not in col and 'ID' not in col and 'index' not in col and 'alt_title' not in col:\n",
    "            if 'TOTAL' in col:\n",
    "                print(col)\n",
    "            try:\n",
    "                if not pd.to_numeric(globals()[name][col], errors='coerce').isna().any():\n",
    "                    globals()[name][col] = pd.to_numeric(globals()[name][col]) / pd.to_numeric(globals()[name][col]).max()\n",
    "                    print('>>> converted', col)\n",
    "                    cols_numeric.append(col)\n",
    "                    \n",
    "                else:\n",
    "                    print('--- did not convert', col)\n",
    "            except:\n",
    "                print('could not convert', col)\n",
    "                pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dec262c8-0655-4cd9-838a-8a5360a61105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for all columns not in col_dte and cols_numeric, if the column only contains unique values, save it to cols_id\n",
    "cols_id = []\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if col not in cols_dte and col not in cols_numeric:\n",
    "            if len(globals()[name][col].unique()) == globals()[name].shape[0] or '_ID' in col or 'index_x' in col or 'index_y' in col:\n",
    "                cols_id.append(col)\n",
    "                print('>>> ID col', col)\n",
    "            else:\n",
    "                print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9ee835-63a5-49ac-82b2-5cf9bc85ec23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for all columns not in col_dte and cols_numeric and cols_id, if the column only contains 2 different values, save it to cols_binary and encode the column to 0 and 1\n",
    "cols_binary = []\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if col not in cols_dte and col not in cols_numeric and col not in cols_id:\n",
    "            if len(globals()[name][col].unique()) == 2:\n",
    "                cols_binary.append(col)\n",
    "                print('>>> binary col', col)\n",
    "                globals()[name][col] = globals()[name][col].astype('category').cat.codes\n",
    "            else:\n",
    "                print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "486e8d83-1870-4050-ad46-fa80b5745997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        print('  ', col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0130372c-2c50-4eb0-bd05-c54e12c9a1a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_domains = set()\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if 'DMN_ID' in col:\n",
    "            print(col)\n",
    "            unique_domains.update(globals()[name][col].unique())\n",
    "            print(name, globals()[name][col].unique())\n",
    "            \n",
    "print('unique domain ids' , unique_domains)\n",
    "\n",
    "# create onehot mapping for all domain ids\n",
    "domain_id_to_onehot = {}\n",
    "for i, domain_id in enumerate(unique_domains):\n",
    "    onehot = [0] * len(unique_domains)\n",
    "    onehot[i] = 1\n",
    "    domain_id_to_onehot[domain_id] = onehot\n",
    "\n",
    "# for all columns, if they are DMN_ID columns, replace the column with the onehot mapping\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if 'DMN_ID' in col:\n",
    "            print(col)\n",
    "            globals()[name][col] = globals()[name][col].apply(lambda x: domain_id_to_onehot[x])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99e7f488-cece-4d6f-99e8-3bd266ac5190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in the qualification program edges, remap the qual_id to the learning item id of the qualification\n",
    "# so that we dont have to remap the skill qualification edges, which use the learning item id of the qualification\n",
    "\n",
    "qual_to_learning_id = {row.QUAL_ID:row.LEARNING_ITEM_ID for i, row in qualifications.iterrows()}\n",
    "\n",
    "\n",
    "course_qualification_edges['QUAL_LEARNING_ID'] = course_qualification_edges['QUAL_ID'].apply(lambda x: qual_to_learning_id[x])\n",
    "del course_qualification_edges['QUAL_ID'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbbec107-7c99-4ca6-80f4-5e8795a0a6a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "cols = {\n",
    "    'courses_and_programs': {\n",
    "        'categorical': [\n",
    "            'CPNT_TYP_ID', 'CPNT_SRC_ID', 'LEARNING_TYPE', 'CPNT_CLASSIFICATION' #, 'DMN_ID' handle dmn id separately\n",
    "        ],\n",
    "        'remove':[\n",
    "            'REAL_ID', \n",
    "            'THUMBNAIL_URI',\n",
    "            'TITLE_TRANSLATION_NEEDED',\n",
    "            'DESCRIPTION_TRANSLATION_NEEDED',\n",
    "            'DESC_TRANSLATION_NEEDED',\n",
    "            'NOTACTIVE'\n",
    "        ],\n",
    "        'text': [\n",
    "            'TITLE', 'DESCRIPTION'\n",
    "        ]\n",
    "    },\n",
    "    'qualifications': {\n",
    "        'categorical': [\n",
    "            'QUAL_TYP_ID', # 'DMN_ID' handle dmn id separately\n",
    "        ],\n",
    "        'remove':[\n",
    "            'REAL_ID', \n",
    "            'TITLE_TRANSLATION_NEEDED',\n",
    "            'DESCRIPTION_TRANSLATION_NEEDED',\n",
    "            'DESC_TRANSLATION_NEEDED',\n",
    "            'NOTACTIVE',\n",
    "            #\"LEARNING_ITEM_ID\"\n",
    "            'QUAL_ID'\n",
    "        ],\n",
    "        'text': [\n",
    "            'TITLE', 'DESCRIPTION'\n",
    "        ]\n",
    "    },\n",
    "    'skills':{\n",
    "        'text':['SKILL']\n",
    "    },\n",
    "    'jobs':{\n",
    "        'remove':[\n",
    "            'O*NET-SOC Code', \n",
    "        ],\n",
    "        'text':['TITLE']\n",
    "    },\n",
    "     'job_job_edges':{\n",
    "        'remove':[\n",
    "            'O*NET-SOC Code', \n",
    "            'Related O*NET-SOC Code',\n",
    "            'Title',\n",
    "            'Related Title'\n",
    "            \n",
    "        ],\n",
    "    },\n",
    "     'job_skill_edges':{\n",
    "        'remove':[\n",
    "            'n_jobdesc_used'\n",
    "            \n",
    "        ],\n",
    "    },\n",
    "     'qualification_skill_edges':{\n",
    "        'remove':[\n",
    "            'IS_APPROXIMATE'\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "      'course_and_program_skill_edges':{\n",
    "        'remove':[\n",
    "            'IS_APPROXIMATE'\n",
    "            \n",
    "        ]\n",
    "        \n",
    "    },\n",
    "      'people':{\n",
    "          'remove':['IS_ACTIVE_INFERRED'],\n",
    "          'categorical':['EMP_TYP_ID']\n",
    "      }\n",
    "    \n",
    "}\n",
    "# for all columns in cols, if the column is in the df, remove the column from the df\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if name in cols.keys() and 'remove' in cols[name].keys() and col in cols[name]['remove']:\n",
    "            print('>>> remove', col)\n",
    "            \n",
    "            globals()[name] = globals()[name].drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0387b50-b564-460b-b336-86825f4c0d90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for all dfs for all columns, if col in cols and categorical in cols[name] and col in df, convert the column to onehot categories\n",
    "onehot_and_numeric_mappings = {}\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    onehot_and_numeric_mappings[name] = {}\n",
    "    for col in globals()[name].columns:\n",
    "        if name in cols.keys() and 'categorical' in cols[name].keys() and col in cols[name]['categorical']:\n",
    "            print('>>> categorical', col)\n",
    "            unique_categories = globals()[name][col].unique()\n",
    "            to_onehot = {}\n",
    "            for i, category in enumerate(unique_categories):\n",
    "                onehot = [0] * len(unique_categories)\n",
    "                onehot[i] = 1\n",
    "                to_onehot[category] = onehot\n",
    "                \n",
    "            globals()[name][col] = globals()[name][col].apply(lambda x: to_onehot[x])\n",
    "            print(name, col, 'categories:',str(to_onehot))\n",
    "            onehot_and_numeric_mappings[name][col] = to_onehot\n",
    "            \n",
    "            \n",
    "job_job_edges['Relatedness Tier'] = job_job_edges['Relatedness Tier'].apply(lambda x: {'Primary-Short':1,'Primary-Long':0.75, 'Supplemental':0.5}[x])\n",
    "onehot_and_numeric_mappings['job_job_edges'] = {'Relatedness Tier':{'Primary-Short':1,'Primary-Long':0.75, 'Supplemental':0.5}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f326b4cc-224a-48fa-959b-6cea35b6619f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for all text columns apply all-mpnet-base-v2 sentence embeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "# for name in df_names:\n",
    "#     print(name)\n",
    "#     for col in globals()[name].columns:\n",
    "#         if name in cols.keys() and 'text' in cols[name].keys() and col in cols[name]['text']:\n",
    "#             if col =='DESCRIPTION':\n",
    "#                 # skip \n",
    "#                 continue \n",
    "            \n",
    "#             print('>>> text col, apply mpnet', col)\n",
    "            \n",
    "#             # save to pickle and if it already exists, load from pickle \n",
    "#             pickle_path = 'final_dataset_courseprograms_joined/'+name+'_'+col+'_mpnet.pkl'\n",
    "#             if os.path.exists(pickle_path):\n",
    "#                 if col == 'TITLE':\n",
    "#                     globals()[name]['TEXT_EMBEDDING'] = pd.read_pickle(pickle_path)\n",
    "#                 else:\n",
    "#                     globals()[name][col] = pd.read_pickle(pickle_path)\n",
    "#             else:\n",
    "#                 if col == 'TITLE': # combine Title and Description\n",
    "#                     globals()[name]['TEXT_EMBEDDING'] = globals()[name]['TITLE'] + '\\n' + globals()[name]['DESCRIPTION']\n",
    "#                     globals()[name]['TEXT_EMBEDDING'] = globals()[name]['TEXT_EMBEDDING'].apply(lambda x: model.encode(x))\n",
    "#                 else:\n",
    "#                     globals()[name][col] = globals()[name][col].apply(lambda x: model.encode(x))\n",
    "                    \n",
    "#                 globals()[name][col].to_pickle(pickle_path)\n",
    "       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96127cd3-8c30-4ef5-8d29-1c99e5f7071d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device='cuda')\n",
    "\n",
    "batch_size = 1000  # Specify your batch size\n",
    "\n",
    "for name in df_names:\n",
    "    print(name)\n",
    "    for col in globals()[name].columns:\n",
    "        if name in cols.keys() and 'text' in cols[name].keys() and col in cols[name]['text']:\n",
    "            if col =='DESCRIPTION':\n",
    "                # skip \n",
    "                continue \n",
    "            \n",
    "            print('>>> text col, apply mpnet', col)\n",
    "            \n",
    "            # save to pickle and if it already exists, load from pickle \n",
    "            pickle_path = ROOT_FOLDER+'final_dataset_courseprograms_joined/'+name+'_'+col+'_mpnet.pkl'\n",
    "            if os.path.exists(pickle_path):\n",
    "                # if col == 'TITLE':\n",
    "                #     globals()[name]['TEXT_EMBEDDING'] = pd.read_pickle(pickle_path)\n",
    "                # else:\n",
    "                globals()[name]['TEXT_EMBEDDING'] = pd.read_pickle(pickle_path)\n",
    "            else:\n",
    "                num_batches = int(np.ceil(len(globals()[name][col]) / batch_size))\n",
    "                embeddings = []\n",
    "                \n",
    "                if col == 'TITLE' and name !='jobs':\n",
    "                    texts = globals()[name]['TITLE'] + '\\n' + globals()[name]['DESCRIPTION']\n",
    "                else:\n",
    "                    texts = globals()[name][col]\n",
    "                \n",
    "                for i in tqdm(range(num_batches)):\n",
    "                    batch_texts = texts[i * batch_size: (i + 1) * batch_size]\n",
    "                    batch_embeddings = model.encode(batch_texts.tolist())\n",
    "                    embeddings.extend(batch_embeddings)\n",
    "                    \n",
    "                # if col == 'TITLE' and name !='jobs':\n",
    "                #     globals()[name]['TEXT_EMBEDDING'] = embeddings\n",
    "                # else:\n",
    "                globals()[name]['TEXT_EMBEDDING'] = embeddings\n",
    "\n",
    "                pd.to_pickle(embeddings, pickle_path)\n",
    "                \n",
    "                \n",
    "skills = skills.drop_duplicates(subset=['SKILL']) # don't want to recompute with the dropped list, so we drop afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84428fe1-7c7a-47d4-a2f9-0e2d89a626d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# job job remapping\n",
    "# job skill remapping\n",
    "# job broader job remapping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb93d67-69bb-4a15-8500-744b511acd26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "# plot job_skill_edges scaled_tfidf, histogram, from 0 to 1, 100 bins\n",
    "plt.hist(job_skill_edges['scaled_tfidf'], bins=100, range=(0,0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8694a22f-6d83-4a19-b0c7-aa14d1a9f719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print all dfs in df_name if they have a name with edge in it\n",
    "for name in df_names:\n",
    "    if 'edge' in name:\n",
    "        print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "440feab0-5b14-4122-9b0b-10d8a835fe7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# rename edges so we can do automatic mapping\n",
    "# for all of the below:\n",
    "# qualification_skill_edges\n",
    "# course_and_program_skill_edges\n",
    "# course_qualification_edges\n",
    "# course_and_programs_student_edges\n",
    "# job_student_edges\n",
    "# supervisor_supervisee_edges\n",
    "# organization_student_edges\n",
    "# job_job_edges\n",
    "# job_skill_edges\n",
    "# broader_job_job_edges\n",
    "# create a dictionary which has the column renamed for each df\n",
    "rename_dict = {}\n",
    "rename_dict['qualification_skill_edges'] = {'LEARNING_ITEM_ID':'qualifications_id', 'Skill':'skills_id'}\n",
    "rename_dict['course_and_program_skill_edges'] = {'Skill':'skills_id','LEARNING_ITEM_ID':'courses_and_programs_id'}\n",
    "rename_dict['course_qualification_edges'] = {'LEARNING_ITEM_ID':'courses_and_programs_id', 'QUAL_LEARNING_ID':'qualifications_id'}\n",
    "rename_dict['course_and_programs_student_edges'] = {'LEARNING_ITEM_ID':'courses_and_programs_id', 'STUD_ID':'people_id'}\n",
    "rename_dict['job_student_edges'] = {'ONET_ID':'jobs_id','STUD_ID':'people_id'}\n",
    "rename_dict['supervisor_supervisee_edges'] = {'STUD_ID':'people_id1', 'SUPER_ID':'people_id2'}\n",
    "rename_dict['organization_student_edges'] = {'ORG_ID':'organizations_id', 'STUD_ID':'people_id'}\n",
    "rename_dict['job_job_edges'] = {'SRC_ID':'jobs_id1','DST_ID':'jobs_id2'}\n",
    "rename_dict['job_skill_edges'] = {'skill':'skills_id','JOB_ID':'jobs_id'}\n",
    "rename_dict['broader_job_job_edges'] = {'SRC_ID':'jobs_id1','DST_ID':'jobs_id2'}\n",
    "rename_dict['skill_skill_edges'] = {'skill':'skills_id1','related_skill':'skills_id2'}\n",
    "\n",
    "# same for the node dfs\n",
    "rename_dict['courses_and_programs'] = {'LEARNING_ITEM_ID':'courses_and_programs_id'}\n",
    "rename_dict['qualifications'] = {'LEARNING_ITEM_ID':'qualifications_id'}\n",
    "#rename_dict['skills'] = {'SKILL':'skills_id'}\n",
    "rename_dict['jobs'] = {'ID':'jobs_id'}\n",
    "rename_dict['people'] = {'STUD_ID':'people_id'}\n",
    "rename_dict['organizations'] = {'ORG_ID':'organizations_id'}\n",
    "\n",
    "\n",
    "# rename all the columns correspondingly\n",
    "for name in df_names:\n",
    "    if name in rename_dict.keys():\n",
    "        globals()[name] = globals()[name].rename(columns=rename_dict[name])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f47f19-1943-486c-96ad-63f858c33a20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print out columns for all edge types:\n",
    "for name in df_names:\n",
    "    if 'edge' in name:\n",
    "        print(name)\n",
    "        print(globals()[name].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0485f46-56a1-4d5c-bcab-49993e85a21c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# in the skills df, rename the SKILL column to skill_id and create a mapping which maps the skill to the skill_id\n",
    "# add the mapping to the onehot_and_numeric_mappings\n",
    "# change the values in the skill_id column to the skill_id mapping\n",
    "# add the original skill name as TITLE\n",
    "if 'skills' in onehot_and_numeric_mappings.keys() and 'skills_id' not in onehot_and_numeric_mappings['skills'].keys() or 'skills' not in onehot_and_numeric_mappings.keys():\n",
    "    skills = skills.rename(columns={'SKILL':'skills_id'})\n",
    "    skills['TITLE'] = skills['skills_id']\n",
    "    skill_to_skill_id = {}\n",
    "    for i, skill in enumerate(skills['skills_id'].unique()):\n",
    "        skill_to_skill_id[skill] = i+1\n",
    "    skills['skills_id'] = skills['skills_id'].apply(lambda x: skill_to_skill_id[x])\n",
    "    onehot_and_numeric_mappings['skills'] = {'skills_id':skill_to_skill_id}\n",
    "\n",
    "# for all edge dfs in df_names, if the df has a column called skill_id, change the values in the skill_id column to the skill_id mapping\n",
    "for name in df_names:\n",
    "    if 'edge' in name:\n",
    "        for postfix in ['', '1', '2']:\n",
    "            if 'skills_id'+postfix in globals()[name].columns:\n",
    "                print(name)\n",
    "                globals()[name]['skills_id'+postfix] = globals()[name]['skills_id'+postfix].apply(lambda x: skill_to_skill_id[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd173c81-532d-467a-810f-5e8a3733050f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# substract 1 from each id column\n",
    "# for all df_names, if the df has a column called id, substract 1 from the column\n",
    "if 'runonce1' not in globals().keys():\n",
    "    runonce1 = True\n",
    "    for name in df_names:\n",
    "        #if 'edge' in name:\n",
    "        if 'skills_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['skills_id'] = globals()[name]['skills_id'] - 1\n",
    "        if 'skills_id1' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['skills_id1'] = globals()[name]['skills_id1'] - 1\n",
    "        if 'skills_id2' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['skills_id2'] = globals()[name]['skills_id2'] - 1\n",
    "        if 'qualifications_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['qualifications_id'] = globals()[name]['qualifications_id'] - 1\n",
    "#        if name =='qualification_skill_edges':\n",
    "#            globals()[name]['LEARNING_ITEM_ID'] =  globals()[name]['LEARNING_ITEM_ID'] - 1\n",
    "        if 'courses_and_programs_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['courses_and_programs_id'] = globals()[name]['courses_and_programs_id'] - 1\n",
    "        if 'jobs_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['jobs_id'] = globals()[name]['jobs_id'] - 1\n",
    "        if 'jobs_id1' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['jobs_id1'] = globals()[name]['jobs_id1'] - 1\n",
    "        if 'jobs_id2' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['jobs_id2'] = globals()[name]['jobs_id2'] - 1\n",
    "        if 'people_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['people_id'] = globals()[name]['people_id'] - 1\n",
    "        if 'people_id1' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['people_id1'] = globals()[name]['people_id1'] - 1\n",
    "        if 'people_id2' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['people_id2'] = globals()[name]['people_id2'] - 1\n",
    "        if 'organizations_id' in globals()[name].columns:\n",
    "            print(name)\n",
    "            globals()[name]['organizations_id'] = globals()[name]['organizations_id'] - 1\n",
    "\n",
    "    # do the same for people, skills, jobs, organizations, qualifications, courses_and_programs\n",
    "    # up top\n",
    "\n",
    "                \n",
    "    # people['people_id'] = people['people_id'] - 1\n",
    "    # skills['skills_id'] = skills['skills_id'] - 1\n",
    "    # jobs['jobs_id1'] = jobs['jobs_id1'] - 1\n",
    "    # jobs['jobs_id2'] = jobs['jobs_id2'] - 1\n",
    "    # organizations['organizations_id'] = organizations['organizations_id'] - 1\n",
    "    # qualifications['qualifications_id'] = qualifications['qualifications_id'] - 1\n",
    "    # #qualifications['LEARNING_ITEM_ID' ] = qualifications['LEARNING_ITEM_ID'] - 1\n",
    "    # courses_and_programs['courses_and_programs_id'] = courses_and_programs['courses_and_programs_id'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc322137-6a7c-420d-9f0c-ca2e8e20d7f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "data = HeteroData()\n",
    "\n",
    "# for all dfs which have no \"edges\" in the name, add the df to the data object as a node, but ignore all columns containing id or ID\n",
    "for name in df_names:\n",
    "    if 'edge' not in name:\n",
    "        for col in globals()[name].columns:\n",
    "            if col.endswith('_id'):\n",
    "                # sort by the id column\n",
    "                globals()[name] = globals()[name].sort_values(by=[col])\n",
    "        \n",
    "for name in df_names:\n",
    "    if 'edge' not in name:\n",
    "        \n",
    "            \n",
    "            tensor_obj = None\n",
    "            print(name)\n",
    "            \n",
    "            for col in globals()[name].columns:\n",
    "                \n",
    "                if '_id' not in col: # lowercase only!\n",
    "                    if type(globals()[name][col].values[0]) == np.ndarray or type(globals()[name][col].values[0]) == list:\n",
    "                        temp_obj = torch.tensor(globals()[name][col].to_numpy().tolist())\n",
    "                        print('>>> ', col, 'list')\n",
    "                        print('    ', len(globals()[name][col].values[0]))\n",
    "                    elif type(globals()[name][col].values[0]) == str:\n",
    "                        data[name][col] = globals()[name][col].values\n",
    "                        print('>>> ', col, 'text')\n",
    "                        print('    ', type(globals()[name][col].values[0]))\n",
    "                        continue # important\n",
    "                        \n",
    "                    else:\n",
    "                        temp_obj = torch.tensor(globals()[name][col].values).reshape(-1,1)\n",
    "                        print('>>> ', col, 'number')\n",
    "                        print('    ', type(globals()[name][col].values[0]))\n",
    "                        \n",
    "                    if tensor_obj is None:\n",
    "                        tensor_obj = temp_obj\n",
    "                    else:\n",
    "                        tensor_obj = torch.cat((tensor_obj, temp_obj), dim=1)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            if tensor_obj is not None:\n",
    "                print('===', tensor_obj.shape)\n",
    "                data[name].x = tensor_obj\n",
    "            else:\n",
    "                data[name].num_nodes = globals()[name].shape[0]\n",
    "\n",
    "# for all edge dfs, take the two columns with _id in name and add them as edge to the data object\n",
    "for name in df_names:\n",
    "    if 'edge' in name:\n",
    "        print(name)\n",
    "        print(globals()[name].columns)\n",
    "        edge_cols = []\n",
    "        non_id_cols = []\n",
    "        for col in globals()[name].columns:\n",
    "            if '_id' in col:\n",
    "                edge_cols.append(col)\n",
    "            else:\n",
    "                non_id_cols.append(col)\n",
    "                \n",
    "        assert len(edge_cols) == 2, f'{name} needs 2 _id cols'\n",
    "        col_name = edge_cols[0].replace('_id1','').replace('_id2','').replace('_id',''), name.replace('_edges',''), edge_cols[1].replace('_id1','').replace('_id2','').replace('_id','')\n",
    "        print(name, col_name)\n",
    "        data[col_name].edge_index = torch.tensor(globals()[name][edge_cols].to_numpy().T)\n",
    "        if len(non_id_cols):\n",
    "            data[col_name].edge_attr = torch.tensor(globals()[name][non_id_cols].to_numpy())\n",
    "            \n",
    "        print('>>> edge_index', edge_cols)\n",
    "        print('>>> edge_attr', non_id_cols)\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f055f27d-acc5-496b-a07c-6a8b20073a78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for typ in data.edge_types:\n",
    "    print()\n",
    "    print(typ)\n",
    "    continue  \n",
    "    for i in range(10000):\n",
    "        \n",
    "        a = data[typ].edge_index[0,i]\n",
    "        b = data[typ].edge_index[1,i]\n",
    "        if 'TITLE' in data[typ[0]].keys():\n",
    "            print(typ[0], data[typ[0]].TITLE[a])\n",
    "        if 'DESCRIPTION' in data[typ[0]].keys():\n",
    "            print(typ[0], data[typ[0]].DESCRIPTION[a])\n",
    "            \n",
    "            \n",
    "        print(typ[2])\n",
    "        if 'TITLE' in data[typ[2]].keys():\n",
    "            print(typ[2], data[typ[2]].TITLE[b])\n",
    "        if 'DESCRIPTION' in data[typ[2]].keys():\n",
    "            print(typ[2], data[typ[2]].DESCRIPTION[b])\n",
    "        \n",
    "        print('---')\n",
    "        \n",
    "        \n",
    "    print('======')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e85cb2-7d37-4eca-a0d9-04d73cfbb968",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.has_self_loops(), data.has_isolated_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46267660-d925-4558-b0df-63fe520a7d80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# gc collect\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c907bad3-6008-4a55-97a3-4ef8367fdf48",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "       T.RemoveIsolatedNodes(),\n",
    "       #T.ToUndirected(merge=False), # don't merge reversed edges into the original edge type\n",
    "       T.RemoveDuplicatedEdges(),\n",
    "])\n",
    "data_before = data\n",
    "data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b385bba4-98ee-4b7f-b2ac-045f4093792e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for edge_type in data.edge_types:\n",
    "    print(edge_type)\n",
    "    print(data_before[edge_type].edge_index.shape)\n",
    "    print(data[edge_type].edge_index.shape)\n",
    "    print('---')\n",
    "\n",
    "for node_type in data.node_types:\n",
    "    print(node_type)\n",
    "    print(data_before[node_type].num_nodes)\n",
    "    print(data[node_type].num_nodes)\n",
    "    print('---')\n",
    "    \n",
    "# ignore the ones where we have duplicate edge, idk why they are there\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec63383-fed9-4e6b-af76-01079b172457",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from utils.plot_degree import plot_degree\n",
    "# add features:\n",
    "# node degrees\n",
    "from torch_geometric.utils import to_dense_adj, degree\n",
    "degrees = {}\n",
    "for node_type in data.node_types:\n",
    "    degrees[node_type] = {}\n",
    "    \n",
    "for edge_type in data.edge_types:\n",
    "    if edge_type[0] == edge_type[2]:\n",
    "        degrees[edge_type[0]][edge_type] = degree(data[edge_type].edge_index.flatten(), num_nodes=data[edge_type[0]].num_nodes)\n",
    "    else:\n",
    "        degrees[edge_type[0]][edge_type] = degree(data[edge_type].edge_index[0], num_nodes=data[edge_type[0]].num_nodes)\n",
    "        degrees[edge_type[2]][edge_type] = degree(data[edge_type].edge_index[1], num_nodes=data[edge_type[2]].num_nodes)\n",
    "\n",
    "# for each node type, add the degrees of all edge types and append it as 'total_degree' to the degrees dict\n",
    "for node_type in data.node_types:\n",
    "    degrees[node_type]['total_degree'] = torch.zeros(data[node_type].num_nodes)\n",
    "    for edge_type in degrees[node_type]:\n",
    "        if edge_type != 'total_degree':\n",
    "            degrees[node_type]['total_degree'] += degrees[node_type][edge_type] \n",
    "\n",
    "# for each node type print degree statistics for each edge type, and the total degree, max, min, mean, median. Also make a small degree plot\n",
    "for node_type in data.node_types:\n",
    "    print(node_type)\n",
    "    # create empty torch array with length of nodes\n",
    "    node_degrees = None\n",
    "    \n",
    "    for edge_type in degrees[node_type]:\n",
    "        print(edge_type, '\\nmean',degrees[node_type][edge_type].mean(), '\\nstd',degrees[node_type][edge_type].std(), '\\nmedian',degrees[node_type][edge_type].median(),'\\nmax', degrees[node_type][edge_type].max(),'\\nmin', degrees[node_type][edge_type].min())\n",
    "        print('We normalize by max degree')\n",
    "        #smaller figure\n",
    "        x = degrees[node_type]['total_degree'].numpy()\n",
    "        if node_type == edge_type[0]:\n",
    "            title_ = f'{node_type}, {\" \".join(edge_type[0].split(\"_\"))}--{\" \".join(edge_type[2].split(\"_\"))}'\n",
    "            save_file_ = f'{node_type}__{\" \".join(edge_type[0].split(\"_\"))}_{\" \".join(edge_type[2].split(\"_\"))}.png'\n",
    "        elif edge_type == 'total_degree':\n",
    "            title_ = f'{node_type}, total degree'\n",
    "            save_file_ = f'{node_type}__total_degree.png'\n",
    "        else:\n",
    "            title_ = f'{node_type}, {\" \".join(edge_type[2].split(\"_\"))}-{\" \".join(edge_type[0].split(\"_\"))}'\n",
    "            save_file_ = f'{node_type}__{\" \".join(edge_type[2].split(\"_\"))}_{\" \".join(edge_type[0].split(\"_\"))}.png'\n",
    "            \n",
    "        if not os.path.exists(ROOT_FOLDER+'plot_images'):\n",
    "            os.mkdir(ROOT_FOLDER+'plot_images')\n",
    "        plot_degree(x, title_, 'Degree', 'Count', ROOT_FOLDER+'plot_images/'+save_file_, savefig=True, xlim=[0, np.mean(x)+np.std(x)*1], color='#b8cce4')\n",
    "        \n",
    "        maximum = degrees[node_type][edge_type].max()\n",
    "        if node_degrees is None:\n",
    "            node_degrees = (degrees[node_type][edge_type]/maximum).unsqueeze(1)\n",
    "        else:\n",
    "            node_degrees = torch.cat((node_degrees, (degrees[node_type][edge_type]/maximum).unsqueeze(1)), dim=1)\n",
    "            \n",
    "        print('node degree shape',node_type, node_degrees.shape)\n",
    "        \n",
    "    if 'x' in data[node_type].keys():\n",
    "        data[node_type].x = torch.cat((data[node_type].x, node_degrees), dim=1)\n",
    "    else:\n",
    "        data[node_type].x = node_degrees\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91b381f-7ab1-4b9f-83be-4e379f56c09e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import gc\n",
    "from torch_geometric.utils import is_undirected\n",
    "\n",
    "\n",
    "''' def triangle_count(adj_matrix:SparseTensor):\n",
    "    # block wise triangle count, to avoid memory issues\n",
    "    diags = []\n",
    "    \n",
    "    row_block_size = 40\n",
    "    rows = adj_matrix.size(0)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print('using',device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        adj_matrix = adj_matrix.to(device)\n",
    "        counter = 0\n",
    "        for block in tqdm(range(0,rows, row_block_size), desc='blockwise sparse matrix-multiplication'):\n",
    "            if counter % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            start = block\n",
    "            end = min(block+row_block_size, rows)\n",
    "            adj_pow_3_block = adj_matrix[start:end].spspmm(adj_matrix).spspmm(adj_matrix)\n",
    "            diag_block = get_diag(adj_pow_3_block[:,start:]).cpu()\n",
    "            diags.append(diag_block)\n",
    "            counter +=1\n",
    "\n",
    "    \n",
    "    return 1/2 * torch.cat(diags, dim=0) '''\n",
    "def triangle_count(adj_matrix:SparseTensor):\n",
    "    # adj_matmul, blockwise, so kernel does not crash\n",
    "    # diag1((row1 to rowS) * full_matrix * full_matrix)\n",
    "    # diag2((rowS+1 to rowR) * full_matrix * full_matrix)\n",
    "    # ....\n",
    "    diags = []\n",
    "    \n",
    "    row_block_size = 20\n",
    "    rows = adj_matrix.size(0)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        adj_matrix = adj_matrix.to(device)\n",
    "        count = 0\n",
    "        for block in tqdm(range(0,rows, row_block_size), desc='blockwise sparse matrix-multiplication'):\n",
    "            if count % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            start = block\n",
    "            end = min(block+row_block_size, rows)\n",
    "            adj_pow_2_block = adj_matrix[start:end].spspmm(adj_matrix)\n",
    "            # get only the diag part now\n",
    "            for i in range(0, adj_pow_2_block.size(dim=0)):\n",
    "                diag_i = adj_pow_2_block[i,:].spspmm(adj_matrix[:,i]).to_dense().cpu()\n",
    "                diags.append(diag_i[0,0].item())\n",
    "            \n",
    "            count +=1\n",
    "    \n",
    "    return 1/2 * torch.tensor(diags)\n",
    "\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_sparse.diag import get_diag\n",
    "\n",
    "def undirected_triangle_counts(edge_index, max_num_nodes): \n",
    "    \"\"\"Get triangles **per node**, to get count for whole graph, divide by 3\"\"\"\n",
    "    \n",
    "    if not is_undirected(edge_index):\n",
    "        print('converting edge index to undirected')\n",
    "        edge_index = to_undirected(edge_index)\n",
    "    \n",
    "    adj_matrix = SparseTensor(row=edge_index[0], col=edge_index[1], value=torch.ones(edge_index[1].shape[0]), sparse_sizes=(max_num_nodes, max_num_nodes))\n",
    "    triangles = triangle_count(adj_matrix) \n",
    "    return triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02be1439-551b-4b4d-9c2d-a10a0f95f63b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for edge types where both ends are the same node type, add the triangle count for the node\n",
    "import torch \n",
    "if not os.path.exists(ROOT_FOLDER+'precomputed_triangles'):\n",
    "    os.mkdir(ROOT_FOLDER+'precomputed_triangles')\n",
    "for edge_type in data.edge_types:\n",
    "    if edge_type[1] == 'broader_job_job' or edge_type[1] == 'supervisor_supervisee':\n",
    "        continue # there are no triangles\n",
    "\n",
    "    if edge_type[0] == edge_type[2]:\n",
    "        print('')\n",
    "        print('add triangles to', edge_type[0])\n",
    "        print(edge_type)\n",
    "        # if file already in pickle, load from pickle, folder is precomputed_triangles\n",
    "        pickle_path = ROOT_FOLDER+'precomputed_triangles/'+edge_type[0]+'_'+('_'.join(edge_type))+'.pt'\n",
    "        if os.path.exists(pickle_path):\n",
    "            triangles = torch.load(pickle_path)\n",
    "        else:\n",
    "            triangles = undirected_triangle_counts(data[edge_type].edge_index, data[edge_type[0]].num_nodes)\n",
    "            torch.save(triangles, pickle_path)\n",
    "            \n",
    "        print(edge_type[0], 'triangles', '\\nmean', triangles.mean(),'\\nstd', triangles.std(), '\\nmedian',triangles.median(), '\\nmax',triangles.max(), '\\nmin',triangles.min())\n",
    "        \n",
    "        if 'x' in data[edge_type[0]].keys():\n",
    "            data[edge_type[0]].x = torch.cat([data[edge_type[0]].x, triangles.unsqueeze(1)], dim=1)\n",
    "        else:\n",
    "            data[edge_type[0]].x = triangles.unsqueeze(1)\n",
    "            \n",
    "\n",
    "\n",
    "print('add homogeneous triangles')\n",
    "homogeneous_data = data.to_homogeneous()\n",
    "if os.path.exists(ROOT_FOLDER+'precomputed_triangles/homogeneous.pt'):\n",
    "    homogenous_triangles = torch.load(ROOT_FOLDER+'precomputed_triangles/homogeneous.pt')\n",
    "else:\n",
    "    homogenous_triangles = undirected_triangle_counts(homogeneous_data.edge_index, homogeneous_data.num_nodes)\n",
    "    torch.save(homogenous_triangles, ROOT_FOLDER+'precomputed_triangles/homogeneous.pt')\n",
    "    \n",
    "print('homogeneous triangles', '\\nmean', homogenous_triangles.mean(),'\\nstd', homogenous_triangles.std(), '\\nmedian',homogenous_triangles.median(), '\\nmax',homogenous_triangles.max(), '\\nmin',homogenous_triangles.min())\n",
    "\n",
    "for i, node_type in zip(homogeneous_data.node_type.unique(), data.node_types):\n",
    "    mask =  homogeneous_data.node_type == i.item()\n",
    "    \n",
    "    if 'x' in data[node_type].keys():\n",
    "        data[node_type].x = torch.cat([data[node_type].x, homogenous_triangles[mask].unsqueeze(1)], dim=1)\n",
    "    else:\n",
    "        data[node_type].x = homogenous_triangles[mask].unsqueeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6488d0c4-7dc8-4477-bfa9-a485c6d264b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdd607d-d434-40ad-ab75-f9d9d65fa51a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "homogeneous_data.node_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85812a3b-feff-45f8-b6f0-5bb9472d2c70",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "homogeneous_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8830d3-2d1a-4045-9694-514cc55c8382",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.node_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b45406-041b-4967-b4c9-2c6aa1f2fb74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['courses_and_programs'].x.shape, torch.sum(homogeneous_data.node_type==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1909bdcf-f2a3-4929-9b33-5c998261ad23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "triangles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d9053f-fcc4-4d50-883f-2edfd16cabd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = T.ToUndirected(merge=False)(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98787bca-cbbf-4b52-94b4-87377b916dae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "\n",
    "filename = ROOT_FOLDER+'HeteroData_Learnings_v1.pt'\n",
    "if os.path.exists(filename):\n",
    "    # data = HeteroData.from_dict(torch.load('./'+filename))\n",
    "    raise Exception('File already exists')\n",
    "else:\n",
    "    torch.save(data.to_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00dc7a78-ac57-4134-b261-0073023670fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# END\n",
    "\n",
    "wadawdwa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a448415e-04cf-4085-a68f-e11107548384",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['courses_and_programs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd087a1a-68a5-406a-a5f5-d384b67abd17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# drop all text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a783a2c-6770-46fe-a049-d247e127d83b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.HGTConv.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a23ea45-fbc6-440b-8ca3-9fb86eb0a5d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wdwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36fe9e9-7ca2-4048-84b9-f0ff620a80bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "\n",
    "data = HeteroData()\n",
    "data['Skill'].x = torch.tensor(skill_sbert_embeddings)\n",
    "data['Job'].x = torch.tensor(job_sbert_embeddings)\n",
    "\n",
    "data['Job','REQUIRES','Skill'].edge_index = torch.tensor(skill_job_edges[['job_src','skill_dst']].to_numpy().T)\n",
    "data['Skill','IS_SIMILAR_SKILL','Skill'].edge_index = torch.tensor(skill_skill_edges[['skill_src','skill_dst']].to_numpy().T)\n",
    "data['Job','IS_SIMILAR_JOB','Job'].edge_index = torch.tensor(job_job_edges[['job_src','job_dst']].to_numpy().T)\n",
    "\n",
    "\n",
    "data['Job','REQUIRES','Skill'].edge_weight = torch.tensor(skill_job_edges['normalized_tfidf']).to(torch.float)\n",
    "data['Skill','IS_SIMILAR_SKILL','Skill'].edge_weight = torch.tensor(skill_skill_edges['cosine_sim_score'].to_numpy()).to(torch.float)\n",
    "data['Job','IS_SIMILAR_JOB','Job'].edge_weight = torch.tensor(job_job_edges['relatedness_weight'].to_numpy()).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "589e25fa-427b-40df-8382-a46cab8abbbd",
     "showTitle": false,
     "title": ""
    },
    "id": "pCzFw-fn9udY"
   },
   "outputs": [],
   "source": [
    "# map 3,6,7,3,3,... to 1,2,3,1,1 ...\n",
    "skillmapping ={}\n",
    "for i,skill in enumerate(skill_nodes.skill.unique()):\n",
    "    skillmapping[skill] =i\n",
    "\n",
    "jobmapping ={}\n",
    "jobmapping_index_to_title_alttile = {}\n",
    "for i, index in enumerate(job_nodes['index'].unique()):\n",
    "    jobmapping[index] =i\n",
    "\n",
    "for _, row in job_nodes.iterrows():\n",
    "    if type(row['Title']) == str:\n",
    "        a = row['Title']\n",
    "    else:\n",
    "        a = ''\n",
    "    \n",
    "    jobmapping_index_to_title_alttile[row['index']] = (a+' / '+row['Alternate Title']).strip('/')\n",
    "\n",
    "inverted_skillmapping = {v:k for k,v in skillmapping.items()}\n",
    "inverted_jobmapping = {v:k for k,v in jobmapping.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "402bf7a6-5dae-45c8-ad18-c582b4080fff",
     "showTitle": false,
     "title": ""
    },
    "id": "BHirmQzn9udY"
   },
   "outputs": [],
   "source": [
    "skill_job_edges['skill_dst'] = skill_job_edges['skill'].apply(lambda x:skillmapping[x])\n",
    "skill_job_edges['job_src'] = skill_job_edges['alt_title'].apply(lambda x:jobmapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f18c966e-e911-4dc8-b260-b8efa8a4f01c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# normalization and removal of job-skill edges\n",
    "# first group by jobs and limit the skills for each job to 175\n",
    "# then normalize those tfidf edgeweights\n",
    "# then group by skill and limit edges to 125 for skills (ordered by normalized tf-idf)\n",
    "skill_job_edges = skill_job_edges.groupby('job_src').apply(lambda group: group.nlargest(200,'scaled_tfidf')).reset_index(drop=True)\n",
    "summed_tfidf_per_job = skill_job_edges.groupby('job_src').sum()\n",
    "skill_job_edges['sum'] = skill_job_edges['job_src'].apply(lambda x: summed_tfidf_per_job.loc[x]['scaled_tfidf'])\n",
    "skill_job_edges['normalized_tfidf'] =  skill_job_edges['scaled_tfidf']/skill_job_edges['sum']\n",
    "skill_job_edges = skill_job_edges.groupby('skill_dst').apply(lambda group: group.nlargest(200,'normalized_tfidf')).reset_index(drop=True)\n",
    "# we dont do the second round of normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa245e5-141e-43d6-9554-369dd75cd72e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# normalization and removing of skill-skill edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3de1149-9478-446e-8560-762bb3c964a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for each alt title select the first 125 skill_job edges, ordered by tfidf\n",
    "# skill_job_edges = skill_job_edges.groupby('job_src')\n",
    "# skill_job_edges = skill_job_edges.groupby('job_src').apply(lambda group: group.nlargest(125,'scaled_tfidf')).reset_index(drop=True)\n",
    "\n",
    "# for each skill only use the 125 edges with the highest tf-idf score\n",
    "#skill_job_edges = skill_job_edges.groupby('skill').apply(lambda group: group.nlargest(125,'scaled_tfidf')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fe696e-7354-4a07-8fdf-1d303580e35d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_job_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7931821-ed8a-4a90-adbd-3a3ff7933c3f",
     "showTitle": false,
     "title": ""
    },
    "id": "8Yfi1fRd9udY"
   },
   "outputs": [],
   "source": [
    "onet_alttitles = pd.read_csv('neo4jgraph/onet_alt_titles_unique.csv')\n",
    "del onet_alttitles['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "021285b8-e8fe-4a6e-9dda-b7ea0c4d63d4",
     "showTitle": false,
     "title": ""
    },
    "id": "8FgE8nyo9udY"
   },
   "outputs": [],
   "source": [
    "onet_alttitle_str_mapping = {}\n",
    "for i,row in onet_alttitles.iterrows():\n",
    "    onet_alttitle_str_mapping[row['index']] = row['Alternate Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e056a7a7-edf8-42ca-a002-6c5fbe01c66a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "mappings = {\n",
    "    'onet_alttitle_str_mapping':onet_alttitle_str_mapping,\n",
    "    'skillmapping':skillmapping,\n",
    "    'inverted_skillmapping':inverted_skillmapping,\n",
    "    'jobmapping':jobmapping,\n",
    "    'inverted_jobmapping':inverted_jobmapping,\n",
    "    'jobmapping_index_to_title_alttile':jobmapping_index_to_title_alttile\n",
    "}\n",
    "\n",
    "torch.save(mappings, 'Job_Skill_HeteroData_name_mappings_withdupes_fulldataset_v2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b598e2ca-95cd-489f-bee4-765859b45778",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "len(jobmapping.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950e43ca-f718-46df-a5e2-0fe185ecd1cb",
     "showTitle": false,
     "title": ""
    },
    "id": "E1_J7gV59udY"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "261b2c96-5750-4644-a6cf-b3eda7c3579a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53c874d-1051-4dbf-a51f-af92e8b57020",
     "showTitle": false,
     "title": ""
    },
    "id": "vK1p6Rf-9udZ"
   },
   "outputs": [],
   "source": [
    "skill_sbert_embeddings = embedder.encode(skill_nodes['skill'].tolist(), convert_to_numpy=True, device='cuda')\n",
    "job_sbert_embeddings = embedder.encode(job_nodes['Alternate Title'].tolist(), convert_to_numpy=True, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7141ed82-8983-493b-874a-4653e94b247f",
     "showTitle": false,
     "title": ""
    },
    "id": "_vD1fRkf9udZ"
   },
   "outputs": [],
   "source": [
    "# add job-job edges, dataset see https://www.onetcenter.org/dictionary/26.3/excel/related_occupations.html\n",
    "job_job_edges = pd.read_csv('neo4jgraph/onet_related_occupations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5ffa89d-0204-4339-ae59-ebf044a63f48",
     "showTitle": false,
     "title": ""
    },
    "id": "hMsTH1Sx9udZ"
   },
   "outputs": [],
   "source": [
    "job_job_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46468220-66d5-4b98-b921-3b5af59eb711",
     "showTitle": false,
     "title": ""
    },
    "id": "p1lR651a9uda"
   },
   "outputs": [],
   "source": [
    "job_job_edges['job_src'] = job_job_edges['index_x'].apply(lambda x: jobmapping[x])\n",
    "job_job_edges['job_dst'] = job_job_edges['index_y'].apply(lambda x: jobmapping[x])\n",
    "relatedness_weight = {\n",
    "    'Supplemental':0.5,\n",
    "    'Primary-Long':0.75,\n",
    "    'Primary-Short':1\n",
    "}\n",
    "job_job_edges['relatedness_weight'] = job_job_edges['Relatedness Tier'].apply(lambda x: relatedness_weight[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3200f5aa-4e46-4627-8d7e-6820063943d4",
     "showTitle": false,
     "title": ""
    },
    "id": "T1Wc7CIL9uda"
   },
   "outputs": [],
   "source": [
    "skill_skill_edges = pd.read_csv('neo4jgraph/skill_skill_edges.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9612c70-77b2-4e07-8ad8-cd45b5c11473",
     "showTitle": false,
     "title": ""
    },
    "id": "8qOHTGfL9uda"
   },
   "outputs": [],
   "source": [
    "#filter out potentially bad skills (which are not in our original skillmapping)\n",
    "skill_skill_edges = skill_skill_edges.loc[(skill_skill_edges.skill.isin(list(skillmapping.keys()))) & (skill_skill_edges.related_skill.isin(list(skillmapping.keys())))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f15d853-b648-4c3c-99cf-d0880327176e",
     "showTitle": false,
     "title": ""
    },
    "id": "jQhTLGTV9uda"
   },
   "outputs": [],
   "source": [
    "skill_skill_edges['skill_src'] = skill_skill_edges['skill'].apply(lambda x: skillmapping[x])\n",
    "skill_skill_edges['skill_dst'] = skill_skill_edges['related_skill'].apply(lambda x: skillmapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce3502d-92a0-4de3-8998-95db27fdab10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # only take largest 125 \"both directions\" (not ideal)\n",
    "# skill_skill_edges =  skill_skill_edges.groupby('skill_src').apply(lambda group: group.nlargest(125,'cosine_sim_score')).reset_index(drop=True)\n",
    "# skill_skill_edges =  skill_skill_edges.groupby('skill_dst').apply(lambda group: group.nlargest(125,'cosine_sim_score')).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "280386b3-43f6-4c1f-9219-c8187f84eaa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get normalized tfidf boxplot using matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.boxplot(skill_job_edges['normalized_tfidf'])\n",
    "plt.show()\n",
    "\n",
    "# get count of skills outside the boxplot\n",
    "len(skill_job_edges.loc[skill_job_edges['normalized_tfidf']>0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f697795-42cc-44cc-81dd-6665e9d806c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(a, bins=100)\n",
    "plt.show()\n",
    "plt.hist(skill_skill_edges['cosine_sim_score'].to_numpy(), bins=100)\n",
    "plt.show()\n",
    "plt.hist(job_job_edges['relatedness_weight'].to_numpy(), bins=100)\n",
    "\n",
    "# change all values over "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b24d14-68e3-42df-af4b-919297f84ddf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# recscale to interval 0.5-1\n",
    "a = skill_job_edges['normalized_tfidf'].copy()\n",
    "a[a>0.08] = 0.08\n",
    "a = (a-a.min())/(a.max()-a.min())\n",
    "# rescale to interval 0.5-1\n",
    "a = (a*0.5)+0.5\n",
    "skill_job_edges['normalized_tfidf'] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ffc137-34db-4ebb-9a55-91813a7530e1",
     "showTitle": false,
     "title": ""
    },
    "id": "La85Q2M59uda"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch\n",
    "\n",
    "data = HeteroData()\n",
    "data['Skill'].x = torch.tensor(skill_sbert_embeddings)\n",
    "data['Job'].x = torch.tensor(job_sbert_embeddings)\n",
    "\n",
    "data['Job','REQUIRES','Skill'].edge_index = torch.tensor(skill_job_edges[['job_src','skill_dst']].to_numpy().T)\n",
    "data['Skill','IS_SIMILAR_SKILL','Skill'].edge_index = torch.tensor(skill_skill_edges[['skill_src','skill_dst']].to_numpy().T)\n",
    "data['Job','IS_SIMILAR_JOB','Job'].edge_index = torch.tensor(job_job_edges[['job_src','job_dst']].to_numpy().T)\n",
    "\n",
    "\n",
    "data['Job','REQUIRES','Skill'].edge_weight = torch.tensor(skill_job_edges['normalized_tfidf']).to(torch.float)\n",
    "data['Skill','IS_SIMILAR_SKILL','Skill'].edge_weight = torch.tensor(skill_skill_edges['cosine_sim_score'].to_numpy()).to(torch.float)\n",
    "data['Job','IS_SIMILAR_JOB','Job'].edge_weight = torch.tensor(job_job_edges['relatedness_weight'].to_numpy()).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d3f89c8-6b8f-4bcb-89a4-7bdc513b7e35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Look at node degree statistics\n",
    "\n",
    "from torch_geometric.utils import to_dense_adj, degree\n",
    "\n",
    "\n",
    "\n",
    "job_n = data['Job'].x.shape[0]\n",
    "skill_n = data['Skill'].x.shape[0]\n",
    "\n",
    "JRS_J = degree(data['Job','REQUIRES','Skill'].edge_index[0], num_nodes=job_n)\n",
    "JRS_S = degree(data['Job','REQUIRES','Skill'].edge_index[1], num_nodes=skill_n)\n",
    "S_SIM_S = degree(data['Skill','IS_SIMILAR_SKILL','Skill'].edge_index.flatten(), num_nodes=skill_n)\n",
    "J_SIM_J = degree(data['Job','IS_SIMILAR_JOB','Job'].edge_index.flatten(), num_nodes=job_n)\n",
    "\n",
    "actual_skill_n = torch.nonzero(JRS_S+S_SIM_S).shape[0] # only skills which have any edge at all\n",
    "actual_job_n = torch.nonzero(JRS_J+J_SIM_J).shape[0] # only job which have any edge at all\n",
    "print(f'Jobs: {job_n}, actual Jobs used (in at least one edge): {actual_job_n}')\n",
    "print(f'Skills: {skill_n}, actual Skills used (in at least one edge): {actual_skill_n}')\n",
    "\n",
    "print('\\nFollowing metrics only include Skills and Jobs with at least one edge:\\n')\n",
    "\n",
    "print(f\"JRS edges: {data['Job','REQUIRES','Skill'].edge_index.shape[1]}\")\n",
    "print(f'Average JRS Job degree: {torch.sum(JRS_J)/actual_job_n}, Skill: {torch.sum(JRS_S)/actual_skill_n}')\n",
    "print(f'Median JRS Job degree: {torch.median(JRS_J[JRS_J!=0])}, Skill: {torch.median(JRS_S[JRS_S!=0])}')\n",
    "print(f'Max JRS Job degree: {torch.max(JRS_J)}, Skill: {torch.max(JRS_S)}\\n')\n",
    "\n",
    "print(f\"S_SIM_S edges: {data['Skill','IS_SIMILAR_SKILL','Skill'].edge_index.shape[1]}\")\n",
    "print(f'Average S_SIM_S degree: {torch.sum(S_SIM_S)/actual_skill_n}')\n",
    "print(f'Median S_SIM_S degree: {torch.median(S_SIM_S[S_SIM_S!=0])}')\n",
    "print(f'Max S_SIM_S degree: {torch.max(S_SIM_S)}')\n",
    "\n",
    "print(f'J_SIM_J edges: {data[\"Job\", \"IS_SIMILAR_JOB\", \"Job\"].edge_index.shape[1]}')\n",
    "print(f'Average J_SIM_J degree: {torch.sum(J_SIM_J)/actual_job_n}')\n",
    "print(f'Median J_SIM_J degree: {torch.median(J_SIM_J[J_SIM_J!=0])}')\n",
    "print(f'Max J_SIM_J degree: {torch.max(J_SIM_J)}\\n')\n",
    "\n",
    "print(f'Average total degree: Job: {(torch.sum(JRS_J)+torch.sum(J_SIM_J))/actual_job_n}')\n",
    "print(f'Average total degree: Skill: {(torch.sum(JRS_S)+torch.sum(S_SIM_S))/actual_skill_n}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "\n",
    "os.makedirs(\"plot_images/\", exist_ok=True)\n",
    "\n",
    "# Plot and save the first plot\n",
    "plt.title('J-R-S Job degree-distribution')\n",
    "plt.hist(JRS_J[JRS_J != 0], bins=25)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('plot_images/JRS_J_degree_distribution.svg', format='svg')\n",
    "plt.show()\n",
    "plt.clf()  # Clear the current figure\n",
    "\n",
    "# Plot and save the second plot\n",
    "plt.title('J-R-S Skill degree-distribution')\n",
    "plt.hist(JRS_S[JRS_S != 0], bins=25)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')t\n",
    "plt.savefig('plot_images/JRS_S_degree_distribution.svg', format='svg')\n",
    "plt.show()\n",
    "plt.clf()  # Clear the current figure\n",
    "\n",
    "# Plot and save the third plot\n",
    "plt.title('S-R-S Skill degree')\n",
    "plt.hist(S_SIM_S[S_SIM_S != 0], bins=25)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('plot_images/S_SIM_S_skill_degree.svg', format='svg')\n",
    "plt.show()\n",
    "plt.clf()  # Clear the current figure\n",
    "\n",
    "# Plot and save the fourth plot\n",
    "plt.title('J-R-J Job degree')\n",
    "plt.hist(J_SIM_J[J_SIM_J != 0], bins=25)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig('plot_images/J_SIM_J_job_degree.svg', format='svg')\n",
    "plt.show()\n",
    "plt.clf()  # Clear the current figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacf9a4a-db13-4565-9d25-f6f78e862c1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add features:\n",
    "# - node degree\n",
    "# - normalize edge weights by node degree\n",
    "# - (triangle count)\n",
    "\n",
    "\n",
    "# add node degree statistics:\n",
    "\n",
    "job_degrees = torch.cat((JRS_J.reshape(-1,1) / 125, J_SIM_J.reshape(-1,1)/ 125), dim=1) # divide by approx. max degrees\n",
    "skill_degrees = torch.cat((JRS_S.reshape(-1,1) / 125, S_SIM_S.reshape(-1,1)/ 125), dim=1) # divide by approx. max degrees\n",
    "\n",
    "data['Job'].x = torch.cat((data['Job'].x, job_degrees), dim=1)\n",
    "data['Skill'].x = torch.cat((data['Skill'].x, skill_degrees), dim=1)\n",
    "\n",
    "# normalize edge weights by node degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a2e5568-4f28-4891-8c70-c814ab4157aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import gc\n",
    "# adj_matmul\n",
    "# row1 to rowS * full_matrix\n",
    "# rowS+1 to rowT * full_matrix\n",
    "# ....\n",
    "\n",
    "# def blockwise_sparse_square_mmul(adj_matrix, blocks=None):\n",
    "#     row_blocks = []\n",
    "    \n",
    "#     if blocks is None:\n",
    "#         row_block_size = 10000\n",
    "#         rows = adj_matrix.size(0)\n",
    "#         for block in tqdm(range(0,rows, row_block_size), desc='blockwise sparse matrix-multiplication'):\n",
    "#             start = block\n",
    "#             end = min(block+row_block_size, rows)\n",
    "#             row_blocks.append(adj_matrix[start:end].spspmm(adj_matrix))\n",
    "#     else:\n",
    "#         for block in tqdm(blocks, desc='blockwise sparse matrix-multiplication'):\n",
    "#             row_blocks.append(block.spspmm(adj_matrix))\n",
    "\n",
    "#     return row_blocks\n",
    "\n",
    "\n",
    "# def blockwise_sparse_get_diag(blocks):\n",
    "#     diags = []\n",
    "#     for block in tqdm(blocks, desc='get blockwise sparse matrix diagonal'):\n",
    "#         diags.append(get_diag(block))\n",
    "    \n",
    "#     return torch.cat(diags, dim=0)\n",
    "\n",
    "def triangle_count(adj_matrix:SparseTensor):\n",
    "    # adj_matmul, blockwise, so kernel does not crash\n",
    "    # diag1((row1 to rowS) * full_matrix * full_matrix)\n",
    "    # diag2((rowS+1 to rowR) * full_matrix * full_matrix)\n",
    "    # ....\n",
    "    diags = []\n",
    "    \n",
    "    row_block_size = 400\n",
    "    rows = adj_matrix.size(0)\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        adj_matrix = adj_matrix.to(device)\n",
    "        for block in tqdm(range(0,rows, row_block_size), desc='blockwise sparse matrix-multiplication'):\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            start = block\n",
    "            end = min(block+row_block_size, rows)\n",
    "            adj_pow_3_block = adj_matrix[start:end].spspmm(adj_matrix).spspmm(adj_matrix)\n",
    "            diag_block = get_diag(adj_pow_3_block[:,start:].cpu()).cpu()\n",
    "            diags.append(diag_block)\n",
    "            # diag = torch.eye(end)\n",
    "            # indices_i, indices_j = diag.nonzero().unbind(dim=1)\n",
    "            \n",
    "            # A = adj_pow_2_block[indices_i]\n",
    "            # B = adj_matrix[:, indices_j]\n",
    "            # print(A, t(B))\n",
    "            # print(type(B), type(A))\n",
    "            # print(A.size(0), A.size(1), B.size(0), B.size(1))\n",
    "            #C = A * t(B)\n",
    "            #C = reduction(C, dim=1)\n",
    "            \n",
    "           \n",
    "            #print(C)\n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    return 1/2 * torch.cat(diags, dim=0)\n",
    "        \n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_sparse.diag import get_diag\n",
    "\n",
    "def undirected_triangle_counts(edge_index, max_num_nodes): \n",
    "    \"\"\"Get triangles **per node**, to get count for whole graph, divide by 3\"\"\"\n",
    "    ud = to_undirected(edge_index)\n",
    "    \n",
    "    adj_matrix = SparseTensor(row=ud[0], col=ud[1], value=torch.ones(ud[1].shape[0]), sparse_sizes=(max_num_nodes, max_num_nodes))\n",
    "    #adj_matrix = torch.sparse_coo_tensor(edge_index, torch.ones(edge_index.shape[1]), (max_num_nodes, max_num_nodes))\n",
    "    #adj_matrix = torch.sparse_csr_tensor(ud[0], ud[1], values=torch.ones(ud[1].shape[0]), dtype=torch.float32).to_sparse_coo()\n",
    "    triangles = triangle_count(adj_matrix) \n",
    "    return triangles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df420ab-220d-4336-8881-de450a881445",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch_sparse import SparseTensor\n",
    "adj = torch.tensor( [\n",
    "       [0,1,1,1,1],\n",
    "        [1,0,1,0,1],\n",
    "        [1,1,0,1,0],\n",
    "        [1,0,1,0,1],\n",
    "       [1,1,0,1,0]\n",
    "    ] ).to(torch.float) \n",
    "X = SparseTensor.from_dense(adj)\n",
    "# triangles = triangle_count(X)\n",
    "# triangles\n",
    "#adj_matrix = SparseTensor(row=ud[0], col=ud[1], value=torch.ones(ud[1].shape[0]), sparse_sizes=(max_num_nodes, max_num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfb06167-b639-41ef-8e54-a3ca62040b49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34789353-a02b-4f53-9fdb-2c557813c7e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "homogeneous_data = data.to_homogeneous()\n",
    "homogenous_triangles = undirected_triangle_counts(homogeneous_data.edge_index, homogeneous_data.x.shape[0])\n",
    "# max triangles: 9000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2398396-de86-4cd4-8684-73fd73662370",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Job_homogeneous_triangles = homogenous_triangles[homogeneous_data.node_type == 1]\n",
    "Skill_homogeneous_triangles = homogenous_triangles[homogeneous_data.node_type != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c783c5eb-4b19-4efd-a5b6-c6e6e174144d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "actual_s_triangles = Skill_homogeneous_triangles[(JRS_S+S_SIM_S)!=0] \n",
    "actual_j_triangles = Job_homogeneous_triangles[(JRS_J+J_SIM_J)!=0] \n",
    "\n",
    "print(f'Median triangles of skill nodes: {actual_s_triangles.median()}, mean: {actual_s_triangles.mean()}, max: {actual_s_triangles.max()}, min: {actual_s_triangles.min()}')\n",
    "print(f'Median triangles of job nodes: {actual_j_triangles.median()}, mean: {actual_j_triangles.mean()}, max: {actual_j_triangles.max()}, min: {actual_s_triangles.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92ccace7-4e06-45e9-a2d2-5cc7a6e095b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plt.title('Skill triangles')\n",
    "plt.hist(actual_s_triangles.numpy(), bins=40)\n",
    "plt.show()\n",
    "plt.title('Job triangles')\n",
    "plt.hist(actual_j_triangles.numpy(), bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0b5508-bb00-42b7-b05b-dd0fe7978f19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f32ca5c-cba5-4313-9100-fa10c3c1f076",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "J_SIM_J_triangles = undirected_triangle_counts(data['Job','IS_SIMILAR_JOB', 'Job'].edge_index, data['Job'].x.shape[0])\n",
    "S_SIM_S_triangles = undirected_triangle_counts(data['Skill','IS_SIMILAR_SKILL', 'Skill'].edge_index, data['Skill'].x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48aef43-ebe7-47d7-b318-b4a129c1b504",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "actual_s_triangles = S_SIM_S_triangles[(JRS_S+S_SIM_S)!=0] \n",
    "actual_j_triangles = J_SIM_J_triangles[(JRS_J+J_SIM_J)!=0] \n",
    "\n",
    "print(f'Median triangles of skill nodes for skill-skill edges: {actual_s_triangles.median()}, mean: {actual_s_triangles.mean()}, max: {actual_s_triangles.max()}, min: {actual_s_triangles.min()}')\n",
    "print(f'Median triangles of job nodes for job-job edges: {actual_j_triangles.median()}, mean: {actual_j_triangles.mean()}, max: {actual_j_triangles.max()}, min: {actual_s_triangles.min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3247a0f6-ac55-4446-9e65-70c547351239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Skill_homogeneous_triangles.max(), Job_homogeneous_triangles.max(), S_SIM_S_triangles.max(), J_SIM_J_triangles.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fda3f11-33f5-41d2-82c4-812f023a3a41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53c647d4-925b-46ad-9e57-6b55603c3ef8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data['Skill'].x = torch.cat((data['Skill'].x, Skill_homogeneous_triangles.reshape(-1,1)/65380), dim=1) # normalize by max 9285.0\n",
    "data['Job'].x = torch.cat((data['Job'].x, Job_homogeneous_triangles.reshape(-1,1)/2059), dim=1) # normalize by max 1320\n",
    "\n",
    "data['Skill'].x = torch.cat((data['Skill'].x, S_SIM_S_triangles.reshape(-1,1)/65380), dim=1) # normalize by max 9285.0\n",
    "data['Job'].x = torch.cat((data['Job'].x, J_SIM_J_triangles.reshape(-1,1)/1284), dim=1) # normalize by max 1320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea951a2a-d65b-4231-9774-745f2a22d0f4",
     "showTitle": false,
     "title": ""
    },
    "id": "GZAF_riX9uda"
   },
   "outputs": [],
   "source": [
    "data.has_isolated_nodes(), data.has_self_loops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf138af0-d269-4644-9dda-79b410139905",
     "showTitle": false,
     "title": ""
    },
    "id": "u5MjvjO09uda"
   },
   "outputs": [],
   "source": [
    "#data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be2a20c-78d2-456d-80d9-2fe3e6d04ff0",
     "showTitle": false,
     "title": ""
    },
    "id": "aMAfCM0u9uda"
   },
   "outputs": [],
   "source": [
    "import torch_geometric.transforms as T\n",
    "\n",
    "transform = T.Compose([\n",
    "       #T.RemoveIsolatedNodes(),\n",
    "       T.RemoveDuplicatedEdges(),\n",
    "       T.ToUndirected(merge=False) # don't merge reversed edges into the original edge type\n",
    "])\n",
    "\n",
    "data = transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426004ed-e254-476f-8625-cb2e0339a31d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(data.to_dict(), path)\n",
    "# data = Data.from_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4cb44ea-8370-469f-a43f-0212e8449fc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "\n",
    "filename = 'Job_Skill_HeteroData_withdupes_fulldataset_v2.pt'\n",
    "if os.path.exists('./'+filename):\n",
    "    # data = HeteroData.from_dict(torch.load('./'+filename))\n",
    "    raise Exception('File already exists')\n",
    "else:\n",
    "    torch.save(data.to_dict(), './'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1adfe637-ba3c-4bcf-a500-cbd031011f4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "make_dataset_with_learnings_v1",
   "widgets": {}
  },
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
